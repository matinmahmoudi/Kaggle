{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686facc3",
   "metadata": {
    "papermill": {
     "duration": 0.007002,
     "end_time": "2024-03-30T10:15:58.520388",
     "exception": false,
     "start_time": "2024-03-30T10:15:58.513386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After identifying cases with poor quality during the **Exploratory Data Analysis (EDA)** step, the data cleaning stage addresses and rectifies these issues.  \n",
    "\n",
    "**Data cleaning** plays a fundamental role in enhancing the overall **quality** and **reliability** of datasets, directly impacting the performance of **machine learning models** and **analytical outcomes**. A well-cleaned dataset serves as the bedrock for **robust** model training and **accurate predictions**. \n",
    "In this notebook, our primary focus is on executing the initial stage of data cleaning, including:\n",
    "* Feature screening\n",
    "* Handling values that fall outside the logical range of respective fields\n",
    "* Addressing inconsistencies within the dataset  \n",
    "\n",
    "This meticulous approach to data cleaning not only enhances the accuracy of our analyses but also contributes significantly to the **overall success** of data science projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fa1af",
   "metadata": {
    "papermill": {
     "duration": 0.006166,
     "end_time": "2024-03-30T10:15:58.533432",
     "exception": false,
     "start_time": "2024-03-30T10:15:58.527266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f572c4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:58.548848Z",
     "iopub.status.busy": "2024-03-30T10:15:58.548267Z",
     "iopub.status.idle": "2024-03-30T10:15:59.539730Z",
     "shell.execute_reply": "2024-03-30T10:15:59.538603Z"
    },
    "papermill": {
     "duration": 1.002675,
     "end_time": "2024-03-30T10:15:59.542616",
     "exception": false,
     "start_time": "2024-03-30T10:15:58.539941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/bank-loan/Bankloan.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec484c3f",
   "metadata": {
    "papermill": {
     "duration": 0.006363,
     "end_time": "2024-03-30T10:15:59.555768",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.549405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To enhance clarity and facilitate streamlined data processing, we **separate the dataset** into two distinct dataframes: one designated for the **target variable** or response, and the other for the **input variables** or predictors. This segregation allows for a more organized and efficient approach in preparing the data for subsequent modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c3282f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.571118Z",
     "iopub.status.busy": "2024-03-30T10:15:59.570724Z",
     "iopub.status.idle": "2024-03-30T10:15:59.582151Z",
     "shell.execute_reply": "2024-03-30T10:15:59.581017Z"
    },
    "papermill": {
     "duration": 0.022155,
     "end_time": "2024-03-30T10:15:59.584720",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.562565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = df.iloc[:,-1]\n",
    "inputs = df.iloc[:,0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb25dc9",
   "metadata": {
    "papermill": {
     "duration": 0.006347,
     "end_time": "2024-03-30T10:15:59.598035",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.591688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create two lists distinguishing **categorical** and **continuous** variables based on their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40e20f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.613150Z",
     "iopub.status.busy": "2024-03-30T10:15:59.612778Z",
     "iopub.status.idle": "2024-03-30T10:15:59.619279Z",
     "shell.execute_reply": "2024-03-30T10:15:59.618075Z"
    },
    "papermill": {
     "duration": 0.017194,
     "end_time": "2024-03-30T10:15:59.621944",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.604750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = inputs.columns\n",
    "\n",
    "# Choose categorical elements \n",
    "categorical_indices = [1]\n",
    "\n",
    "# Use a list comprehension to select the elements at the specified indices\n",
    "categorical_fields = [columns[i] for i in categorical_indices]\n",
    "\n",
    "# Create a new list of columns excluding categorical_fields (continuous)\n",
    "continuous_fields = [j for j in columns if j not in categorical_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db13e0",
   "metadata": {
    "papermill": {
     "duration": 0.006341,
     "end_time": "2024-03-30T10:15:59.635092",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.628751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Screening \n",
    "Filter out these features:  \n",
    "\n",
    "* **Features with a coefficient of variation less than 0.1 for continuous variables**  \n",
    "Identifying and screening out **continuous variables** with low variability ensures that the selected features provide **meaningful information** for analysis and modeling.\n",
    "\n",
    "* **Features where the mode category percentage is greater than 95% for categorical variables**  \n",
    "This step focuses on retaining **categorical variables** where one category overwhelmingly dominates, helping to streamline the dataset and enhance the interpretability of the resulting models.\n",
    "\n",
    "* **Features with a percentage of unique categories exceeding 90% for categorical variables**  \n",
    "Screening out **categorical variables** with a high percentage of unique categories contributes to simplifying the dataset and mitigating the risk of overfitting, ensuring a more robust and generalizable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a1ce1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.650117Z",
     "iopub.status.busy": "2024-03-30T10:15:59.649734Z",
     "iopub.status.idle": "2024-03-30T10:15:59.683979Z",
     "shell.execute_reply": "2024-03-30T10:15:59.683067Z"
    },
    "papermill": {
     "duration": 0.044561,
     "end_time": "2024-03-30T10:15:59.686356",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.641795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age  employ  address  income  debtinc   creddebt   othdebt\n",
      "0    41.0      17       12   176.0      9.3  11.359392  5.008608\n",
      "1    27.0      10        6    31.0     17.3   1.362202  4.000798\n",
      "2    40.0      15        7     NaN      5.5   0.856075  2.168925\n",
      "3    41.0      15       14   120.0      2.9   2.658720  0.821280\n",
      "4    24.0       2        0    28.0     17.3   1.787436  3.056564\n",
      "..    ...     ...      ...     ...      ...        ...       ...\n",
      "695  36.0       6       15    27.0      4.6   0.262062  0.979938\n",
      "696  29.0       6        4    21.0     11.5   0.369495  2.045505\n",
      "697  33.0      15        3    32.0      7.6   0.491264  1.940736\n",
      "698  45.0      19       22    77.0      8.4   2.302608  4.165392\n",
      "699  37.0      12       14     NaN     14.7   2.994684  3.473316\n",
      "\n",
      "[700 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a minimum value for coefficient of variation\n",
    "min_cv = 0.1\n",
    "\n",
    "# Calculate the coefficient of variation for each column\n",
    "cv_values = inputs[continuous_fields].std() / inputs[continuous_fields].mean()\n",
    "\n",
    "# Filter out columns with CV less than 0.1\n",
    "selected_columns =  cv_values[cv_values < 0.1].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "filtered_con = inputs[selected_columns]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "inputs_con = inputs[continuous_fields].drop(selected_columns, axis=1)\n",
    "print(inputs_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04191141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.702442Z",
     "iopub.status.busy": "2024-03-30T10:15:59.702042Z",
     "iopub.status.idle": "2024-03-30T10:15:59.718972Z",
     "shell.execute_reply": "2024-03-30T10:15:59.717484Z"
    },
    "papermill": {
     "duration": 0.028192,
     "end_time": "2024-03-30T10:15:59.721620",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.693428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ed\n",
      "0    3.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    NaN\n",
      "4    2.0\n",
      "..   ...\n",
      "695  2.0\n",
      "696  2.0\n",
      "697  1.0\n",
      "698  1.0\n",
      "699  1.0\n",
      "\n",
      "[700 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a threshold for the dominant category percentage\n",
    "threshold = 95\n",
    "\n",
    "# Calculate the percentage of the mode category for each column\n",
    "mode_category = (inputs[categorical_fields].apply(lambda x: x.value_counts().max() / len(x)) * 100)\n",
    "\n",
    "# Select columns where the mode category percentage is greater than the threshold\n",
    "selected_categorical_columns = mode_category[mode_category > threshold].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "mode_filtered_inputs = inputs[selected_categorical_columns]\n",
    "\n",
    "# Filter out selected columns and print the resulting DataFrame\n",
    "inputs_cat = inputs[categorical_fields].drop(selected_categorical_columns, axis=1)\n",
    "print(inputs_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70398dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.738088Z",
     "iopub.status.busy": "2024-03-30T10:15:59.737518Z",
     "iopub.status.idle": "2024-03-30T10:15:59.751784Z",
     "shell.execute_reply": "2024-03-30T10:15:59.750641Z"
    },
    "papermill": {
     "duration": 0.025383,
     "end_time": "2024-03-30T10:15:59.754181",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.728798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ed\n",
      "0    3.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    NaN\n",
      "4    2.0\n",
      "..   ...\n",
      "695  2.0\n",
      "696  2.0\n",
      "697  1.0\n",
      "698  1.0\n",
      "699  1.0\n",
      "\n",
      "[700 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set a threshold for excluding columns \n",
    "threshold = 90\n",
    "\n",
    "# Calculate the percentage of distinct categories in categorical variables\n",
    "distinct_percentage = (inputs_cat[categorical_fields].apply(lambda x: x.dropna().nunique() / x.count()) * 100)\n",
    "\n",
    "# Select categorical columns based on distinct percentage threshold\n",
    "selected_categorical_columns = distinct_percentage[distinct_percentage > threshold].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "distinct_filtered_inputs = inputs_cat[selected_categorical_columns]\n",
    "\n",
    "# Filter out selected columns and print the resulting DataFrame\n",
    "inputs_cat = inputs_cat.drop(selected_categorical_columns, axis=1)\n",
    "print(inputs_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a915e6e",
   "metadata": {
    "papermill": {
     "duration": 0.007326,
     "end_time": "2024-03-30T10:15:59.769013",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.761687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create a new dataframe by excluding both continuous and categorical features through feature screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb44fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.785612Z",
     "iopub.status.busy": "2024-03-30T10:15:59.785153Z",
     "iopub.status.idle": "2024-03-30T10:15:59.792050Z",
     "shell.execute_reply": "2024-03-30T10:15:59.790885Z"
    },
    "papermill": {
     "duration": 0.018063,
     "end_time": "2024-03-30T10:15:59.794427",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.776364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = pd.concat([inputs_con, inputs_cat, target], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31080e",
   "metadata": {
    "papermill": {
     "duration": 0.007027,
     "end_time": "2024-03-30T10:15:59.808984",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.801957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###  Handling Values Outside the Logical Range  \n",
    "In data analysis, handling values that fall outside the logical range of respective fields is a critical step to maintain the **integrity** and **reliability** of the dataset. Values significantly deviating from the expected range, can distort analytical results and impact the overall **quality of findings**.Whether in continuous fields, addressing values beyond the logical range ensures that subsequent modeling or statistical techniques are based on a more representative and **trustworthy** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad3dc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.825767Z",
     "iopub.status.busy": "2024-03-30T10:15:59.825073Z",
     "iopub.status.idle": "2024-03-30T10:15:59.883989Z",
     "shell.execute_reply": "2024-03-30T10:15:59.882829Z"
    },
    "papermill": {
     "duration": 0.070554,
     "end_time": "2024-03-30T10:15:59.886859",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.816305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age  employ  address  income  debtinc   creddebt   othdebt   ed default\n",
      "0    41.0      17       12   176.0      9.3  11.359392  5.008608  3.0       1\n",
      "1    27.0      10        6    31.0     17.3   1.362202  4.000798  1.0       0\n",
      "2    40.0      15        7     NaN      5.5   0.856075  2.168925  1.0       0\n",
      "3    41.0      15       14   120.0      2.9   2.658720  0.821280  NaN       0\n",
      "4    24.0       2        0    28.0     17.3   1.787436  3.056564  2.0       1\n",
      "..    ...     ...      ...     ...      ...        ...       ...  ...     ...\n",
      "695  36.0       6       15    27.0      4.6   0.262062  0.979938  2.0       1\n",
      "696  29.0       6        4    21.0     11.5   0.369495  2.045505  2.0       0\n",
      "697  33.0      15        3    32.0      7.6   0.491264  1.940736  1.0       0\n",
      "698  45.0      19       22    77.0      8.4   2.302608  4.165392  1.0       0\n",
      "699  37.0      12       14     NaN     14.7   2.994684  3.473316  1.0       0\n",
      "\n",
      "[700 rows x 9 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700 entries, 0 to 699\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       680 non-null    float64\n",
      " 1   employ    700 non-null    int64  \n",
      " 2   address   700 non-null    int64  \n",
      " 3   income    663 non-null    float64\n",
      " 4   debtinc   700 non-null    float64\n",
      " 5   creddebt  700 non-null    float64\n",
      " 6   othdebt   700 non-null    float64\n",
      " 7   ed        680 non-null    float64\n",
      " 8   default   700 non-null    object \n",
      "dtypes: float64(6), int64(2), object(1)\n",
      "memory usage: 49.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define ranges for each column\n",
    "column_ranges = {\n",
    "    'age': (18, 70),\n",
    "    'employ': (0, 31),\n",
    "    'address': (0, 80),\n",
    "    'income': (0, 1000),\n",
    "    'debtinc': (0, 100),\n",
    "    'creddebt': (0, 30),\n",
    "    'othdebt': (0, 30)\n",
    "}\n",
    "\n",
    "# Iterate through each column and fill NaN values outside the defined range\n",
    "for column, (min_val, max_val) in column_ranges.items():\n",
    "    filtered_df[column] = filtered_df[column].apply(lambda x: x if min_val <= x <= max_val else None)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(filtered_df)\n",
    "filtered_df.describe()\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e104d8",
   "metadata": {
    "papermill": {
     "duration": 0.007365,
     "end_time": "2024-03-30T10:15:59.902734",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.895369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Handling Inconsistent Data  \n",
    "In the area of data analysis, addressing inconsistent data is a basic task to ensure the **reliability** of results. Inconsistent data in **categorical variables**, whether due to **data entry errors** or discrepancies in **data Integration**, can introduce noise and **inaccuracies** into the dataset, potentially leading to misleading findings. For instance, one employee may enter customer addresses as \"block 1/23\", while another may use \"block 1-23\".  \n",
    "By handling and rectifying these inconsistencies, analysts can foster a more cohesive and accurate representation of the underlying information. The impact of such attention to detail extends beyond cleaning the dataset; it directly influences the **credibility** of analysis reports. A meticulously curated dataset, free from inconsistencies in codes, lays the groundwork for **robust** statistical analyses and more informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d27ad",
   "metadata": {
    "papermill": {
     "duration": 0.007301,
     "end_time": "2024-03-30T10:15:59.917531",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.910230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First detect inconsistent data in frequency tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924f91e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.934299Z",
     "iopub.status.busy": "2024-03-30T10:15:59.933882Z",
     "iopub.status.idle": "2024-03-30T10:15:59.943543Z",
     "shell.execute_reply": "2024-03-30T10:15:59.942158Z"
    },
    "papermill": {
     "duration": 0.020796,
     "end_time": "2024-03-30T10:15:59.945848",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.925052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0': Count: 1, Percentage: 0.14%\n",
      "0: Count: 515, Percentage: 73.57%\n",
      "1: Count: 183, Percentage: 26.14%\n",
      ":0: Count: 1, Percentage: 0.14%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def frequency_table(variable):\n",
    "    \n",
    "    # Get unique elements and their counts\n",
    "    unique_elements, counts = np.unique(variable.dropna(), return_counts=True)\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages = (counts / len(variable)) * 100\n",
    "\n",
    "    # Create a dictionary to store the value counts and percentages\n",
    "    value_counts_and_percentages = zip(unique_elements, counts, percentages)\n",
    "\n",
    "    # Print the value counts and percentages\n",
    "    for i, j, k in value_counts_and_percentages:\n",
    "        print(f\"{i}: Count: {j}, Percentage: {k:.2f}%\")\n",
    "    return\n",
    "\n",
    "\n",
    "frequency_table(filtered_df['default'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e33c0",
   "metadata": {
    "papermill": {
     "duration": 0.007539,
     "end_time": "2024-03-30T10:15:59.961735",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.954196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After detecting inconsistent data in the frequency tables, the logical next step is to replace the incorrect data with the correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59311978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:15:59.980117Z",
     "iopub.status.busy": "2024-03-30T10:15:59.979284Z",
     "iopub.status.idle": "2024-03-30T10:15:59.986041Z",
     "shell.execute_reply": "2024-03-30T10:15:59.984909Z"
    },
    "papermill": {
     "duration": 0.018962,
     "end_time": "2024-03-30T10:15:59.988900",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.969938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df['default'] = filtered_df['default'].replace([':0', \"'0'\"], '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9032c",
   "metadata": {
    "papermill": {
     "duration": 0.007553,
     "end_time": "2024-03-30T10:16:00.004377",
     "exception": false,
     "start_time": "2024-03-30T10:15:59.996824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Check the frequency table to ensure data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da937d3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:16:00.021232Z",
     "iopub.status.busy": "2024-03-30T10:16:00.020812Z",
     "iopub.status.idle": "2024-03-30T10:16:00.028671Z",
     "shell.execute_reply": "2024-03-30T10:16:00.027398Z"
    },
    "papermill": {
     "duration": 0.01913,
     "end_time": "2024-03-30T10:16:00.031121",
     "exception": false,
     "start_time": "2024-03-30T10:16:00.011991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Count: 517, Percentage: 73.86%\n",
      "1: Count: 183, Percentage: 26.14%\n"
     ]
    }
   ],
   "source": [
    "frequency_table(filtered_df['default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9692743a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:16:00.048712Z",
     "iopub.status.busy": "2024-03-30T10:16:00.048263Z",
     "iopub.status.idle": "2024-03-30T10:16:00.063635Z",
     "shell.execute_reply": "2024-03-30T10:16:00.062316Z"
    },
    "papermill": {
     "duration": 0.02742,
     "end_time": "2024-03-30T10:16:00.066415",
     "exception": false,
     "start_time": "2024-03-30T10:16:00.038995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df.to_csv('/kaggle/working/Bankloan_Cleanedv1.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4216596,
     "sourceId": 7273365,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.363853,
   "end_time": "2024-03-30T10:16:00.597856",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-30T10:15:54.234003",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
