{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14569d4b",
   "metadata": {
    "papermill": {
     "duration": 0.006468,
     "end_time": "2024-07-22T03:00:28.682234",
     "exception": false,
     "start_time": "2024-07-22T03:00:28.675766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸŒ± ðŸ§  Complete Guide to GPU PyTorch Commands\n",
    "\n",
    "Welcome to the comprehensive guide on GPU commands in PyTorch! This notebook is designed to equip you with everything you need to know about utilizing GPUs in PyTorch, from basic setup to advanced techniques for optimizing performance.\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "We'll explore various GPU commands in PyTorch, each designed to enhance your deep learning workflows by leveraging the power of GPUs. By the end of this guide, you'll have a solid understanding of how to efficiently use GPUs with PyTorch. Hereâ€™s an overview of what weâ€™ll cover:\n",
    "\n",
    "1. Checking GPU Availability\n",
    "2. Setting the Device\n",
    "3. Transferring Tensors to GPU\n",
    "4. Transferring Models to GPU\n",
    "5. Clearing GPU Cache\n",
    "6. Storing GPU in a Variable\n",
    "7. Optimizing GPU Usage\n",
    "8. Monitoring GPU Utilization\n",
    "9. Best Practices for GPU Usage\n",
    "10. Troubleshooting GPU Issues\n",
    "\n",
    "## Why This Guide?\n",
    "\n",
    "- **Step-by-Step Tutorials:** Each section includes clear explanations followed by practical examples, ensuring you not only learn but also apply your knowledge.\n",
    "- **Interactive Learning:** Engage with interactive code cells that allow you to see the effects of pip commands in real-time, facilitating a hands-on learning experience.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- **Run the Cells:** Follow along with the code examples by running the cells yourself. Experiment with different commands to see their effects and deepen your understanding.\n",
    "- **Interactive Examples:** Each section contains interactive examples that you can modify and run to see immediate results.\n",
    "- **Explore Further:** After completing the guided sections, try applying the commands to your own projects to reinforce your learning and adapt the techniques to your specific needs.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610b223",
   "metadata": {
    "papermill": {
     "duration": 0.005441,
     "end_time": "2024-07-22T03:00:28.693689",
     "exception": false,
     "start_time": "2024-07-22T03:00:28.688248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Checking GPU Availability\n",
    "\n",
    "To check if a GPU is available, use the following command. This is crucial in scenarios where code needs to adapt dynamically to the available hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909d30ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:28.706422Z",
     "iopub.status.busy": "2024-07-22T03:00:28.706055Z",
     "iopub.status.idle": "2024-07-22T03:00:32.405128Z",
     "shell.execute_reply": "2024-07-22T03:00:32.404064Z"
    },
    "papermill": {
     "duration": 3.708176,
     "end_time": "2024-07-22T03:00:32.407350",
     "exception": false,
     "start_time": "2024-07-22T03:00:28.699174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU Available:\", gpu_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d991",
   "metadata": {
    "papermill": {
     "duration": 0.005394,
     "end_time": "2024-07-22T03:00:32.418361",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.412967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Setting the Device\n",
    "\n",
    "Setting the device for computations ensures your model and tensors run on the GPU if available, which is vital for large-scale deep learning tasks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a11838c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:32.430884Z",
     "iopub.status.busy": "2024-07-22T03:00:32.430477Z",
     "iopub.status.idle": "2024-07-22T03:00:32.435757Z",
     "shell.execute_reply": "2024-07-22T03:00:32.434791Z"
    },
    "papermill": {
     "duration": 0.01387,
     "end_time": "2024-07-22T03:00:32.437666",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.423796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b02376",
   "metadata": {
    "papermill": {
     "duration": 0.005257,
     "end_time": "2024-07-22T03:00:32.448429",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.443172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Transferring Tensors to GPU\n",
    "\n",
    "Transferring tensors to the GPU accelerates operations, significantly reducing computation time in training loops:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5fed68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:32.460744Z",
     "iopub.status.busy": "2024-07-22T03:00:32.460312Z",
     "iopub.status.idle": "2024-07-22T03:00:32.655014Z",
     "shell.execute_reply": "2024-07-22T03:00:32.653885Z"
    },
    "papermill": {
     "duration": 0.203299,
     "end_time": "2024-07-22T03:00:32.657249",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.453950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a sample tensor and transfer it to the GPU\n",
    "tensor = torch.randn(10)\n",
    "tensor = tensor.to(device)\n",
    "print(\"Tensor on device:\", tensor.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dde22a",
   "metadata": {
    "papermill": {
     "duration": 0.005492,
     "end_time": "2024-07-22T03:00:32.668641",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.663149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Transferring Models to GPU\n",
    "\n",
    "Transferring models to the GPU is essential for leveraging GPU acceleration during training and inference, enhancing performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345144df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:32.681371Z",
     "iopub.status.busy": "2024-07-22T03:00:32.680818Z",
     "iopub.status.idle": "2024-07-22T03:00:32.698036Z",
     "shell.execute_reply": "2024-07-22T03:00:32.697169Z"
    },
    "papermill": {
     "duration": 0.025546,
     "end_time": "2024-07-22T03:00:32.699904",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.674358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a sample model and transfer it to the GPU\n",
    "model = nn.Linear(10, 1)\n",
    "model = model.to(device)\n",
    "print(\"Model on device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8184b89",
   "metadata": {
    "papermill": {
     "duration": 0.005523,
     "end_time": "2024-07-22T03:00:32.711180",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.705657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Clearing GPU Cache\n",
    "\n",
    "Clearing the GPU cache helps manage memory usage, preventing out-of-memory errors during extensive training sessions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dd3dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:32.723746Z",
     "iopub.status.busy": "2024-07-22T03:00:32.723485Z",
     "iopub.status.idle": "2024-07-22T03:00:32.727847Z",
     "shell.execute_reply": "2024-07-22T03:00:32.726977Z"
    },
    "papermill": {
     "duration": 0.013397,
     "end_time": "2024-07-22T03:00:32.730262",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.716865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clear the GPU cache to free up unused memory\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc120b8",
   "metadata": {
    "papermill": {
     "duration": 0.005485,
     "end_time": "2024-07-22T03:00:32.741442",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.735957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Storing GPU in a Variable\n",
    "\n",
    "Storing the GPU in a variable allows easy reference throughout your code, simplifying device management in complex workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d226a209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:32.753873Z",
     "iopub.status.busy": "2024-07-22T03:00:32.753624Z",
     "iopub.status.idle": "2024-07-22T03:00:32.758528Z",
     "shell.execute_reply": "2024-07-22T03:00:32.757365Z"
    },
    "papermill": {
     "duration": 0.014653,
     "end_time": "2024-07-22T03:00:32.761833",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.747180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored GPU device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Store the GPU device in a variable for easy access\n",
    "gpu = torch.device(\"cuda\")\n",
    "print(\"Stored GPU device:\", gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad541df",
   "metadata": {
    "papermill": {
     "duration": 0.006779,
     "end_time": "2024-07-22T03:00:32.777361",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.770582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Optimizing GPU Usage\n",
    "\n",
    "Mixed precision training optimizes GPU usage by reducing memory footprint and speeding up computations, crucial for large-scale models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cd10c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:32.794429Z",
     "iopub.status.busy": "2024-07-22T03:00:32.793995Z",
     "iopub.status.idle": "2024-07-22T03:00:34.497763Z",
     "shell.execute_reply": "2024-07-22T03:00:34.496571Z"
    },
    "papermill": {
     "duration": 1.714886,
     "end_time": "2024-07-22T03:00:34.500004",
     "exception": false,
     "start_time": "2024-07-22T03:00:32.785118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization with mixed precision completed.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Sample model, input, target, loss function, and optimizer\n",
    "model = nn.Linear(10, 1).to(device)\n",
    "input = torch.randn(10, device=device)\n",
    "target = torch.randn(1, device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Mixed precision training loop\n",
    "with autocast():\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "print(\"Optimization with mixed precision completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857699e7",
   "metadata": {
    "papermill": {
     "duration": 0.005732,
     "end_time": "2024-07-22T03:00:34.512695",
     "exception": false,
     "start_time": "2024-07-22T03:00:34.506963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Monitoring GPU Utilization\n",
    "\n",
    "Monitoring GPU utilization helps in identifying bottlenecks and optimizing performance by understanding resource usage patterns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093de246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:34.525980Z",
     "iopub.status.busy": "2024-07-22T03:00:34.525579Z",
     "iopub.status.idle": "2024-07-22T03:00:35.615903Z",
     "shell.execute_reply": "2024-07-22T03:00:35.614914Z"
    },
    "papermill": {
     "duration": 1.099622,
     "end_time": "2024-07-22T03:00:35.618331",
     "exception": false,
     "start_time": "2024-07-22T03:00:34.518709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 22 03:00:35 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   45C    P0             28W /   70W |     161MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   46C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Command to monitor GPU utilization\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1aabdd",
   "metadata": {
    "papermill": {
     "duration": 0.006476,
     "end_time": "2024-07-22T03:00:35.631244",
     "exception": false,
     "start_time": "2024-07-22T03:00:35.624768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Best Practices for GPU Usage\n",
    "\n",
    "- **Inference Efficiency:** Use `torch.no_grad()` to avoid unnecessary computations during inference, saving GPU memory and compute resources.\n",
    "- **Multi-GPU Training:** Prefer `DataParallel` for distributing workload across multiple GPUs, increasing training speed.\n",
    "- **Profiling:** Regularly profile your GPU usage with tools like NVIDIA Nsight to identify and optimize bottlenecks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d320e82",
   "metadata": {
    "papermill": {
     "duration": 0.006152,
     "end_time": "2024-07-22T03:00:35.643815",
     "exception": false,
     "start_time": "2024-07-22T03:00:35.637663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Troubleshooting GPU Issues\n",
    "\n",
    "If you encounter issues with GPU usage, consider the following steps:\n",
    "- **Driver Compatibility:** Ensure you have the correct GPU drivers installed.\n",
    "- **Version Check:** Verify that your CUDA and PyTorch versions are compatible.\n",
    "- **Memory Management:** Check for sufficient GPU memory and free up resources if needed.\n",
    "- **Memory Report:** Use `torch.cuda.memory_summary()` to get a detailed report on GPU memory usage, which helps in diagnosing memory-related issues:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a459f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:00:35.658166Z",
     "iopub.status.busy": "2024-07-22T03:00:35.657749Z",
     "iopub.status.idle": "2024-07-22T03:00:35.663916Z",
     "shell.execute_reply": "2024-07-22T03:00:35.663062Z"
    },
    "papermill": {
     "duration": 0.01689,
     "end_time": "2024-07-22T03:00:35.666900",
     "exception": false,
     "start_time": "2024-07-22T03:00:35.650010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  16645 KiB |  16647 KiB |  16655 KiB |  10240 B   |\n",
      "|       from large pool |  16640 KiB |  16640 KiB |  16640 KiB |      0 B   |\n",
      "|       from small pool |      5 KiB |      7 KiB |     15 KiB |  10240 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  16645 KiB |  16647 KiB |  16655 KiB |  10240 B   |\n",
      "|       from large pool |  16640 KiB |  16640 KiB |  16640 KiB |      0 B   |\n",
      "|       from small pool |      5 KiB |      7 KiB |     15 KiB |  10240 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  16640 KiB |  16640 KiB |  16640 KiB |    166 B   |\n",
      "|       from large pool |  16640 KiB |  16640 KiB |  16640 KiB |      0 B   |\n",
      "|       from small pool |      0 KiB |      0 KiB |      0 KiB |    166 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |\n",
      "|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   5882 KiB |  14203 KiB |  14217 KiB |   8335 KiB |\n",
      "|       from large pool |   3840 KiB |  12160 KiB |  12160 KiB |   8320 KiB |\n",
      "|       from small pool |   2042 KiB |   2047 KiB |   2057 KiB |     15 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      13    |      17    |      33    |      20    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |      11    |      15    |      31    |      20    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      13    |      17    |      33    |      20    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |      11    |      15    |      31    |      20    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |       4    |      11    |       9    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       3    |      10    |       9    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a detailed report on GPU memory usage\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971a192",
   "metadata": {
    "papermill": {
     "duration": 0.00647,
     "end_time": "2024-07-22T03:00:35.680204",
     "exception": false,
     "start_time": "2024-07-22T03:00:35.673734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Continuous Improvement\n",
    "\n",
    "This notebook is a living document and will be continuously updated based on your recommendations and feedback. If you have any suggestions for additional content, improvements, or corrections, please feel free to share them. Your input is invaluable in making this guide more comprehensive and useful for everyone.\n",
    "\n",
    "Thank you for your contributions and for helping to make this resource better!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084808c",
   "metadata": {
    "papermill": {
     "duration": 0.006362,
     "end_time": "2024-07-22T03:00:35.693146",
     "exception": false,
     "start_time": "2024-07-22T03:00:35.686784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect with Me\n",
    "\n",
    "Stay updated with my latest insights and projects! Connect with me on:\n",
    "\n",
    "### Medium\n",
    "[matinmahmoudi.com](https://matinmahmoudi.com)  \n",
    "Dive into my detailed articles and tutorials on data science.\n",
    "\n",
    "### LinkedIn\n",
    "[linkedin.com/in/matinmahmoudicom](https://www.linkedin.com/in/matinmahmoudicom/)  \n",
    "Let's network and share professional experiences.\n",
    "\n",
    "### GitHub\n",
    "[github.com/matinmahmoudi](https://github.com/matinmahmoudi)  \n",
    "Explore my open-source projects and code repositories.\n",
    "\n",
    "Thanks for reading! ðŸ˜Š\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.106009,
   "end_time": "2024-07-22T03:00:36.921415",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-22T03:00:25.815406",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
