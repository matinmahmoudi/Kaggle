{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e68d35",
   "metadata": {
    "papermill": {
     "duration": 0.009185,
     "end_time": "2024-06-20T12:26:33.232639",
     "exception": false,
     "start_time": "2024-06-20T12:26:33.223454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ”ï¸ ðŸ§© Complete Guide to Statistical Testing A to Z\n",
    "\n",
    "Welcome to this comprehensive guide on statistical testing! This notebook is designed to equip you with everything you need to know, from basic concepts to advanced applications in data science. Whether you're just starting out or you're a seasoned professional looking to sharpen your skills, this guide is for you!\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "We'll dive into a variety of statistical tests, each with a unique purpose in data analysis. By the end, you'll have a toolkit ready to tackle any data-driven challenge. Here's a peek at what we'll cover:\n",
    "\n",
    "- **Chi-Square Test:** Learn how to test relationships between categorical variables.\n",
    "- **Two-Sample T-Test & Paired T-Test:** Discover how to compare means from different groups and understand the statistical significance of their differences.\n",
    "- **ANOVA (Analysis of Variance):** Explore how to test differences across multiple groups simultaneously.\n",
    "- **Test of Correlation:** Uncover relationships and associations between continuous variables.\n",
    "\n",
    "## Why This Guide?\n",
    "\n",
    "- **Step-by-Step Tutorials:** Each section includes clear explanations followed by practical examples, ensuring you not only learn but also apply your knowledge.\n",
    "- **Interactive Learning:** Engage with interactive code cells that allow you to see the effects of statistical tests in real-time.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- **Run the Cells:** Follow along with the code examples by running the cells yourself. Play around with the parameters to see how the results change.\n",
    "- **Explore Further:** After completing the guided sections, try applying the tests to your own datasets to reinforce your learning.\n",
    "\n",
    "Get ready to unlock the full potential of statistical testing in data science. Let's dive in and turn data into decisions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f32f1",
   "metadata": {
    "papermill": {
     "duration": 0.008233,
     "end_time": "2024-06-20T12:26:33.249868",
     "exception": false,
     "start_time": "2024-06-20T12:26:33.241635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Description\n",
    "\n",
    "The dataset used in this notebook is `Sales_and_Satisfaction_V1.3.csv`. It contains information on sales and customer satisfaction before and after an intervention, along with purchase behavior. Below is a detailed description of each column, including the data types and a brief explanation.\n",
    "\n",
    "### Columns:\n",
    "\n",
    "1. **Group** (object)\n",
    "   - Indicates whether the entry is part of the control or treatment group.\n",
    "   - Example values: `Control`, `Treatment`\n",
    "   \n",
    "\n",
    "2. **Customer_Segment** (object)\n",
    "   - Segments customers into different value groups.\n",
    "   - Example values: `High Value`, `Medium Value`, `Low Value`\n",
    "   \n",
    "\n",
    "3. **Sales_Before** (float64)\n",
    "   - Sales amount before the intervention.\n",
    "   - Example values: `216.21`, `225.09`\n",
    "   \n",
    "\n",
    "4. **Sales_After** (float64)\n",
    "   - Sales amount after the intervention.\n",
    "   - Example values: `246.87`, `257.57`\n",
    "   \n",
    "\n",
    "5. **Customer_Satisfaction_Before** (float64)\n",
    "   - Customer satisfaction score before the intervention.\n",
    "   - Example values: `58.93`, `84.71`\n",
    "   \n",
    "\n",
    "6. **Customer_Satisfaction_After** (float64)\n",
    "   - Customer satisfaction score after the intervention.\n",
    "   - Example values: `61.60`, `83.84`\n",
    "   \n",
    "\n",
    "7. **Purchase_Made** (object)\n",
    "   - Indicates whether a purchase was made.\n",
    "   - Example values: `Yes`, `No`\n",
    "   \n",
    "\n",
    "### Dataset Overview:\n",
    "\n",
    "- **Total Entries:** 10,000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aaa0c",
   "metadata": {
    "papermill": {
     "duration": 0.008427,
     "end_time": "2024-06-20T12:26:33.267109",
     "exception": false,
     "start_time": "2024-06-20T12:26:33.258682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chi-Square Test\n",
    "\n",
    "The Chi-Square Test determines whether there is a significant association between two categorical variables.\n",
    "\n",
    "## What is the Chi-Square Test?\n",
    "\n",
    "A non-parametric test that assesses whether observed frequency distributions align with expected distributions under the null hypothesis.\n",
    "\n",
    "## When to Use the Chi-Square Test\n",
    "\n",
    "- Variables are categorical.\n",
    "- Sample data is randomly drawn.\n",
    "- Expected frequency for each cell is at least 5.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. Data should be randomly sampled.\n",
    "2. Each expected frequency should be at least 5. If not, use Fisher's Exact Test for small sample sizes.\n",
    "\n",
    "## Hypotheses for Chi-Square Test for Independence\n",
    "\n",
    "- **Null Hypothesis (H0)**: No association between the categorical variables (independent).\n",
    "- **Alternative Hypothesis (H1)**: An association exists between the categorical variables (not independent).\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. **Create a Contingency Table**: Displays the frequency distribution of variables.\n",
    "2. **Calculate the Chi-Square Statistic**:\n",
    "\n",
    "   $$\n",
    "   \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "   $$\n",
    "   \n",
    "   where \\( O_i \\) is the observed frequency and \\( E_i \\) is the expected frequency.\n",
    "3. **Determine the p-value**: Compare the Chi-Square statistic to a Chi-Square distribution with degrees of freedom:\n",
    "\n",
    "   $$\n",
    "   df = (r-1) \\times (c-1)\n",
    "   $$\n",
    "   \n",
    "4. **Interpret the Results**: If the p-value is â‰¤ 0.05, reject the null hypothesis.\n",
    "\n",
    "## Sample Size Considerations\n",
    "\n",
    "Each statistical test has a unique sample size formula. However, to avoid complexity in some exploratory data analysis (EDA) tasks, we use a sample size of 10,000, which is generally sufficient for robust statistical testing.\n",
    "\n",
    "## Handling Multiple Groups\n",
    "\n",
    "When we have more than two groups in each variable, it may be better to merge them based on residuals to simplify the analysis, especially when the sample size is small and the expected frequencies are less than 5 in some cells. The residuals help us identify which groups can be combined without losing significant information.\n",
    "\n",
    "### Calculating Residuals\n",
    "\n",
    "Residuals help in understanding the contribution of each cell to the overall Chi-Square statistic. They are calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Residual} = O_i - E_i\n",
    "$$\n",
    "\n",
    "Where \\( O_i \\) is the observed frequency and \\( E_i \\) is the expected frequency.\n",
    "\n",
    "## Odds Ratio\n",
    "\n",
    "The odds ratio measures the association between two categorical variables. It represents the odds that an outcome will occur given a particular exposure compared to the odds of the outcome occurring without that exposure.\n",
    "\n",
    "### When to Use the Odds Ratio\n",
    "\n",
    "The odds ratio is particularly useful when you have found a significant association between two categorical variables using the Chi-Square Test or Fisher's Exact Test. It helps to quantify the strength and direction of the association.\n",
    "\n",
    "### Calculating the Odds Ratio\n",
    "\n",
    "For a 2x2 contingency table:\n",
    "\n",
    "|                | Outcome Present (Yes) | Outcome Absent (No) |\n",
    "|----------------|------------------------|---------------------|\n",
    "| Exposure (Yes) | \\( a \\)                | \\( b \\)             |\n",
    "| No Exposure    | \\( c \\)                | \\( d \\)             |\n",
    "\n",
    "The odds ratio (OR) is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Odds Ratio} = \\frac{(a \\cdot d)}{(b \\cdot c)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( a \\) = Number of cases with exposure and outcome present\n",
    "- \\( b \\) = Number of cases with exposure and outcome absent\n",
    "- \\( c \\) = Number of cases without exposure and outcome present\n",
    "- \\( d \\) = Number of cases without exposure and outcome absent\n",
    "\n",
    "### Interpretation of the Odds Ratio\n",
    "\n",
    "- **Odds Ratio > 1**: Positive association between the variables.\n",
    "- **Odds Ratio = 1**: No association between the variables.\n",
    "- **Odds Ratio < 1**: Negative association between the variables.\n",
    "\n",
    "## Example: Chi-Square Test for Independence\n",
    "\n",
    "Using `Group` and `Purchase_Made` variables:\n",
    "\n",
    "- **Null Hypothesis (H0)**: No association between `Group` and `Purchase_Made`.\n",
    "- **Alternative Hypothesis (H1)**: An association exists between `Group` and `Purchase_Made`.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Create a Contingency Table**: Summarize the data.\n",
    "2. **Calculate the Chi-Square Statistic**: Use observed and expected frequencies.\n",
    "3. **Determine the p-value**: Compare the statistic to the Chi-Square distribution.\n",
    "4. **Interpret the Results**: Based on the p-value.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Chi-Square Statistic**: Measures the discrepancy between observed and expected frequencies.\n",
    "- **p-value**: Probability of observing the test results under the null hypothesis.\n",
    "- **Degrees of Freedom**: Calculated as (rows - 1) * (columns - 1).\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating a significant association. Otherwise, fail to reject the null hypothesis.\n",
    "\n",
    "### Odds Ratio Interpretation\n",
    "\n",
    "- **Odds Ratio > 1**: Positive association between `Group` and `Purchase_Made`.\n",
    "- **Odds Ratio = 1**: No association between `Group` and `Purchase_Made`.\n",
    "- **Odds Ratio < 1**: Negative association between `Group` and `Purchase_Made`.\n",
    "\n",
    "## Fisher's Exact Test\n",
    "\n",
    "Fisher's Exact Test is an alternative to the Chi-Square Test when the sample size is small, or the expected frequency assumption is not met. Though we meet the conditions for the Chi-Square Test in this example, we'll demonstrate Fisher's Exact Test for practice purposes. Note that Fisher's Exact Test is only applicable for 2x2 contingency tables.\n",
    "\n",
    "### Example: Fisher's Exact Test\n",
    "\n",
    "Using the same `Group` and `Purchase_Made` variables:\n",
    "\n",
    "- **Null Hypothesis (H0)**: No association between `Group` and `Purchase_Made`.\n",
    "- **Alternative Hypothesis (H1)**: An association exists between `Group` and `Purchase_Made`.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Create a Contingency Table**: Summarize the data.\n",
    "2. **Perform Fisher's Exact Test**: Calculate the exact p-value.\n",
    "3. **Interpret the Results**: Based on the p-value.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **p-value**: Provides the probability of observing the test results under the null hypothesis.\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating a significant association. Otherwise, fail to reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9428f559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:33.287120Z",
     "iopub.status.busy": "2024-06-20T12:26:33.286499Z",
     "iopub.status.idle": "2024-06-20T12:26:36.027756Z",
     "shell.execute_reply": "2024-06-20T12:26:36.026395Z"
    },
    "papermill": {
     "duration": 2.755386,
     "end_time": "2024-06-20T12:26:36.031294",
     "exception": false,
     "start_time": "2024-06-20T12:26:33.275908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Checking Assumptions...\n",
      "\n",
      "Contingency Table:\n",
      "Purchase_Made   No  Yes\n",
      "Group                  \n",
      "Control        248  252\n",
      "Treatment      237  263\n",
      "\n",
      "Expected Frequencies:\n",
      "Purchase_Made     No    Yes\n",
      "Group                      \n",
      "Control        242.5  257.5\n",
      "Treatment      242.5  257.5\n",
      "\n",
      "All expected frequencies are at least 5. Assumptions are satisfied.\n",
      "\n",
      "Chi-Square Test Results:\n",
      "Chi-Square Statistic: 0.4004\n",
      "p-value: 0.5269\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. There is no significant association between Group and Purchase_Made.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check the assumptions\n",
    "print(\"\\nChecking Assumptions...\")\n",
    "\n",
    "# Assumption 1: Random Sampling\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Assumption 2: Expected Frequency\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(sampled_data['Group'], sampled_data['Purchase_Made'])\n",
    "\n",
    "# Display the contingency table\n",
    "print(\"\\nContingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Perform the Chi-Square test and check the expected frequencies\n",
    "chi2_result = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "expected_frequencies = chi2_result.expected_freq\n",
    "print(\"\\nExpected Frequencies:\")\n",
    "print(pd.DataFrame(expected_frequencies, index=contingency_table.index, columns=contingency_table.columns))\n",
    "\n",
    "# Check if all expected frequencies are at least 5\n",
    "if (expected_frequencies < 5).any():\n",
    "    print(\"\\nWarning: Some expected frequencies are less than 5. Switching to Fisher's Exact Test.\")\n",
    "    \n",
    "    # Perform Fisher's Exact Test\n",
    "    if contingency_table.shape == (2, 2):\n",
    "        fisher_result = stats.fisher_exact(contingency_table)\n",
    "        odds_ratio, fisher_p_value = fisher_result\n",
    "\n",
    "        print(\"\\nFisher's Exact Test Results:\")\n",
    "        print(f\"Odds Ratio: {odds_ratio:.4f}\")\n",
    "        print(f\"p-value: {fisher_p_value:.4f}\")\n",
    "\n",
    "        # Interpret the p-value from Fisher's Exact Test\n",
    "        alpha = 0.05\n",
    "        print(f\"\\nSignificance Level: {alpha}\")\n",
    "        if fisher_p_value < alpha:\n",
    "            print(\"Conclusion: Reject the null hypothesis. There is a significant association between Group and Purchase_Made (Fisher's Exact Test).\")\n",
    "        else:\n",
    "            print(\"Conclusion: Fail to reject the null hypothesis. There is no significant association between Group and Purchase_Made (Fisher's Exact Test).\")\n",
    "    else:\n",
    "        print(\"\\nFisher's Exact Test is not applicable for this contingency table size.\")\n",
    "else:\n",
    "    print(\"\\nAll expected frequencies are at least 5. Assumptions are satisfied.\")\n",
    "    \n",
    "    # Display the results of the Chi-Square test\n",
    "    chi2_statistic = chi2_result.statistic\n",
    "    p_value = chi2_result.pvalue\n",
    "    degrees_of_freedom = chi2_result.dof\n",
    "    print(\"\\nChi-Square Test Results:\")\n",
    "    print(f\"Chi-Square Statistic: {chi2_statistic:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    print(f\"Degrees of Freedom: {degrees_of_freedom}\")\n",
    "\n",
    "    # Interpret the p-value\n",
    "    alpha = 0.05\n",
    "    print(f\"\\nSignificance Level: {alpha}\")\n",
    "    if p_value < alpha:\n",
    "        print(\"Conclusion: Reject the null hypothesis. There is a significant association between Group and Purchase_Made.\")\n",
    "        \n",
    "        # Calculate the odds ratio for a 2x2 contingency table\n",
    "        if contingency_table.shape == (2, 2):\n",
    "            odds_ratio = (contingency_table.iloc[0, 0] * contingency_table.iloc[1, 1]) / (contingency_table.iloc[0, 1] * contingency_table.iloc[1, 0])\n",
    "            print(f\"\\nOdds Ratio: {odds_ratio:.4f}\")\n",
    "        else:\n",
    "            print(\"\\nOdds Ratio calculation is only applicable for 2x2 contingency tables.\")\n",
    "    else:\n",
    "        print(\"Conclusion: Fail to reject the null hypothesis. There is no significant association between Group and Purchase_Made.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f91d95",
   "metadata": {
    "papermill": {
     "duration": 0.00913,
     "end_time": "2024-06-20T12:26:36.049726",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.040596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Chi-Square Test for Customer Segment and Purchase Made\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Create a Contingency Table**: Summarize the data for `Customer_Segment` and `Purchase_Made`.\n",
    "2. **Normalize the Contingency Table**: Check the proportions before the test.\n",
    "3. **Calculate the Chi-Square Statistic**: Use observed and expected frequencies.\n",
    "4. **Calculate Residuals**: Check which cells contribute most to the Chi-Square statistic.\n",
    "5. **Check Expected Frequencies**: Ensure all expected frequencies are at least 5.\n",
    "6. **Interpret the Results**: Based on the p-value from the Chi-Square test or Fisher's Exact Test if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a033f9d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:36.070460Z",
     "iopub.status.busy": "2024-06-20T12:26:36.069945Z",
     "iopub.status.idle": "2024-06-20T12:26:36.157269Z",
     "shell.execute_reply": "2024-06-20T12:26:36.155684Z"
    },
    "papermill": {
     "duration": 0.102404,
     "end_time": "2024-06-20T12:26:36.161214",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.058810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Contingency Table:\n",
      "Purchase_Made      No  Yes\n",
      "Customer_Segment          \n",
      "High Value        162  178\n",
      "Low Value         158  170\n",
      "Medium Value      165  167\n",
      "\n",
      "Normalized Contingency Table:\n",
      "Purchase_Made           No       Yes\n",
      "Customer_Segment                    \n",
      "High Value        0.476471  0.523529\n",
      "Low Value         0.481707  0.518293\n",
      "Medium Value      0.496988  0.503012\n",
      "\n",
      "Expected Frequencies:\n",
      "Purchase_Made         No     Yes\n",
      "Customer_Segment                \n",
      "High Value        164.90  175.10\n",
      "Low Value         159.08  168.92\n",
      "Medium Value      161.02  170.98\n",
      "\n",
      "Residuals:\n",
      "Purchase_Made       No   Yes\n",
      "Customer_Segment            \n",
      "High Value       -2.90  2.90\n",
      "Low Value        -1.08  1.08\n",
      "Medium Value      3.98 -3.98\n",
      "\n",
      "All expected frequencies are at least 5. Assumptions are satisfied.\n",
      "\n",
      "Chi-Square Test Results:\n",
      "Chi-Square Statistic: 0.3043\n",
      "p-value: 0.8589\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. There is no significant association between Customer_Segment and Purchase_Made.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Create a contingency table for Customer_Segment and Purchase_Made\n",
    "contingency_table = pd.crosstab(sampled_data['Customer_Segment'], sampled_data['Purchase_Made'])\n",
    "\n",
    "# Display the contingency table\n",
    "print(\"\\nContingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Display the normalized contingency table\n",
    "normalized_contingency_table = pd.crosstab(sampled_data['Customer_Segment'], sampled_data['Purchase_Made'], normalize='index')\n",
    "print(\"\\nNormalized Contingency Table:\")\n",
    "print(normalized_contingency_table)\n",
    "\n",
    "# Perform the Chi-Square test and check the expected frequencies\n",
    "chi2_result = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "expected_frequencies = chi2_result.expected_freq\n",
    "print(\"\\nExpected Frequencies:\")\n",
    "print(pd.DataFrame(expected_frequencies, index=contingency_table.index, columns=contingency_table.columns))\n",
    "\n",
    "# Calculate residuals (usually calculated when rejecting H0, but shown here for educational purposes)\n",
    "residuals = contingency_table - expected_frequencies\n",
    "print(\"\\nResiduals:\")\n",
    "print(residuals)\n",
    "\n",
    "# Check if all expected frequencies are at least 5\n",
    "if (expected_frequencies < 5).any():\n",
    "    print(\"\\nWarning: Some expected frequencies are less than 5. Switching to Fisher's Exact Test.\")\n",
    "    \n",
    "    # Perform Fisher's Exact Test\n",
    "    if contingency_table.shape == (2, 2):\n",
    "        fisher_result = stats.fisher_exact(contingency_table)\n",
    "        odds_ratio, fisher_p_value = fisher_result\n",
    "\n",
    "        print(\"\\nFisher's Exact Test Results:\")\n",
    "        print(f\"Odds Ratio: {odds_ratio:.4f}\")\n",
    "        print(f\"p-value: {fisher_p_value:.4f}\")\n",
    "\n",
    "        # Interpret the p-value from Fisher's Exact Test\n",
    "        alpha = 0.05\n",
    "        print(f\"\\nSignificance Level: {alpha}\")\n",
    "        if fisher_p_value < alpha:\n",
    "            print(\"Conclusion: Reject the null hypothesis. There is a significant association between Customer_Segment and Purchase_Made (Fisher's Exact Test).\")\n",
    "        else:\n",
    "            print(\"Conclusion: Fail to reject the null hypothesis. There is no significant association between Customer_Segment and Purchase_Made (Fisher's Exact Test).\")\n",
    "    else:\n",
    "        print(\"\\nFisher's Exact Test is not applicable for this contingency table size.\")\n",
    "else:\n",
    "    print(\"\\nAll expected frequencies are at least 5. Assumptions are satisfied.\")\n",
    "    \n",
    "    # Display the results of the Chi-Square test\n",
    "    chi2_statistic = chi2_result.statistic\n",
    "    p_value = chi2_result.pvalue\n",
    "    degrees_of_freedom = chi2_result.dof\n",
    "    print(\"\\nChi-Square Test Results:\")\n",
    "    print(f\"Chi-Square Statistic: {chi2_statistic:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    print(f\"Degrees of Freedom: {degrees_of_freedom}\")\n",
    "\n",
    "    # Interpret the p-value\n",
    "    alpha = 0.05\n",
    "    print(f\"\\nSignificance Level: {alpha}\")\n",
    "    if p_value < alpha:\n",
    "        print(\"Conclusion: Reject the null hypothesis. There is a significant association between Customer_Segment and Purchase_Made.\")\n",
    "        \n",
    "        # Calculate the odds ratio for a 2x2 contingency table\n",
    "        if contingency_table.shape == (2, 2):\n",
    "            odds_ratio = (contingency_table.iloc[0, 0] * contingency_table.iloc[1, 1]) / (contingency_table.iloc[0, 1] * contingency_table.iloc[1, 0])\n",
    "            print(f\"\\nOdds Ratio: {odds_ratio:.4f}\")\n",
    "        else:\n",
    "            print(\"\\nOdds Ratio calculation is only applicable for 2x2 contingency tables.\")\n",
    "    else:\n",
    "        print(\"Conclusion: Fail to reject the null hypothesis. There is no significant association between Customer_Segment and Purchase_Made.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcba958",
   "metadata": {
    "papermill": {
     "duration": 0.008678,
     "end_time": "2024-06-20T12:26:36.179496",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.170818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Independent Samples T-Test (Two-Sample T-Test)\n",
    "\n",
    "The Independent Samples T-Test, also known as the Two-Sample T-Test, determines whether there is a statistically significant difference between the means of two independent groups.\n",
    "\n",
    "## What is the Independent Samples T-Test?\n",
    "\n",
    "A parametric test that compares the means of two independent groups to see if they are significantly different from each other.\n",
    "\n",
    "## When to Use the Independent Samples T-Test\n",
    "\n",
    "- The dependent variable is continuous.\n",
    "- The independent variable consists of two categorical, independent groups.\n",
    "- Observations are independent of each other.\n",
    "- The dependent variable is approximately normally distributed in each group.\n",
    "- Homogeneity of variances: The variances in the two groups are equal.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. The dependent variable is normally distributed in each group.\n",
    "   - If the sample size is large (n > 30), we can skip the normality check due to the Central Limit Theorem, which states that the sampling distribution of the mean will be approximately normal regardless of the distribution of the population, provided the sample size is sufficiently large.\n",
    "   - If normality is not met and the sample size is small, consider using a non-parametric test such as the Mann-Whitney U Test.\n",
    "\n",
    "\n",
    "2. Homogeneity of variances.\n",
    "   - If homogeneity of variances is not met, use Welch's T-Test, which does not assume equal variances.\n",
    "\n",
    "## Hypotheses for Independent Samples T-Test\n",
    "\n",
    "- **Null Hypothesis (H0)**: The means of the two groups are equal.\n",
    "- **Alternative Hypothesis (H1)**: The means of the two groups are not equal.\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. **Check Assumptions**: Verify normality (if the sample size is small) and homogeneity of variances.\n",
    "2. **Calculate the T-Test Statistic**:\n",
    "\n",
    "   $$\n",
    "   t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n",
    "   $$\n",
    "\n",
    "   where $(\\bar{X}_1)$ and $(\\bar{X}_2)$ are the sample means, and $s_p$ is the pooled standard deviation calculated as:\n",
    "\n",
    "   $$\n",
    "   s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n",
    "   $$\n",
    "\n",
    "3. **Determine the p-value**: Compare the T-Test statistic to a t-distribution with degrees of freedom:\n",
    "\n",
    "   $$\n",
    "   df = n_1 + n_2 - 2\n",
    "   $$\n",
    "\n",
    "4. **Interpret the Results**: If the p-value is â‰¤ 0.05, reject the null hypothesis.\n",
    "\n",
    "## Example: Independent Samples T-Test\n",
    "\n",
    "Using `Sales_Before` variable to compare `Control` and `Treatment` groups:\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean `Sales_Before` in the `Control` group is equal to the mean `Sales_Before` in the `Treatment` group.\n",
    "- **Alternative Hypothesis (H1)**: The mean `Sales_Before` in the `Control` group is not equal to the mean `Sales_Before` in the `Treatment` group.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Check Assumptions**: Verify normality (if sample size is small) and homogeneity of variances.\n",
    "2. **Calculate the T-Test Statistic**: Use the sample means, variances, and sizes.\n",
    "3. **Determine the p-value**: Compare the statistic to a t-distribution.\n",
    "4. **Interpret the Results**: Based on the p-value.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **T-Test Statistic**: Measures the difference between group means relative to the variability in the groups.\n",
    "- **p-value**: Probability of observing the test results under the null hypothesis.\n",
    "- **Degrees of Freedom**: Calculated using the formula above.\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating a significant difference between the means. Otherwise, fail to reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68672100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:36.199866Z",
     "iopub.status.busy": "2024-06-20T12:26:36.199351Z",
     "iopub.status.idle": "2024-06-20T12:26:36.253454Z",
     "shell.execute_reply": "2024-06-20T12:26:36.252100Z"
    },
    "papermill": {
     "duration": 0.068325,
     "end_time": "2024-06-20T12:26:36.256756",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.188431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Checking Assumptions...\n",
      "Normality Check: Skipped (sample size > 30, Central Limit Theorem applied)\n",
      "\n",
      "Homogeneity of Variances Test (Levene's Test):\n",
      "p-value: 0.9613\n",
      "\n",
      "Independent Samples T-Test Results:\n",
      "Test Statistic: -0.3620\n",
      "p-value: 0.7175\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. There is no significant difference in Sales_Before between Control and Treatment groups (using Independent Samples T-Test).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Extract relevant columns from the sampled data\n",
    "control_group = sampled_data[sampled_data['Group'] == 'Control']['Sales_Before']\n",
    "treatment_group = sampled_data[sampled_data['Group'] == 'Treatment']['Sales_Before']\n",
    "\n",
    "# Check assumptions\n",
    "print(\"\\nChecking Assumptions...\")\n",
    "\n",
    "# Assumption 1: Normality (skipped due to Central Limit Theorem if n > 30)\n",
    "if len(control_group) > 30 and len(treatment_group) > 30:\n",
    "    normality_check = True\n",
    "    print(\"Normality Check: Skipped (sample size > 30, Central Limit Theorem applied)\")\n",
    "else:\n",
    "    control_normality = stats.shapiro(control_group)\n",
    "    treatment_normality = stats.shapiro(treatment_group)\n",
    "    normality_check = (control_normality.pvalue > 0.05) and (treatment_normality.pvalue > 0.05)\n",
    "    print(\"\\nNormality Test (Shapiro-Wilk):\")\n",
    "    print(f\"Control group p-value: {control_normality.pvalue:.4f}\")\n",
    "    print(f\"Treatment group p-value: {treatment_normality.pvalue:.4f}\")\n",
    "\n",
    "# Assumption 2: Homogeneity of variances\n",
    "levene_test = stats.levene(control_group, treatment_group)\n",
    "homogeneity_check = levene_test.pvalue > 0.05\n",
    "print(\"\\nHomogeneity of Variances Test (Levene's Test):\")\n",
    "print(f\"p-value: {levene_test.pvalue:.4f}\")\n",
    "\n",
    "# Perform the appropriate T-Test based on the assumptions\n",
    "if normality_check:\n",
    "    if homogeneity_check:\n",
    "        # Perform the Independent Samples T-Test\n",
    "        t_test_result = stats.ttest_ind(control_group, treatment_group, equal_var=True)\n",
    "        test_used = \"Independent Samples T-Test\"\n",
    "    else:\n",
    "        # Perform Welch's T-Test\n",
    "        t_test_result = stats.ttest_ind(control_group, treatment_group, equal_var=False)\n",
    "        test_used = \"Welch's T-Test\"\n",
    "else:\n",
    "    # Perform the Mann-Whitney U Test\n",
    "    t_test_result = stats.mannwhitneyu(control_group, treatment_group)\n",
    "    test_used = \"Mann-Whitney U Test\"\n",
    "\n",
    "# Display the results of the T-Test\n",
    "t_statistic = t_test_result.statistic\n",
    "p_value = t_test_result.pvalue\n",
    "print(f\"\\n{test_used} Results:\")\n",
    "print(f\"Test Statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level: {alpha}\")\n",
    "if p_value < alpha:\n",
    "    print(f\"Conclusion: Reject the null hypothesis. There is a significant difference in Sales_Before between Control and Treatment groups (using {test_used}).\")\n",
    "else:\n",
    "    print(f\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in Sales_Before between Control and Treatment groups (using {test_used}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c421e",
   "metadata": {
    "papermill": {
     "duration": 0.008884,
     "end_time": "2024-06-20T12:26:36.275264",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.266380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Independent Samples T-Test (Two-Sample T-Test) by Customer Segment\n",
    "\n",
    "This analysis uses the Independent Samples T-Test to determine whether there is a statistically significant difference in `Sales_Before` between the `Control` and `Treatment` groups within each `Customer_Segment`.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Group by Customer Segment**: Divide the data into segments.\n",
    "2. **Check Assumptions**: Verify normality (if the sample size is small) and homogeneity of variances within each segment.\n",
    "3. **Perform the T-Test**: Apply the appropriate T-Test based on the assumptions.\n",
    "4. **Interpret the Results**: Determine if there is a significant difference in `Sales_Before` between the `Control` and `Treatment` groups within each segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88fda04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:36.296201Z",
     "iopub.status.busy": "2024-06-20T12:26:36.295654Z",
     "iopub.status.idle": "2024-06-20T12:26:36.359075Z",
     "shell.execute_reply": "2024-06-20T12:26:36.357699Z"
    },
    "papermill": {
     "duration": 0.078105,
     "end_time": "2024-06-20T12:26:36.362496",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.284391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Customer Segment: High Value\n",
      "\n",
      "Checking Assumptions...\n",
      "Normality Check: Skipped (sample size > 30, Central Limit Theorem applied)\n",
      "\n",
      "Homogeneity of Variances Test (Levene's Test):\n",
      "p-value: 0.8329\n",
      "\n",
      "Independent Samples T-Test Results:\n",
      "Test Statistic: 0.3070\n",
      "p-value: 0.7590\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. There is no significant difference in Sales_Before between Control and Treatment groups (using Independent Samples T-Test).\n",
      "\n",
      "Customer Segment: Low Value\n",
      "\n",
      "Checking Assumptions...\n",
      "Normality Check: Skipped (sample size > 30, Central Limit Theorem applied)\n",
      "\n",
      "Homogeneity of Variances Test (Levene's Test):\n",
      "p-value: 0.8100\n",
      "\n",
      "Independent Samples T-Test Results:\n",
      "Test Statistic: -1.4635\n",
      "p-value: 0.1443\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. There is no significant difference in Sales_Before between Control and Treatment groups (using Independent Samples T-Test).\n",
      "\n",
      "Customer Segment: Medium Value\n",
      "\n",
      "Checking Assumptions...\n",
      "Normality Check: Skipped (sample size > 30, Central Limit Theorem applied)\n",
      "\n",
      "Homogeneity of Variances Test (Levene's Test):\n",
      "p-value: 0.5761\n",
      "\n",
      "Independent Samples T-Test Results:\n",
      "Test Statistic: 0.3013\n",
      "p-value: 0.7634\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. There is no significant difference in Sales_Before between Control and Treatment groups (using Independent Samples T-Test).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Group by 'Customer_Segment'\n",
    "grouped_data = sampled_data.groupby('Customer_Segment')\n",
    "\n",
    "# Function to perform T-Test within each Customer Segment\n",
    "def perform_t_test(segment, segment_data):\n",
    "    print(f\"\\nCustomer Segment: {segment}\")\n",
    "    control_group = segment_data[segment_data['Group'] == 'Control']['Sales_Before']\n",
    "    treatment_group = segment_data[segment_data['Group'] == 'Treatment']['Sales_Before']\n",
    "\n",
    "    # Check assumptions\n",
    "    print(\"\\nChecking Assumptions...\")\n",
    "\n",
    "    # Assumption 1: Normality (skipped due to Central Limit Theorem if n > 30)\n",
    "    if len(control_group) > 30 and len(treatment_group) > 30:\n",
    "        normality_check = True\n",
    "        print(\"Normality Check: Skipped (sample size > 30, Central Limit Theorem applied)\")\n",
    "    else:\n",
    "        control_normality = stats.shapiro(control_group)\n",
    "        treatment_normality = stats.shapiro(treatment_group)\n",
    "        normality_check = (control_normality.pvalue > 0.05) and (treatment_normality.pvalue > 0.05)\n",
    "        print(\"\\nNormality Test (Shapiro-Wilk):\")\n",
    "        print(f\"Control group p-value: {control_normality.pvalue:.4f}\")\n",
    "        print(f\"Treatment group p-value: {treatment_normality.pvalue:.4f}\")\n",
    "\n",
    "    # Assumption 2: Homogeneity of variances\n",
    "    levene_test = stats.levene(control_group, treatment_group)\n",
    "    homogeneity_check = levene_test.pvalue > 0.05\n",
    "    print(\"\\nHomogeneity of Variances Test (Levene's Test):\")\n",
    "    print(f\"p-value: {levene_test.pvalue:.4f}\")\n",
    "\n",
    "    # Perform the appropriate T-Test based on the assumptions\n",
    "    if normality_check:\n",
    "        if homogeneity_check:\n",
    "            # Perform the Independent Samples T-Test\n",
    "            t_test_result = stats.ttest_ind(control_group, treatment_group, equal_var=True)\n",
    "            test_used = \"Independent Samples T-Test\"\n",
    "        else:\n",
    "            # Perform Welch's T-Test\n",
    "            t_test_result = stats.ttest_ind(control_group, treatment_group, equal_var=False)\n",
    "            test_used = \"Welch's T-Test\"\n",
    "    else:\n",
    "        # Perform the Mann-Whitney U Test\n",
    "        t_test_result = stats.mannwhitneyu(control_group, treatment_group)\n",
    "        test_used = \"Mann-Whitney U Test\"\n",
    "\n",
    "    # Display the results of the T-Test\n",
    "    t_statistic = t_test_result.statistic\n",
    "    p_value = t_test_result.pvalue\n",
    "    print(f\"\\n{test_used} Results:\")\n",
    "    print(f\"Test Statistic: {t_statistic:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "    # Interpret the p-value\n",
    "    alpha = 0.05\n",
    "    print(f\"\\nSignificance Level: {alpha}\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"Conclusion: Reject the null hypothesis. There is a significant difference in Sales_Before between Control and Treatment groups (using {test_used}).\")\n",
    "    else:\n",
    "        print(f\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in Sales_Before between Control and Treatment groups (using {test_used}).\")\n",
    "\n",
    "# Apply the T-Test for each customer segment using groupby\n",
    "for segment, segment_data in grouped_data:\n",
    "    perform_t_test(segment, segment_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dded70",
   "metadata": {
    "papermill": {
     "duration": 0.009176,
     "end_time": "2024-06-20T12:26:36.381453",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.372277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paired T-Test and One-Sample T-Test\n",
    "\n",
    "The Paired T-Test determines whether there is a statistically significant difference between the means of two related groups, while the One-Sample T-Test checks if the mean of a single sample is different from a known or hypothesized population mean.\n",
    "\n",
    "## What is the Paired T-Test?\n",
    "\n",
    "A parametric test that compares the means of two related groups to see if they are significantly different from each other. It is useful for cases such as before-and-after measurements on the same subjects.\n",
    "\n",
    "## When to Use the Paired T-Test\n",
    "\n",
    "- The dependent variable is continuous.\n",
    "- The observations are paired or matched in some meaningful way (e.g., measurements before and after an intervention on the same subjects).\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. **Normality**: The differences between the paired measurements should be normally distributed. If this assumption is not met, use a non-parametric test such as the Wilcoxon Signed-Rank Test.\n",
    "\n",
    "## Hypotheses for Paired T-Test\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean difference between the paired observations is less than or equal to zero.\n",
    "- **Alternative Hypothesis (H1)**: The mean difference between the paired observations is greater than zero.\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. **Check Assumptions**: Verify the normality of the differences.\n",
    "2. **Calculate the T-Test Statistic**:\n",
    "\n",
    "   $$\n",
    "   t = \\frac{\\bar{d}}{s_d / \\sqrt{n}}\n",
    "   $$\n",
    "\n",
    "   where $\\bar{d}$ is the mean of the differences, $s_d$ is the standard deviation of the differences, and $n$ is the number of pairs.\n",
    "3. **Determine the p-value**: Compare the T-Test statistic to a t-distribution with $n-1$ degrees of freedom.\n",
    "4. **Interpret the Results**: If the p-value is â‰¤ 0.05, reject the null hypothesis.\n",
    "\n",
    "## Example: Paired T-Test\n",
    "\n",
    "Using `Sales_Before` and `Sales_After` variables:\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean difference between `Sales_After` and `Sales_Before` is less than or equal to zero.\n",
    "- **Alternative Hypothesis (H1)**: The mean difference between `Sales_After` and `Sales_Before` is greater than zero.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Check Assumptions**: Verify normality of the differences.\n",
    "2. **Perform the Paired T-Test**: Use the sample means, variances, and sizes.\n",
    "3. **Determine the p-value**: Compare the statistic to a t-distribution.\n",
    "4. **Interpret the Results**: Based on the p-value.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **T-Test Statistic**: Measures the difference between the paired means relative to the variability in the differences.\n",
    "- **p-value**: Probability of observing the test results under the null hypothesis.\n",
    "- **Degrees of Freedom**: \\(n - 1\\)\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating a significant difference between the paired means. Otherwise, fail to reject the null hypothesis.\n",
    "\n",
    "## What is the One-Sample T-Test?\n",
    "\n",
    "The One-Sample T-Test checks if the mean of a single sample is different from a known or hypothesized population mean.\n",
    "\n",
    "## When to Use the One-Sample T-Test\n",
    "\n",
    "- When you have a single sample and want to test if its mean is different from a specified value.\n",
    "- The dependent variable is continuous.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. **Normality**: The data should be approximately normally distributed. If this assumption is not met, consider using a non-parametric test such as the Wilcoxon Signed-Rank Test, but this example only covers the parametric test.\n",
    "2. **Independence**: The observations in the sample should be independent of each other.\n",
    "\n",
    "## Hypotheses for One-Sample T-Test\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean difference is less than or equal to the population mean.\n",
    "- **Alternative Hypothesis (H1)**: The mean difference is greater than the population mean.\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. **Check Assumptions**: Verify normality of the sample data.\n",
    "2. **Calculate the T-Test Statistic**:\n",
    "\n",
    "   $$\n",
    "   t = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}}\n",
    "   $$\n",
    "\n",
    "   where $\\bar{X}$ is the sample mean, $s$ is the sample standard deviation, $n$ is the sample size, and $\\mu_0$ is the population mean.\n",
    "3. **Determine the p-value**: Compare the T-Test statistic to a t-distribution with $n-1$ degrees of freedom.\n",
    "4. **Interpret the Results**: If the p-value is â‰¤ 0.05, reject the null hypothesis.\n",
    "\n",
    "## Example: One-Sample T-Test\n",
    "\n",
    "Using the differences between two columns (`Sales_Before` and `Sales_After`), test if the mean difference is greater than a specified value.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Calculate Differences**: Compute the differences between the paired observations in the two columns.\n",
    "2. **Check Assumptions**: Verify normality of the differences.\n",
    "3. **Perform the One-Sample T-Test**: Use `ttest_1samp` on the differences with a population mean of 0.\n",
    "4. **Determine the p-value**: Compare the statistic to a t-distribution.\n",
    "5. **Interpret the Results**: Based on the p-value.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **T-Test Statistic**: Measures how many standard deviations the sample mean is from the population mean.\n",
    "- **p-value**: Probability of observing the test results under the null hypothesis.\n",
    "- **Degrees of Freedom**: $n - 1$\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating a significant difference from the specified value. Otherwise, fail to reject the null hypothesis.\n",
    "\n",
    "## `alternative` Parameter in T-Tests\n",
    "\n",
    "The `alternative` parameter specifies the alternative hypothesis to be tested. It can take three values:\n",
    "\n",
    "- **'two-sided'**: Tests if the mean is significantly different from the hypothesized value in either direction.\n",
    "- **'less'**: Tests if the mean is significantly less than the hypothesized value.\n",
    "- **'greater'**: Tests if the mean is significantly greater than the hypothesized value.\n",
    "\n",
    "In our examples, we use `alternative='greater'` to test if `Sales_After` is significantly greater than `Sales_Before` for the Paired T-Test, and to test if the mean difference is greater than zero for the One-Sample T-Test.\n",
    "\n",
    "## Testing Against a Threshold\n",
    "\n",
    "When using the One-Sample T-Test, you might want to test if the mean difference is greater than a specific threshold. For instance, if you want to check whether the mean difference between `Sales_After` and `Sales_Before` exceeds a certain threshold (e.g., 10 units), you can set the `popmean` parameter to that threshold value.\n",
    "\n",
    "### Example: One-Sample T-Test Against a Threshold\n",
    "\n",
    "If the threshold is 10, set `popmean=10` in the `ttest_1samp` function.\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean difference is less than or equal to 10.\n",
    "- **Alternative Hypothesis (H1)**: The mean difference is greater than 10.\n",
    "\n",
    "This allows you to test whether the mean difference exceeds a specific practical or theoretical threshold, rather than just zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6155af78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:36.402912Z",
     "iopub.status.busy": "2024-06-20T12:26:36.402394Z",
     "iopub.status.idle": "2024-06-20T12:26:36.455101Z",
     "shell.execute_reply": "2024-06-20T12:26:36.453504Z"
    },
    "papermill": {
     "duration": 0.067416,
     "end_time": "2024-06-20T12:26:36.458492",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.391076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Paired T-Test\n",
      "\n",
      "Normality Test (Shapiro-Wilk):\n",
      "p-value: 0.0000\n",
      "\n",
      "Wilcoxon Signed-Rank Test Results:\n",
      "Test Statistic: 500498.0000\n",
      "p-value: 0.0000\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Reject the null hypothesis. Sales_After is significantly greater than Sales_Before (using Wilcoxon Signed-Rank Test).\n",
      "\n",
      "One-Sample T-Test\n",
      "\n",
      "Normality Test (Shapiro-Wilk) for One-Sample T-Test:\n",
      "p-value: 0.0000\n",
      "The normality assumption is not met for the One-Sample T-Test. The test is not performed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Extract the relevant columns for the Paired T-Test\n",
    "sales_before = sampled_data['Sales_Before']\n",
    "sales_after = sampled_data['Sales_After']\n",
    "\n",
    "# Paired T-Test\n",
    "print(\"\\nPaired T-Test\")\n",
    "\n",
    "# Calculate the differences\n",
    "differences = sales_after - sales_before\n",
    "\n",
    "# Check normality of differences\n",
    "normality_test = stats.shapiro(differences)\n",
    "print(\"\\nNormality Test (Shapiro-Wilk):\")\n",
    "print(f\"p-value: {normality_test.pvalue:.4f}\")\n",
    "normality_check = normality_test.pvalue > 0.05\n",
    "\n",
    "# Perform the appropriate test based on the assumptions\n",
    "if normality_check:\n",
    "    # Perform the Paired T-Test\n",
    "    t_test_result = stats.ttest_rel(sales_after, sales_before, alternative='greater')\n",
    "    test_used = \"Paired T-Test\"\n",
    "else:\n",
    "    # Perform the Wilcoxon Signed-Rank Test\n",
    "    t_test_result = stats.wilcoxon(sales_after, sales_before, alternative='greater')\n",
    "    test_used = \"Wilcoxon Signed-Rank Test\"\n",
    "\n",
    "# Display the results of the test\n",
    "t_statistic = t_test_result.statistic\n",
    "p_value = t_test_result.pvalue\n",
    "print(f\"\\n{test_used} Results:\")\n",
    "print(f\"Test Statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level: {alpha}\")\n",
    "if p_value < alpha:\n",
    "    print(f\"Conclusion: Reject the null hypothesis. Sales_After is significantly greater than Sales_Before (using {test_used}).\")\n",
    "else:\n",
    "    print(f\"Conclusion: Fail to reject the null hypothesis. Sales_After is not significantly greater than Sales_Before (using {test_used}).\")\n",
    "\n",
    "# One-Sample T-Test\n",
    "print(\"\\nOne-Sample T-Test\")\n",
    "\n",
    "# Specify the population mean to test against (e.g., 10)\n",
    "population_mean = 10\n",
    "\n",
    "# Check normality of the differences\n",
    "normality_test = stats.shapiro(differences)\n",
    "print(\"\\nNormality Test (Shapiro-Wilk) for One-Sample T-Test:\")\n",
    "print(f\"p-value: {normality_test.pvalue:.4f}\")\n",
    "normality_check = normality_test.pvalue > 0.05\n",
    "\n",
    "# Perform the One-Sample T-Test if normality assumption is met\n",
    "if normality_check:\n",
    "    # Perform the One-Sample T-Test\n",
    "    one_sample_test_result = stats.ttest_1samp(differences, popmean=population_mean, alternative='greater')\n",
    "    test_used = \"One-Sample T-Test\"\n",
    "\n",
    "    # Display the results of the test\n",
    "    t_statistic = one_sample_test_result.statistic\n",
    "    p_value = one_sample_test_result.pvalue\n",
    "    print(f\"\\n{test_used} Results:\")\n",
    "    print(f\"Test Statistic: {t_statistic:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "    # Interpret the p-value\n",
    "    alpha = 0.05\n",
    "    print(f\"\\nSignificance Level: {alpha}\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"Conclusion: Reject the null hypothesis. The mean difference (Sales_After - Sales_Before) is significantly greater than {population_mean}.\")\n",
    "    else:\n",
    "        print(f\"Conclusion: Fail to reject the null hypothesis. The mean difference (Sales_After - Sales_Before) is not significantly greater than {population_mean}.\")\n",
    "else:\n",
    "    print(\"The normality assumption is not met for the One-Sample T-Test. The test is not performed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ef963",
   "metadata": {
    "papermill": {
     "duration": 0.011962,
     "end_time": "2024-06-20T12:26:36.480596",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.468634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a Synthetic Dataset\n",
    "\n",
    "Because the previous test results on the actual dataset did not meet the normality assumption for the One-Sample T-Test, we will create a synthetic dataset to ensure that the differences follow a normal distribution. This will allow us to properly demonstrate the use of both the Paired T-Test and the One-Sample T-Test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9409f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:36.503551Z",
     "iopub.status.busy": "2024-06-20T12:26:36.503029Z",
     "iopub.status.idle": "2024-06-20T12:26:36.532314Z",
     "shell.execute_reply": "2024-06-20T12:26:36.530999Z"
    },
    "papermill": {
     "duration": 0.044474,
     "end_time": "2024-06-20T12:26:36.535768",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.491294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "   Sales_Before  Sales_After\n",
      "0     54.967142    68.993554\n",
      "1     48.617357    64.246337\n",
      "2     56.476885    55.596304\n",
      "3     65.230299    48.530632\n",
      "4     47.658466    61.982233\n",
      "\n",
      "Paired T-Test\n",
      "\n",
      "Normality Test (Shapiro-Wilk):\n",
      "p-value: 0.4960\n",
      "\n",
      "Paired T-Test Results:\n",
      "Test Statistic: 12.2324\n",
      "p-value: 0.0000\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Reject the null hypothesis. Sales_After is significantly greater than Sales_Before (using Paired T-Test).\n",
      "\n",
      "One-Sample T-Test\n",
      "\n",
      "Normality Test (Shapiro-Wilk) for One-Sample T-Test:\n",
      "p-value: 0.4960\n",
      "\n",
      "One-Sample T-Test Results:\n",
      "Test Statistic: -98.6681\n",
      "p-value: 1.0000\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Fail to reject the null hypothesis. The mean difference (Sales_After - Sales_Before) is not significantly greater than 50.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate two normally distributed variables\n",
    "n_samples = 1000\n",
    "mu1, sigma1 = 50, 10  # mean and standard deviation for first variable\n",
    "mu2, sigma2 = 55, 10  # mean and standard deviation for second variable\n",
    "\n",
    "sales_before = np.random.normal(mu1, sigma1, n_samples)\n",
    "sales_after = np.random.normal(mu2, sigma2, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Sales_Before': sales_before,\n",
    "    'Sales_After': sales_after\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Extract the relevant columns for the Paired T-Test\n",
    "sales_before = data['Sales_Before']\n",
    "sales_after = data['Sales_After']\n",
    "\n",
    "# Paired T-Test\n",
    "print(\"\\nPaired T-Test\")\n",
    "\n",
    "# Calculate the differences\n",
    "differences = sales_after - sales_before\n",
    "\n",
    "# Check normality of differences\n",
    "normality_test = stats.shapiro(differences)\n",
    "print(\"\\nNormality Test (Shapiro-Wilk):\")\n",
    "print(f\"p-value: {normality_test.pvalue:.4f}\")\n",
    "normality_check = normality_test.pvalue > 0.05\n",
    "\n",
    "# Perform the appropriate test based on the assumptions\n",
    "if normality_check:\n",
    "    # Perform the Paired T-Test\n",
    "    t_test_result = stats.ttest_rel(sales_after, sales_before, alternative='greater')\n",
    "    test_used = \"Paired T-Test\"\n",
    "else:\n",
    "    # Perform the Wilcoxon Signed-Rank Test\n",
    "    t_test_result = stats.wilcoxon(differences, alternative='greater')\n",
    "    test_used = \"Wilcoxon Signed-Rank Test\"\n",
    "\n",
    "# Display the results of the test\n",
    "t_statistic = t_test_result.statistic\n",
    "p_value = t_test_result.pvalue\n",
    "print(f\"\\n{test_used} Results:\")\n",
    "print(f\"Test Statistic: {t_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level: {alpha}\")\n",
    "if p_value < alpha:\n",
    "    print(f\"Conclusion: Reject the null hypothesis. Sales_After is significantly greater than Sales_Before (using {test_used}).\")\n",
    "else:\n",
    "    print(f\"Conclusion: Fail to reject the null hypothesis. Sales_After is not significantly greater than Sales_Before (using {test_used}).\")\n",
    "\n",
    "# One-Sample T-Test\n",
    "print(\"\\nOne-Sample T-Test\")\n",
    "\n",
    "# Specify the population mean to test against (e.g., 50 or any threshold value)\n",
    "population_mean = 50 # Change this value to test against a different threshold\n",
    "\n",
    "# Check normality of the differences\n",
    "normality_test = stats.shapiro(differences)\n",
    "print(\"\\nNormality Test (Shapiro-Wilk) for One-Sample T-Test:\")\n",
    "print(f\"p-value: {normality_test.pvalue:.4f}\")\n",
    "normality_check = normality_test.pvalue > 0.05\n",
    "\n",
    "# Perform the One-Sample T-Test if normality assumption is met\n",
    "if normality_check:\n",
    "    # Perform the One-Sample T-Test\n",
    "    one_sample_test_result = stats.ttest_1samp(differences, popmean=population_mean, alternative='greater')\n",
    "    test_used = \"One-Sample T-Test\"\n",
    "\n",
    "    # Display the results of the test\n",
    "    t_statistic = one_sample_test_result.statistic\n",
    "    p_value = one_sample_test_result.pvalue\n",
    "    print(f\"\\n{test_used} Results:\")\n",
    "    print(f\"Test Statistic: {t_statistic:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "    # Interpret the p-value\n",
    "    alpha = 0.05\n",
    "    print(f\"\\nSignificance Level: {alpha}\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"Conclusion: Reject the null hypothesis. The mean difference (Sales_After - Sales_Before) is significantly greater than {population_mean}.\")\n",
    "    else:\n",
    "        print(f\"Conclusion: Fail to reject the null hypothesis. The mean difference (Sales_After - Sales_Before) is not significantly greater than {population_mean}.\")\n",
    "else:\n",
    "    print(\"The normality assumption is not met for the One-Sample T-Test. The test is not performed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ad09a",
   "metadata": {
    "papermill": {
     "duration": 0.010087,
     "end_time": "2024-06-20T12:26:36.556089",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.546002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ANOVA (Analysis of Variance)\n",
    "\n",
    "The ANOVA test determines whether there are statistically significant differences between the means of three or more independent groups.\n",
    "\n",
    "## What is ANOVA?\n",
    "\n",
    "A parametric test that compares the means of three or more independent groups to see if at least one of the group means is significantly different from the others.\n",
    "\n",
    "## When to Use ANOVA\n",
    "\n",
    "- The dependent variable is continuous.\n",
    "- The independent variable consists of three or more categorical, independent groups.\n",
    "- Observations are independent of each other.\n",
    "- The dependent variable is approximately normally distributed in each group.\n",
    "- Homogeneity of variances: The variances in the groups are equal.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. The dependent variable is normally distributed in each group.\n",
    "   - If normality is not met, consider using a non-parametric test such as the Kruskal-Wallis Test.\n",
    "2. Homogeneity of variances.\n",
    "   - If homogeneity of variances is not met, use Welch's ANOVA.\n",
    "\n",
    "## Hypotheses for ANOVA\n",
    "\n",
    "- **Null Hypothesis (H0)**: All group means are equal.\n",
    "- **Alternative Hypothesis (H1)**: At least one group mean is different.\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. **Check Assumptions**: Verify normality and homogeneity of variances.\n",
    "2. **Calculate the ANOVA F-Statistic**:\n",
    "\n",
    "   $$\n",
    "   F = \\frac{\\text{between-group variability}}{\\text{within-group variability}}\n",
    "   $$\n",
    "\n",
    "3. **Determine the p-value**: Compare the F-Statistic to an F-distribution with appropriate degrees of freedom.\n",
    "4. **Interpret the Results**: If the p-value is â‰¤ 0.05, reject the null hypothesis.\n",
    "\n",
    "## Post Hoc Analysis\n",
    "\n",
    "If the ANOVA test indicates significant differences, conduct a post hoc analysis to identify which groups differ from each other.\n",
    "\n",
    "### Tukey's HSD Test\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD) test compares all possible pairs of group means to determine which groups are significantly different.\n",
    "\n",
    "### Interpretation of Tukey's HSD Test\n",
    "\n",
    "- **group1 and group2**: The names of the two groups being compared.\n",
    "- **meandiff**: The difference in the mean values of the two groups.\n",
    "- **p-adj**: The adjusted p-value for the comparison between the two groups.\n",
    "- **lower and upper**: The lower and upper bounds of the confidence interval for the mean difference.\n",
    "- **reject**: Indicates whether the null hypothesis of equal means is rejected (True) or not (False). A True value indicates a significant difference between the group means.\n",
    "\n",
    "### Pairwise Wilcoxon (Dunn's) Test for Kruskal-Wallis\n",
    "\n",
    "When using the Kruskal-Wallis test, use pairwise Wilcoxon tests (Dunn's test) for post hoc analysis.\n",
    "\n",
    "### Interpretation of Pairwise Wilcoxon (Dunn's) Test\n",
    "\n",
    "- The matrix contains p-values for pairwise comparisons between groups.\n",
    "- A lower triangular matrix where the (i, j) element is the p-value for the comparison between group i and group j.\n",
    "- Adjusted p-values (using Bonferroni correction) to control for multiple comparisons.\n",
    "- If a p-value is less than the significance level (e.g., 0.05), the null hypothesis of equal distributions between the two groups is rejected, indicating a significant difference between those groups.\n",
    "\n",
    "## Example: ANOVA\n",
    "\n",
    "Using `Customer_Satisfaction_After` variable across different `Customer_Segment` categories:\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean `Customer_Satisfaction_After` is equal across all `Customer_Segment` categories.\n",
    "- **Alternative Hypothesis (H1)**: At least one mean `Customer_Satisfaction_After` is different across `Customer_Segment` categories.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Check Assumptions**: Verify normality and homogeneity of variances.\n",
    "2. **Perform ANOVA**: Use the sample means, variances, and sizes.\n",
    "3. **Determine the p-value**: Compare the statistic to an F-distribution.\n",
    "4. **Interpret the Results**: Based on the p-value.\n",
    "5. **Conduct Post Hoc Analysis**: If ANOVA is significant, perform Tukey's HSD test to determine which groups differ. If using Kruskal-Wallis, perform pairwise Wilcoxon tests.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **ANOVA F-Statistic**: Measures the ratio of between-group variability to within-group variability.\n",
    "- **p-value**: Probability of observing the test results under the null hypothesis.\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating that at least one group mean is significantly different. Otherwise, fail to reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a80c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:36.579755Z",
     "iopub.status.busy": "2024-06-20T12:26:36.579195Z",
     "iopub.status.idle": "2024-06-20T12:26:54.875904Z",
     "shell.execute_reply": "2024-06-20T12:26:54.874377Z"
    },
    "papermill": {
     "duration": 18.312214,
     "end_time": "2024-06-20T12:26:54.879369",
     "exception": false,
     "start_time": "2024-06-20T12:26:36.567155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit_posthocs\r\n",
      "  Downloading scikit_posthocs-0.9.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from scikit_posthocs) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit_posthocs) (1.11.4)\r\n",
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (from scikit_posthocs) (0.14.1)\r\n",
      "Requirement already satisfied: pandas>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from scikit_posthocs) (2.2.2)\r\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from scikit_posthocs) (0.12.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from scikit_posthocs) (3.7.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.20.0->scikit_posthocs) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.20.0->scikit_posthocs) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.20.0->scikit_posthocs) (2023.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->scikit_posthocs) (3.1.1)\r\n",
      "Requirement already satisfied: patsy>=0.5.4 in /opt/conda/lib/python3.10/site-packages (from statsmodels->scikit_posthocs) (0.5.6)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.4->statsmodels->scikit_posthocs) (1.16.0)\r\n",
      "Downloading scikit_posthocs-0.9.0-py3-none-any.whl (32 kB)\r\n",
      "Installing collected packages: scikit_posthocs\r\n",
      "Successfully installed scikit_posthocs-0.9.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit_posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972502f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:54.904130Z",
     "iopub.status.busy": "2024-06-20T12:26:54.903570Z",
     "iopub.status.idle": "2024-06-20T12:26:57.572964Z",
     "shell.execute_reply": "2024-06-20T12:26:57.571490Z"
    },
    "papermill": {
     "duration": 2.686286,
     "end_time": "2024-06-20T12:26:57.576314",
     "exception": false,
     "start_time": "2024-06-20T12:26:54.890028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Checking Assumptions...\n",
      "\n",
      "Normality Test (Shapiro-Wilk) for each group:\n",
      "Customer_Segment\n",
      "High Value      5.652129e-19\n",
      "Low Value       2.664373e-05\n",
      "Medium Value    1.415237e-05\n",
      "Name: Customer_Satisfaction_After, dtype: float64\n",
      "\n",
      "Homogeneity of Variances Test (Levene's Test):\n",
      "p-value: 0.0011\n",
      "\n",
      "Kruskal-Wallis Test Results:\n",
      "KruskalResult(statistic=487.16277968511116, pvalue=1.636614886093797e-106)\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Reject the null hypothesis. There is a significant difference in Customer_Satisfaction_After across different Customer_Segment categories (using Kruskal-Wallis Test).\n",
      "\n",
      "Performing pairwise Wilcoxon tests...\n",
      "                 High Value      Low Value  Medium Value\n",
      "High Value     1.000000e+00  3.451255e-107  2.016508e-22\n",
      "Low Value     3.451255e-107   1.000000e+00  1.758905e-33\n",
      "Medium Value   2.016508e-22   1.758905e-33  1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Extract relevant columns from the sampled data\n",
    "customer_satisfaction_after = sampled_data['Customer_Satisfaction_After']\n",
    "customer_segment = sampled_data['Customer_Segment']\n",
    "\n",
    "# Check assumptions\n",
    "print(\"\\nChecking Assumptions...\")\n",
    "\n",
    "# Assumption: Normality of each group\n",
    "groups = sampled_data.groupby('Customer_Segment')['Customer_Satisfaction_After']\n",
    "normality_p_values = groups.apply(lambda x: stats.shapiro(x)[1])\n",
    "print(\"\\nNormality Test (Shapiro-Wilk) for each group:\")\n",
    "print(normality_p_values)\n",
    "\n",
    "# Assumption: Homogeneity of variances\n",
    "levene_test = stats.levene(*[group for name, group in groups])\n",
    "print(\"\\nHomogeneity of Variances Test (Levene's Test):\")\n",
    "print(f\"p-value: {levene_test.pvalue:.4f}\")\n",
    "\n",
    "# Perform the appropriate ANOVA test based on the assumptions\n",
    "if all(normality_p_values > 0.05):\n",
    "    if levene_test.pvalue > 0.05:\n",
    "        # Perform ANOVA\n",
    "        model = ols('Customer_Satisfaction_After ~ C(Customer_Segment)', data=sampled_data).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        test_used = \"ANOVA\"\n",
    "    else:\n",
    "        # Perform Welch's ANOVA\n",
    "        model = ols('Customer_Satisfaction_After ~ C(Customer_Segment)', data=sampled_data).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2, robust='hc3')\n",
    "        test_used = \"Welch's ANOVA\"\n",
    "else:\n",
    "    # Perform Kruskal-Wallis Test\n",
    "    kruskal_test = stats.kruskal(*[group for name, group in groups])\n",
    "    test_used = \"Kruskal-Wallis Test\"\n",
    "\n",
    "# Display the results of the ANOVA test\n",
    "print(f\"\\n{test_used} Results:\")\n",
    "\n",
    "if test_used == \"Kruskal-Wallis Test\":\n",
    "    print(kruskal_test)\n",
    "    p_val = kruskal_test.pvalue\n",
    "else:\n",
    "    print(anova_table)\n",
    "    p_val = anova_table['PR(>F)'][0]\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level: {alpha}\")\n",
    "if p_val < alpha:\n",
    "    print(f\"Conclusion: Reject the null hypothesis. There is a significant difference in Customer_Satisfaction_After across different Customer_Segment categories (using {test_used}).\")\n",
    "    \n",
    "    if test_used == \"ANOVA\":\n",
    "        # Perform Post Hoc Analysis using Tukey's HSD Test\n",
    "        print(\"\\nPerforming Tukey's HSD Test...\")\n",
    "        tukey = pairwise_tukeyhsd(endog=sampled_data['Customer_Satisfaction_After'], groups=sampled_data['Customer_Segment'], alpha=0.05)\n",
    "        print(tukey)\n",
    "        \n",
    "    elif test_used == \"Kruskal-Wallis Test\":\n",
    "        # Perform pairwise comparisons for Kruskal-Wallis using Dunn's test\n",
    "        print(\"\\nPerforming pairwise Wilcoxon tests...\")\n",
    "        pairwise_results = sp.posthoc_dunn(sampled_data, val_col='Customer_Satisfaction_After', group_col='Customer_Segment', p_adjust='bonferroni')\n",
    "        print(pairwise_results)\n",
    "else:\n",
    "    print(f\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in Customer_Satisfaction_After across different Customer_Segment categories (using {test_used}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b4747",
   "metadata": {
    "papermill": {
     "duration": 0.010646,
     "end_time": "2024-06-20T12:26:57.597890",
     "exception": false,
     "start_time": "2024-06-20T12:26:57.587244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Correlation Test\n",
    "\n",
    "The correlation test determines the strength and direction of the linear relationship between two continuous variables.\n",
    "\n",
    "## What is a Correlation Test?\n",
    "\n",
    "A statistical method used to assess the strength and direction of the linear relationship between two continuous variables.\n",
    "\n",
    "## When to Use a Correlation Test\n",
    "\n",
    "- Both variables are continuous.\n",
    "- The relationship between the variables is linear.\n",
    "- Data pairs are independent of each other.\n",
    "\n",
    "## Types of Correlation Tests\n",
    "\n",
    "1. **Pearson Correlation**: Measures the linear relationship between two continuous variables. Assumes both variables are normally distributed.\n",
    "   - If normality is not met, use Spearman's Rank Correlation.\n",
    "2. **Spearman's Rank Correlation**: Measures the monotonic relationship between two continuous or ordinal variables.\n",
    "\n",
    "## Hypotheses for Correlation Test\n",
    "\n",
    "- **Null Hypothesis (H0)**: There is no correlation between the variables.\n",
    "- **Alternative Hypothesis (H1)**: There is a correlation between the variables.\n",
    "\n",
    "## Step-by-Step Guide\n",
    "\n",
    "1. **Check Assumptions**: Verify normality of both variables.\n",
    "   - If normality is not met, use Spearman's Rank Correlation.\n",
    "2. **Calculate the Correlation Coefficient**:\n",
    "\n",
    "   $$\n",
    "   r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n \\sum x^2 - (\\sum x)^2][n \\sum y^2 - (\\sum y)^2]}}\n",
    "   $$\n",
    "\n",
    "3. **Determine the p-value**: Compare the correlation coefficient to a t-distribution.\n",
    "4. **Interpret the Results**: If the p-value is â‰¤ 0.05, reject the null hypothesis.\n",
    "\n",
    "## Example: Correlation Test\n",
    "\n",
    "Using `Sales_Before` and `Sales_After` variables:\n",
    "\n",
    "- **Null Hypothesis (H0)**: There is no correlation between `Sales_Before` and `Sales_After`.\n",
    "- **Alternative Hypothesis (H1)**: There is a correlation between `Sales_Before` and `Sales_After`.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Check Assumptions**: Verify normality of both variables.\n",
    "2. **Perform the Correlation Test**: Use the sample means, variances, and sizes.\n",
    "3. **Determine the p-value**: Compare the statistic to a t-distribution.\n",
    "4. **Interpret the Results**: Based on the p-value.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Correlation Coefficient (r)**: Measures the strength and direction of the linear relationship between the variables.\n",
    "- **p-value**: Probability of observing the test results under the null hypothesis.\n",
    "\n",
    "If the p-value is < 0.05, reject the null hypothesis, indicating a significant correlation between the variables. Otherwise, fail to reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df49ccd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T12:26:57.622645Z",
     "iopub.status.busy": "2024-06-20T12:26:57.621477Z",
     "iopub.status.idle": "2024-06-20T12:26:57.671703Z",
     "shell.execute_reply": "2024-06-20T12:26:57.670327Z"
    },
    "papermill": {
     "duration": 0.066176,
     "end_time": "2024-06-20T12:26:57.674869",
     "exception": false,
     "start_time": "2024-06-20T12:26:57.608693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       Group Customer_Segment  Sales_Before  Sales_After  \\\n",
      "0    Control       High Value    240.548359   300.007568   \n",
      "1  Treatment       High Value    246.862114   381.337555   \n",
      "2    Control       High Value    156.978084   179.330464   \n",
      "3    Control     Medium Value    192.126708   229.278031   \n",
      "4    Control       High Value    229.685623   270.167701   \n",
      "\n",
      "   Customer_Satisfaction_Before  Customer_Satisfaction_After Purchase_Made  \n",
      "0                     74.684767                    74.093658            No  \n",
      "1                    100.000000                   100.000000           Yes  \n",
      "2                     98.780735                   100.000000            No  \n",
      "3                     49.333766                    39.811841           Yes  \n",
      "4                     83.974852                    87.738591           Yes  \n",
      "\n",
      "Checking Assumptions...\n",
      "\n",
      "Normality Test (Shapiro-Wilk):\n",
      "Sales_Before p-value: 0.4559\n",
      "Sales_After p-value: 0.0004\n",
      "\n",
      "Spearman's Rank Correlation Results:\n",
      "Correlation Coefficient: 0.8737\n",
      "p-value: 0.0000\n",
      "\n",
      "Significance Level: 0.05\n",
      "Conclusion: Reject the null hypothesis. There is a significant correlation between Sales_Before and Sales_After (using Spearman's Rank Correlation).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/sales-and-satisfaction/Sales_without_NaNs_v1.3.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Sample the dataset to reduce size for computation\n",
    "sampled_data = data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Extract relevant columns from the sampled data\n",
    "sales_before = sampled_data['Sales_Before']\n",
    "sales_after = sampled_data['Sales_After']\n",
    "\n",
    "# Check assumptions\n",
    "print(\"\\nChecking Assumptions...\")\n",
    "\n",
    "# Assumption: Normality of both variables\n",
    "normality_test_before = stats.shapiro(sales_before)\n",
    "normality_test_after = stats.shapiro(sales_after)\n",
    "\n",
    "print(\"\\nNormality Test (Shapiro-Wilk):\")\n",
    "print(f\"Sales_Before p-value: {normality_test_before.pvalue:.4f}\")\n",
    "print(f\"Sales_After p-value: {normality_test_after.pvalue:.4f}\")\n",
    "\n",
    "# Perform the appropriate correlation test based on the assumptions\n",
    "if normality_test_before.pvalue > 0.05 and normality_test_after.pvalue > 0.05:\n",
    "    # Perform Pearson Correlation\n",
    "    correlation_coefficient, p_val = stats.pearsonr(sales_before, sales_after)\n",
    "    test_used = \"Pearson Correlation\"\n",
    "else:\n",
    "    # Perform Spearman's Rank Correlation\n",
    "    correlation_coefficient, p_val = stats.spearmanr(sales_before, sales_after)\n",
    "    test_used = \"Spearman's Rank Correlation\"\n",
    "\n",
    "# Display the results of the correlation test\n",
    "print(f\"\\n{test_used} Results:\")\n",
    "print(f\"Correlation Coefficient: {correlation_coefficient:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level: {alpha}\")\n",
    "if p_val < alpha:\n",
    "    print(f\"Conclusion: Reject the null hypothesis. There is a significant correlation between Sales_Before and Sales_After (using {test_used}).\")\n",
    "else:\n",
    "    print(f\"Conclusion: Fail to reject the null hypothesis. There is no significant correlation between Sales_Before and Sales_After (using {test_used}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed49f2d",
   "metadata": {
    "papermill": {
     "duration": 0.010649,
     "end_time": "2024-06-20T12:26:57.697324",
     "exception": false,
     "start_time": "2024-06-20T12:26:57.686675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Thank You for Exploring This Notebook!\n",
    "\n",
    "\n",
    "If you have any questions, suggestions, or just want to discuss any of the topics further, please don't hesitate to reach out or leave a comment. Your feedback is not only welcome but also invaluable!\n",
    "\n",
    "Happy analyzing, and stay curious!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5003639,
     "sourceId": 8484073,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.236456,
   "end_time": "2024-06-20T12:26:58.748916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-20T12:26:29.512460",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
