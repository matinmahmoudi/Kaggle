{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c63349",
   "metadata": {
    "papermill": {
     "duration": 0.003192,
     "end_time": "2025-02-26T17:27:18.198383",
     "exception": false,
     "start_time": "2025-02-26T17:27:18.195191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Advanced Machine Learning: MLE, NLL, MAP, and the Relationship with MSE and MAE\n",
    "\n",
    "## Important Note:\n",
    "**This notebook is not intended for beginners.** It is designed for upper intermediate machine learning learners who are seeking a deep understanding of Maximum Likelihood Estimation (MLE), Negative Log-Likelihood (NLL), Maximum A Posteriori Estimation (MAP), and how they relate to common loss functions such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). We will explore these concepts with rigorous mathematical explanations and detailed Python implementations from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d15da3",
   "metadata": {
    "papermill": {
     "duration": 0.002014,
     "end_time": "2025-02-26T17:27:18.202865",
     "exception": false,
     "start_time": "2025-02-26T17:27:18.200851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction to Estimation Theory\n",
    "Estimation theory lies at the intersection of statistics and machine learning, where the goal is to infer unknown parameters of probabilistic models from observed data. We focus on two major estimation paradigms:\n",
    "\n",
    "1. **Maximum Likelihood Estimation (MLE)**: A frequentist approach based on maximizing the probability of observed data.\n",
    "2. **Maximum A Posteriori Estimation (MAP)**: A Bayesian method that incorporates prior knowledge about the parameters into the estimation process.\n",
    "\n",
    "These techniques are foundational in machine learning, underpinning many algorithms from linear regression to deep learning. In this notebook, we will derive these estimators mathematically, analyze their properties, and relate them to commonly used loss functions like MSE and MAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c94682",
   "metadata": {
    "papermill": {
     "duration": 0.00184,
     "end_time": "2025-02-26T17:27:18.206918",
     "exception": false,
     "start_time": "2025-02-26T17:27:18.205078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Maximum Likelihood Estimation (MLE)\n",
    "Maximum Likelihood Estimation is a fundamental method for parameter estimation in statistics. It is based on the principle of choosing the parameter values that maximize the likelihood of the observed data under the assumed model.\n",
    "\n",
    "### 2.1 Mathematical Formulation\n",
    "Let $X = \\{x_1, x_2, \\dots, x_n\\}$ be an independent and identically distributed (i.i.d.) sample from a probability distribution with parameter $\\theta$. The likelihood function is:\n",
    "\n",
    "$$ L(\\theta; X) = \\prod_{i=1}^{n} p(x_i | \\theta) $$\n",
    "\n",
    "The log-likelihood function, which simplifies the product to a sum, is given by:\n",
    "\n",
    "$$ \\log L(\\theta; X) = \\sum_{i=1}^{n} \\log p(x_i | \\theta) $$\n",
    "\n",
    "The MLE $\\hat{\\theta}_{MLE}$ is the value of $\\theta$ that maximizes the log-likelihood:\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\log L(\\theta; X) $$\n",
    "\n",
    "### 2.2 Derivation for Normal Distribution\n",
    "Consider the normal distribution with unknown mean $\\mu$ and variance $\\sigma^2$. The probability density function (PDF) is:\n",
    "\n",
    "$$ p(x_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "The log-likelihood for a sample $X$ is:\n",
    "\n",
    "$$ \\log L(\\mu, \\sigma^2; X) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 $$\n",
    "\n",
    "To find the MLE, we differentiate this log-likelihood with respect to $\\mu$ and $\\sigma^2$, set the derivatives to zero, and solve for the parameters.\n",
    "\n",
    "### 2.3 Proof of MLE Consistency\n",
    "MLE is consistent under regularity conditions, meaning that as $n \\to \\infty$, the MLE converges in probability to the true parameter value $\\theta_0$. This is formalized by the law of large numbers applied to the score function (the derivative of the log-likelihood). The detailed proof involves showing that the Fisher Information matrix is positive definite and that the likelihood function is asymptotically normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2872331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T17:27:18.212857Z",
     "iopub.status.busy": "2025-02-26T17:27:18.212504Z",
     "iopub.status.idle": "2025-02-26T17:27:19.916060Z",
     "shell.execute_reply": "2025-02-26T17:27:19.914874Z"
    },
    "papermill": {
     "duration": 1.708991,
     "end_time": "2025-02-26T17:27:19.917971",
     "exception": false,
     "start_time": "2025-02-26T17:27:18.208980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Log-Likelihood: -3.926523529277739\n",
      "Score Function (mu, sigma^2): (1.5770213417970976e-15, 1.7763568394002505e-15)\n",
      "Fisher Information Matrix:\n",
      "[[17.75568182  0.        ]\n",
      " [ 0.         31.52642368]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "class MLENormal:\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "    \n",
    "    def log_likelihood(self, mu, sigma2):\n",
    "        \"\"\"Compute the log-likelihood for normal distribution.\"\"\"\n",
    "        n = len(self.data)\n",
    "        ll = -n/2 * np.log(2 * np.pi * sigma2) - np.sum((self.data - mu)**2) / (2 * sigma2)\n",
    "        return ll\n",
    "    \n",
    "    def score_function(self, mu, sigma2):\n",
    "        \"\"\"Compute the score function (gradient of log-likelihood).\"\"\"\n",
    "        n = len(self.data)\n",
    "        score_mu = np.sum(self.data - mu) / sigma2\n",
    "        score_sigma2 = -n / (2 * sigma2) + np.sum((self.data - mu)**2) / (2 * sigma2**2)\n",
    "        return score_mu, score_sigma2\n",
    "    \n",
    "    def fisher_information(self, sigma2):\n",
    "        \"\"\"Compute the Fisher information matrix for normal distribution.\"\"\"\n",
    "        n = len(self.data)\n",
    "        fisher_mu2 = n / sigma2\n",
    "        fisher_sigma2_2 = n / (2 * sigma2**2)\n",
    "        return np.array([[fisher_mu2, 0], [0, fisher_sigma2_2]])\n",
    "\n",
    "# Example usage\n",
    "data = [2.3, 2.5, 3.1, 3.8, 2.7]\n",
    "mle = MLENormal(data)\n",
    "\n",
    "mu_initial, sigma2_initial = np.mean(data), np.var(data)\n",
    "log_likelihood = mle.log_likelihood(mu_initial, sigma2_initial)\n",
    "score_mu, score_sigma2 = mle.score_function(mu_initial, sigma2_initial)\n",
    "fisher_matrix = mle.fisher_information(sigma2_initial)\n",
    "\n",
    "print(f\"Initial Log-Likelihood: {log_likelihood}\")\n",
    "print(f\"Score Function (mu, sigma^2): ({score_mu}, {score_sigma2})\")\n",
    "print(f\"Fisher Information Matrix:\\n{fisher_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33f9d3",
   "metadata": {
    "papermill": {
     "duration": 0.002733,
     "end_time": "2025-02-26T17:27:19.923177",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.920444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Negative Log-Likelihood (NLL)\n",
    "The Negative Log-Likelihood (NLL) is the negative of the log-likelihood and is often used as a loss function in machine learning, particularly in models that predict probabilities. Minimizing the NLL corresponds to maximizing the likelihood.\n",
    "\n",
    "For a normal distribution, the NLL is:\n",
    "\n",
    "$$ \\text{NLL}(\\mu, \\sigma^2) = \\frac{n}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 $$\n",
    "\n",
    "The NLL is derived by taking the negative of the log-likelihood, and it represents the cost function that we minimize in many probabilistic models.\n",
    "\n",
    "### 3.1 Gradient of the NLL\n",
    "To perform gradient-based optimization, we need the gradient (or score function) of the NLL with respect to the parameters. For the normal distribution:\n",
    "\n",
    "$$ \\frac{\\partial \\text{NLL}}{\\partial \\mu} = -\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) $$\n",
    "\n",
    "$$ \\frac{\\partial \\text{NLL}}{\\partial \\sigma^2} = \\frac{n}{2\\sigma^2} - \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (x_i - \\mu)^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27ca0cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T17:27:19.929786Z",
     "iopub.status.busy": "2025-02-26T17:27:19.929412Z",
     "iopub.status.idle": "2025-02-26T17:27:19.937851Z",
     "shell.execute_reply": "2025-02-26T17:27:19.936945Z"
    },
    "papermill": {
     "duration": 0.014369,
     "end_time": "2025-02-26T17:27:19.940162",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.925793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL Value: 3.926523529277739\n",
      "NLL Gradient (mu, sigma^2): (-1.5770213417970976e-15, -1.7763568394002505e-15)\n"
     ]
    }
   ],
   "source": [
    "# NLL calculation and gradient implementation\n",
    "def negative_log_likelihood(mu, sigma2, data):\n",
    "    n = len(data)\n",
    "    nll = n / 2 * np.log(2 * np.pi * sigma2) + np.sum((data - mu)**2) / (2 * sigma2)\n",
    "    return nll\n",
    "\n",
    "def nll_gradient(mu, sigma2, data):\n",
    "    n = len(data)\n",
    "    grad_mu = -np.sum(data - mu) / sigma2\n",
    "    grad_sigma2 = n / (2 * sigma2) - np.sum((data - mu)**2) / (2 * sigma2**2)\n",
    "    return grad_mu, grad_sigma2\n",
    "\n",
    "nll_value = negative_log_likelihood(mu_initial, sigma2_initial, data)\n",
    "grad_mu, grad_sigma2 = nll_gradient(mu_initial, sigma2_initial, data)\n",
    "\n",
    "print(f\"NLL Value: {nll_value}\")\n",
    "print(f\"NLL Gradient (mu, sigma^2): ({grad_mu}, {grad_sigma2})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6efae",
   "metadata": {
    "papermill": {
     "duration": 0.00333,
     "end_time": "2025-02-26T17:27:19.947465",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.944135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Maximum A Posteriori Estimation (MAP)\n",
    "\n",
    "### 4.1 Derivation of MAP for Normal Distribution with Gaussian Prior\n",
    "Given a dataset $X = \\{x_1, x_2, \\dots, x_n\\}$, suppose we assume that the data is normally distributed with unknown mean $\\mu$ and known variance $\\sigma^2$. Additionally, we assume a Gaussian prior on $\\mu$ with mean $\\mu_0$ and variance $\\sigma_0^2$:\n",
    "\n",
    "$$ P(\\mu) \\propto \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2}\\right) $$\n",
    "\n",
    "The posterior distribution, according to Bayes' theorem, is proportional to the product of the likelihood and the prior:\n",
    "\n",
    "$$ P(\\mu | X) \\propto \\exp\\left(-\\frac{n}{2\\sigma^2} (\\mu - \\bar{x})^2 - \\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2}\\right) $$\n",
    "\n",
    "Taking the logarithm of the posterior (log-posterior):\n",
    "\n",
    "$$ \\log P(\\mu | X) = -\\frac{n}{2\\sigma^2} (\\mu - \\bar{x})^2 - \\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} + C $$\n",
    "\n",
    "Where $C$ is a constant that does not depend on $\\mu$. To find the MAP estimate, we differentiate the log-posterior with respect to $\\mu$, set the derivative to zero, and solve for $\\mu$:\n",
    "\n",
    "$$ \\hat{\\mu}_{MAP} = \\frac{\\sigma^2 \\mu_0 + n\\sigma_0^2 \\bar{x}}{n\\sigma_0^2 + \\sigma^2} $$\n",
    "\n",
    "This is the MAP estimate for $\\mu$, which is a weighted average of the prior mean $\\mu_0$ and the sample mean $\\bar{x}$, with weights depending on the variances of the prior and the data.\n",
    "\n",
    "### 4.2 Interpretation of MAP\n",
    "The MAP estimator introduces regularization into the estimation process by incorporating prior information. When the prior variance $\\sigma_0^2$ is large (i.e., weak prior belief), the MAP estimate approaches the MLE. Conversely, when the prior variance is small (i.e., strong prior belief), the MAP estimate is biased towards the prior mean $\\mu_0$. This regularization effect is particularly useful in situations where the data is sparse, and prior knowledge can guide the estimation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10c1d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T17:27:19.955663Z",
     "iopub.status.busy": "2025-02-26T17:27:19.955346Z",
     "iopub.status.idle": "2025-02-26T17:27:19.962842Z",
     "shell.execute_reply": "2025-02-26T17:27:19.962013Z"
    },
    "papermill": {
     "duration": 0.01387,
     "end_time": "2025-02-26T17:27:19.964780",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.950910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP estimate for mu: 2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def map_estimation(data, mu_prior, sigma2_prior, sigma2_data):\n",
    "    \"\"\"\n",
    "    MAP estimate for the mean of a normal distribution with a Gaussian prior.\n",
    "    \n",
    "    Args:\n",
    "    - data: Array of data points.\n",
    "    - mu_prior: Mean of the Gaussian prior on mu.\n",
    "    - sigma2_prior: Variance of the Gaussian prior on mu.\n",
    "    - sigma2_data: Variance of the data (assumed known).\n",
    "    \n",
    "    Returns:\n",
    "    - mu_map: The MAP estimate of mu.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    sample_mean = np.mean(data)\n",
    "    \n",
    "    # MAP estimate formula\n",
    "    mu_map = (sigma2_data * mu_prior + n * sigma2_prior * sample_mean) / (n * sigma2_prior + sigma2_data)\n",
    "    return mu_map\n",
    "\n",
    "# Example data and prior parameters\n",
    "data = np.array([2.3, 2.5, 2.7, 2.9, 3.1])  # Replace with your actual data points\n",
    "mu_prior = 2.0         # Mean of the Gaussian prior\n",
    "sigma2_prior = 0.5     # Variance of the Gaussian prior\n",
    "sigma2_data = 1.0      # Known variance of data\n",
    "\n",
    "# Calculate the MAP estimate\n",
    "mu_map_estimate = map_estimation(data, mu_prior, sigma2_prior, sigma2_data)\n",
    "\n",
    "print(f\"MAP estimate for mu: {mu_map_estimate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a8a7e",
   "metadata": {
    "papermill": {
     "duration": 0.002566,
     "end_time": "2025-02-26T17:27:19.970780",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.968214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Relationship Between MSE, MAE, and Likelihoods\n",
    "\n",
    "### 5.1 Mean Squared Error (MSE) and Gaussian Likelihood\n",
    "Mean Squared Error (MSE) is a commonly used loss function in regression models. It can be derived from the assumption that the errors (residuals) are normally distributed. Suppose the model predictions are $f(x_i)$ and the true values are $y_i$, then the residuals $r_i = y_i - f(x_i)$ follow a normal distribution:\n",
    "\n",
    "$$ r_i \\sim \\mathcal{N}(0, \\sigma^2) $$\n",
    "\n",
    "The likelihood of the residuals is:\n",
    "\n",
    "$$ L(\\sigma^2; r) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "The negative log-likelihood (NLL) for this Gaussian model is:\n",
    "\n",
    "$$ \\text{NLL} = \\frac{n}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} r_i^2 $$\n",
    "\n",
    "Minimizing the NLL with respect to $\\sigma^2$ is equivalent to minimizing the MSE:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 $$\n",
    "\n",
    "### 5.2 Mean Absolute Error (MAE) and Laplace Likelihood\n",
    "Mean Absolute Error (MAE) is another common loss function, particularly when robustness to outliers is desired. It corresponds to assuming that the residuals follow a Laplace distribution:\n",
    "\n",
    "$$ r_i \\sim \\text{Laplace}(0, b) $$\n",
    "\n",
    "The likelihood of the residuals is:\n",
    "\n",
    "$$ L(b; r) = \\prod_{i=1}^{n} \\frac{1}{2b} \\exp\\left(-\\frac{|r_i|}{b}\\right) $$\n",
    "\n",
    "The negative log-likelihood for this model is:\n",
    "\n",
    "$$ \\text{NLL} = n \\log(2b) + \\frac{1}{b} \\sum_{i=1}^{n} |r_i| $$\n",
    "\n",
    "Minimizing this NLL with respect to $b$ is equivalent to minimizing the MAE:\n",
    "\n",
    "$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |r_i| $$\n",
    "\n",
    "### 5.3 Connection to NLL\n",
    "Both MSE and MAE can be viewed as special cases of the negative log-likelihood (NLL). MSE corresponds to assuming Gaussian noise, while MAE corresponds to assuming Laplace noise. By framing regression problems in this probabilistic context, we can better understand the assumptions underlying different loss functions and choose the appropriate model for the given data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2808875d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T17:27:19.977080Z",
     "iopub.status.busy": "2025-02-26T17:27:19.976774Z",
     "iopub.status.idle": "2025-02-26T17:27:19.984219Z",
     "shell.execute_reply": "2025-02-26T17:27:19.983135Z"
    },
    "papermill": {
     "duration": 0.012679,
     "end_time": "2025-02-26T17:27:19.985864",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.973185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009999999999999981\n",
      "Gaussian likelihood: 82.94956719377812\n"
     ]
    }
   ],
   "source": [
    "# MSE calculation and Gaussian likelihood implementation\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error (MSE).\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def gaussian_likelihood(residuals, sigma2):\n",
    "    \"\"\"Compute the Gaussian likelihood of residuals.\"\"\"\n",
    "    n = len(residuals)\n",
    "    likelihood = (1 / np.sqrt(2 * np.pi * sigma2)) ** n * np.exp(-np.sum(residuals**2) / (2 * sigma2))\n",
    "    return likelihood\n",
    "\n",
    "# Example data\n",
    "y_true = np.array([2.3, 2.5, 3.1, 3.8, 2.7])\n",
    "y_pred = np.array([2.4, 2.6, 3.0, 3.7, 2.8])\n",
    "residuals = y_true - y_pred\n",
    "\n",
    "# MSE and Gaussian likelihood\n",
    "mse_value = mean_squared_error(y_true, y_pred)\n",
    "gaussian_likelihood_value = gaussian_likelihood(residuals, mse_value)\n",
    "\n",
    "print(f\"MSE: {mse_value}\")\n",
    "print(f\"Gaussian likelihood: {gaussian_likelihood_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10051bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T17:27:19.992184Z",
     "iopub.status.busy": "2025-02-26T17:27:19.991858Z",
     "iopub.status.idle": "2025-02-26T17:27:19.999457Z",
     "shell.execute_reply": "2025-02-26T17:27:19.998160Z"
    },
    "papermill": {
     "duration": 0.012715,
     "end_time": "2025-02-26T17:27:20.001195",
     "exception": false,
     "start_time": "2025-02-26T17:27:19.988480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.09999999999999991\n",
      "Laplace likelihood: 21.05608437214218\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import laplace\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Absolute Error (MAE).\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def laplace_likelihood(residuals, scale):\n",
    "    \"\"\"Compute the Laplace likelihood of residuals.\"\"\"\n",
    "    n = len(residuals)\n",
    "    likelihood = (1 / (2 * scale)) ** n * np.exp(-np.sum(np.abs(residuals)) / scale)\n",
    "    return likelihood\n",
    "\n",
    "# MAE and Laplace likelihood\n",
    "mae_value = mean_absolute_error(y_true, y_pred)\n",
    "laplace_likelihood_value = laplace_likelihood(residuals, mae_value)\n",
    "\n",
    "print(f\"MAE: {mae_value}\")\n",
    "print(f\"Laplace likelihood: {laplace_likelihood_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567a0e2",
   "metadata": {
    "papermill": {
     "duration": 0.002321,
     "end_time": "2025-02-26T17:27:20.007981",
     "exception": false,
     "start_time": "2025-02-26T17:27:20.005660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Thank You for Exploring This Notebook!\n",
    "\n",
    "\n",
    "If you have any questions, suggestions, or just want to discuss any of the topics further, please don't hesitate to reach out or leave a comment. Your feedback is not only welcome but also invaluable!\n",
    "\n",
    "Happy analyzing, and stay curious!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.812809,
   "end_time": "2025-02-26T17:27:20.432480",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-26T17:27:14.619671",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
