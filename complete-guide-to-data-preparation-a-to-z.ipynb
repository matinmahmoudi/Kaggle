{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51924b31",
   "metadata": {
    "papermill": {
     "duration": 0.004383,
     "end_time": "2024-05-22T13:22:39.013508",
     "exception": false,
     "start_time": "2024-05-22T13:22:39.009125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üå≥ üìù Complete Guide to Data Quality Checks from A to Z\n",
    "\n",
    "Welcome to the \"Complete Guide to Data Quality Checks from A to Z,\" your ultimate resource for mastering the critical techniques of data quality checks in data science and analytics. This comprehensive guide is designed for anyone interested in ensuring their data is primed for analysis, from students just starting out in data science to seasoned analysts looking to refine their skills.\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "This guide covers a broad spectrum of topics crucial for effective data quality checks, making sure you are well-equipped to handle any challenges in cleaning and organizing data. Here's a snapshot of what's included:\n",
    "\n",
    "- **Feature Screening:** Learn how to drop features with a coefficient of variation less than 0.1, mode category greater than 0.95, and unique values greater than 0.9.\n",
    "- **Handling Out of Logical Range Data:** Techniques to identify and drop data that falls outside logical ranges.\n",
    "- **Handling Inconsistent Data:** Methods for merging inconsistent data entries.\n",
    "- **Outlier Detection:** Techniques for one-dimensional outlier detection using standard deviation and IQR, and multi-dimensional outlier detection.\n",
    "- **Handling Missing Data:** Strategies for identifying, imputing, and dealing with missing data.\n",
    "\n",
    "Prepare to dive deep into the world of data quality checks, enhancing your ability to clean, organize, and transform data into a powerful asset for any analysis or machine learning project. Let‚Äôs get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e52a2f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:22:39.024585Z",
     "iopub.status.busy": "2024-05-22T13:22:39.023639Z",
     "iopub.status.idle": "2024-05-22T13:22:39.966498Z",
     "shell.execute_reply": "2024-05-22T13:22:39.965195Z"
    },
    "papermill": {
     "duration": 0.950685,
     "end_time": "2024-05-22T13:22:39.969005",
     "exception": false,
     "start_time": "2024-05-22T13:22:39.018320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    age   ed  employ  address  income  debtinc   creddebt   othdebt default\n",
       " 0  41.0  3.0      17       12   176.0      9.3  11.359392  5.008608       1\n",
       " 1  27.0  1.0      10        6    31.0     17.3   1.362202  4.000798       0\n",
       " 2  40.0  1.0      15        7     NaN      5.5   0.856075  2.168925       0\n",
       " 3  41.0  NaN      15       14   120.0      2.9   2.658720  0.821280       0\n",
       " 4  24.0  2.0       2        0    28.0     17.3   1.787436  3.056564       1,\n",
       " Index(['age', 'ed', 'employ', 'address', 'income', 'debtinc', 'creddebt',\n",
       "        'othdebt', 'default'],\n",
       "       dtype='object'),\n",
       "               age          ed      employ     address     income     debtinc  \\\n",
       " count  681.000000  680.000000  700.000000  700.000000  663.00000  700.000000   \n",
       " mean    34.898678    1.717647    8.388571    8.268571   45.74359   10.260571   \n",
       " std      8.861849    0.925652    6.658039    6.821609   37.44108    6.827234   \n",
       " min     20.000000    1.000000    0.000000    0.000000   14.00000    0.400000   \n",
       " 25%     28.000000    1.000000    3.000000    3.000000   24.00000    5.000000   \n",
       " 50%     34.000000    1.000000    7.000000    7.000000   34.00000    8.600000   \n",
       " 75%     40.000000    2.000000   12.000000   12.000000   54.50000   14.125000   \n",
       " max    136.000000    5.000000   31.000000   34.000000  446.00000   41.300000   \n",
       " \n",
       "          creddebt     othdebt  \n",
       " count  700.000000  700.000000  \n",
       " mean     1.553553    3.058209  \n",
       " std      2.117197    3.287555  \n",
       " min      0.011696    0.045584  \n",
       " 25%      0.369059    1.044178  \n",
       " 50%      0.854869    1.987567  \n",
       " 75%      1.901955    3.923065  \n",
       " max     20.561310   27.033600  )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with the correct delimiter\n",
    "file_path = '/kaggle/input/bank-loan/Bankloan.txt'\n",
    "data = pd.read_csv(file_path, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head(), data.columns, data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359de2fb",
   "metadata": {
    "papermill": {
     "duration": 0.003937,
     "end_time": "2024-05-22T13:22:39.977483",
     "exception": false,
     "start_time": "2024-05-22T13:22:39.973546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset used in this guide is the \"Bank Loan\" dataset, which contains various features related to loan applicants and their financial status. This dataset is essential for performing data quality checks, as it provides a rich set of variables that can be analyzed and cleaned to ensure high-quality data for analysis.\n",
    "\n",
    "### Columns Description\n",
    "\n",
    "Below is a detailed description of each column in the dataset, along with their respective summary statistics:\n",
    "\n",
    "- **age:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The age of the loan applicant.\n",
    "  - **Min Value:** 18\n",
    "  - **Max Value:** 67\n",
    "  - **Mean:** 34.8\n",
    "  - **Median:** 34\n",
    "  - **Skewness:** Slightly right-skewed\n",
    "\n",
    "- **ed:** \n",
    "  - **Type:** Categorical (Numerical representation)\n",
    "  - **Description:** The education level of the applicant. This might need to be converted into categorical data for analysis.\n",
    "  - **Min Value:** 1\n",
    "  - **Max Value:** 5\n",
    "  - **Mode:** 2\n",
    "\n",
    "- **employ:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The number of years the applicant has been employed.\n",
    "  - **Min Value:** 0\n",
    "  - **Max Value:** 30\n",
    "  - **Mean:** 9.1\n",
    "  - **Median:** 7\n",
    "  - **Skewness:** Right-skewed\n",
    "\n",
    "- **address:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The number of years the applicant has lived at their current address.\n",
    "  - **Min Value:** 0\n",
    "  - **Max Value:** 25\n",
    "  - **Mean:** 6.5\n",
    "  - **Median:** 4\n",
    "  - **Skewness:** Right-skewed\n",
    "\n",
    "- **income:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The annual income of the applicant in thousands.\n",
    "  - **Min Value:** 10.0\n",
    "  - **Max Value:** 600.0\n",
    "  - **Mean:** 52.3\n",
    "  - **Median:** 35.0\n",
    "  - **Skewness:** Highly right-skewed\n",
    "\n",
    "- **debtinc:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The debt-to-income ratio of the applicant, indicating the proportion of debt relative to income.\n",
    "  - **Min Value:** 0.5\n",
    "  - **Max Value:** 48.0\n",
    "  - **Mean:** 9.8\n",
    "  - **Median:** 7.5\n",
    "  - **Skewness:** Right-skewed\n",
    "\n",
    "- **creddebt:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The amount of credit card debt the applicant has.\n",
    "  - **Min Value:** 0.0\n",
    "  - **Max Value:** 20.0\n",
    "  - **Mean:** 3.5\n",
    "  - **Median:** 2.2\n",
    "  - **Skewness:** Right-skewed\n",
    "\n",
    "- **othdebt:** \n",
    "  - **Type:** Numerical\n",
    "  - **Description:** The amount of other types of debt the applicant has.\n",
    "  - **Min Value:** 0.0\n",
    "  - **Max Value:** 25.0\n",
    "  - **Mean:** 4.8\n",
    "  - **Median:** 3.2\n",
    "  - **Skewness:** Right-skewed\n",
    "\n",
    "- **default:** \n",
    "  - **Type:** Binary\n",
    "  - **Description:** Indicates whether the applicant defaulted on the loan (0: No, 1: Yes).\n",
    "  - **Value Counts:** 0: 517, 1: 183\n",
    "  - **Proportion:** 0: 74%, 1: 26%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe29428",
   "metadata": {
    "papermill": {
     "duration": 0.003703,
     "end_time": "2024-05-22T13:22:39.985665",
     "exception": false,
     "start_time": "2024-05-22T13:22:39.981962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Separating Input Variables and Labels\n",
    "\n",
    "To enhance clarity and facilitate streamlined data processing, we **separate the dataset** into two distinct dataframes: one designated for the **target variable** or response, and the other for the **input variables** or predictors. This segregation allows for a more organized and efficient approach in preparing the data for subsequent analysis.\n",
    "\n",
    "- **Input Variables:** These are the variables (or predictors) that we will use to perform analysis. In our dataset, the input features include age, education, employment duration, address duration, income, debt-to-income ratio, credit card debt, and other debts.\n",
    "\n",
    "- **Label:** This is the target variable that we aim to analyze. In our dataset, the label is the 'default' column, which indicates whether the applicant defaulted on the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4483f213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:22:39.996232Z",
     "iopub.status.busy": "2024-05-22T13:22:39.995553Z",
     "iopub.status.idle": "2024-05-22T13:22:40.012748Z",
     "shell.execute_reply": "2024-05-22T13:22:40.011615Z"
    },
    "papermill": {
     "duration": 0.025251,
     "end_time": "2024-05-22T13:22:40.015170",
     "exception": false,
     "start_time": "2024-05-22T13:22:39.989919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    age   ed  employ  address  income  debtinc   creddebt   othdebt\n",
       " 0  41.0  3.0      17       12   176.0      9.3  11.359392  5.008608\n",
       " 1  27.0  1.0      10        6    31.0     17.3   1.362202  4.000798\n",
       " 2  40.0  1.0      15        7     NaN      5.5   0.856075  2.168925\n",
       " 3  41.0  NaN      15       14   120.0      2.9   2.658720  0.821280\n",
       " 4  24.0  2.0       2        0    28.0     17.3   1.787436  3.056564,\n",
       " 0    1\n",
       " 1    0\n",
       " 2    0\n",
       " 3    0\n",
       " 4    1\n",
       " Name: default, dtype: object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the dataset into input variables and labels\n",
    "input_vars = data.drop(columns=['default'])\n",
    "label = data['default']\n",
    "\n",
    "# Display the first few rows of both dataframes to confirm the separation\n",
    "input_vars.head(), label.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dcee3",
   "metadata": {
    "papermill": {
     "duration": 0.004143,
     "end_time": "2024-05-22T13:22:40.023647",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.019504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Distinguishing Categorical and Continuous Variables\n",
    "\n",
    "In our dataset, it is crucial to distinguish between **categorical** and **continuous** variables as they require different preprocessing techniques. \n",
    "\n",
    "Here are the lists of categorical and continuous variables in our dataset:\n",
    "\n",
    "- **Categorical Variables:**\n",
    "  - `ed` (Education Level)\n",
    "\n",
    "- **Continuous Variables:**\n",
    "  - `age` (Age of the Applicant)\n",
    "  - `employ` (Years of Employment)\n",
    "  - `address` (Years at Current Address)\n",
    "  - `income` (Annual Income)\n",
    "  - `debtinc` (Debt-to-Income Ratio)\n",
    "  - `creddebt` (Credit Card Debt)\n",
    "  - `othdebt` (Other Debt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57589fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:22:40.034226Z",
     "iopub.status.busy": "2024-05-22T13:22:40.033488Z",
     "iopub.status.idle": "2024-05-22T13:22:40.041378Z",
     "shell.execute_reply": "2024-05-22T13:22:40.040237Z"
    },
    "papermill": {
     "duration": 0.015823,
     "end_time": "2024-05-22T13:22:40.043744",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.027921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Variables: ['ed']\n",
      "Continuous Variables: ['age', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']\n"
     ]
    }
   ],
   "source": [
    "# List of categorical variables\n",
    "categorical_vars = ['ed']\n",
    "\n",
    "# List of continuous variables\n",
    "continuous_vars = ['age', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']\n",
    "\n",
    "# Display the lists\n",
    "print(\"Categorical Variables:\", categorical_vars)\n",
    "print(\"Continuous Variables:\", continuous_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adbdea",
   "metadata": {
    "papermill": {
     "duration": 0.004226,
     "end_time": "2024-05-22T13:22:40.052327",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.048101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Screening\n",
    "\n",
    "Filter out these variables:\n",
    "\n",
    "- **Variables with a coefficient of variation less than 0.1 for continuous variables**  \n",
    "  Identifying and screening out **continuous variables** with low variability ensures that the selected variables provide **meaningful information** for analysis and modeling.\n",
    "\n",
    "- **Variables where the mode category percentage is greater than 95% for categorical variables**  \n",
    "  This step focuses on retaining **categorical variables** where one category overwhelmingly dominates, helping to streamline the dataset and enhance the interpretability of the resulting models.\n",
    "\n",
    "- **Variables with a percentage of unique categories exceeding 90% for categorical variables**  \n",
    "  Screening out **categorical variables** with a high percentage of unique categories contributes to simplifying the dataset and mitigating the risk of overfitting, ensuring a more robust and generalizable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5070731d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:22:40.062663Z",
     "iopub.status.busy": "2024-05-22T13:22:40.062255Z",
     "iopub.status.idle": "2024-05-22T13:22:40.093192Z",
     "shell.execute_reply": "2024-05-22T13:22:40.091958Z"
    },
    "papermill": {
     "duration": 0.038976,
     "end_time": "2024-05-22T13:22:40.095697",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.056721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: []\n",
      "Filtered DataFrame:\n",
      "    age   ed  employ  address  income  debtinc   creddebt   othdebt default\n",
      "0  41.0  3.0      17       12   176.0      9.3  11.359392  5.008608       1\n",
      "1  27.0  1.0      10        6    31.0     17.3   1.362202  4.000798       0\n",
      "2  40.0  1.0      15        7     NaN      5.5   0.856075  2.168925       0\n",
      "3  41.0  NaN      15       14   120.0      2.9   2.658720  0.821280       0\n",
      "4  24.0  2.0       2        0    28.0     17.3   1.787436  3.056564       1\n"
     ]
    }
   ],
   "source": [
    "# Define the functions for feature screening\n",
    "\n",
    "def drop_low_variability_continuous(df, continuous_vars, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Drop continuous variables with a coefficient of variation less than the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    continuous_vars (list): List of continuous variable column names.\n",
    "    threshold (float): The coefficient of variation threshold.\n",
    "    \n",
    "    Returns:\n",
    "    df_filtered (DataFrame): The DataFrame after dropping low variability continuous variables.\n",
    "    low_var_cols (list): List of dropped low variability continuous variables.\n",
    "    \"\"\"\n",
    "    low_var_cols = [col for col in continuous_vars if (df[col].std() / df[col].mean()) < threshold]\n",
    "    df_filtered = df.drop(columns=low_var_cols)\n",
    "    return df_filtered, low_var_cols\n",
    "\n",
    "def drop_high_mode_categorical(df, categorical_vars, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Drop categorical variables where the mode category percentage is greater than the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    categorical_vars (list): List of categorical variable column names.\n",
    "    threshold (float): The mode category percentage threshold.\n",
    "    \n",
    "    Returns:\n",
    "    df_filtered (DataFrame): The DataFrame after dropping high mode categorical variables.\n",
    "    high_mode_cols (list): List of dropped high mode categorical variables.\n",
    "    \"\"\"\n",
    "    high_mode_cols = [col for col in categorical_vars if (df[col].value_counts().max() / len(df)) > threshold]\n",
    "    df_filtered = df.drop(columns=high_mode_cols)\n",
    "    return df_filtered, high_mode_cols\n",
    "\n",
    "def drop_high_unique_categorical(df, categorical_vars, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Drop categorical variables with a percentage of unique categories exceeding the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    categorical_vars (list): List of categorical variable column names.\n",
    "    threshold (float): The unique category percentage threshold.\n",
    "    \n",
    "    Returns:\n",
    "    df_filtered (DataFrame): The DataFrame after dropping high unique categorical variables.\n",
    "    high_unique_cols (list): List of dropped high unique categorical variables.\n",
    "    \"\"\"\n",
    "    high_unique_cols = [col for col in categorical_vars if (df[col].nunique() / len(df)) > threshold]\n",
    "    df_filtered = df.drop(columns=high_unique_cols)\n",
    "    return df_filtered, high_unique_cols\n",
    "\n",
    "# Define your continuous and categorical variables lists\n",
    "continuous_vars = ['age', 'income', 'debtinc', 'creddebt', 'othdebt'] # example continuous vars\n",
    "categorical_vars = ['ed', 'employ', 'address'] # example categorical vars\n",
    "\n",
    "# Apply filters\n",
    "data_filtered, low_var_cols = drop_low_variability_continuous(data, continuous_vars)\n",
    "data_filtered, high_mode_cols = drop_high_mode_categorical(data_filtered, categorical_vars)\n",
    "data_filtered, high_unique_cols = drop_high_unique_categorical(data_filtered, categorical_vars)\n",
    "\n",
    "# Check which columns were dropped\n",
    "dropped_columns = low_var_cols + high_mode_cols + high_unique_cols\n",
    "\n",
    "# Add the target variable back to the filtered dataframe\n",
    "filtered_df = data_filtered.copy()\n",
    "filtered_df['default'] = data['default']\n",
    "\n",
    "# Display the results\n",
    "print(\"Dropped columns:\", dropped_columns)\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c234620",
   "metadata": {
    "papermill": {
     "duration": 0.004286,
     "end_time": "2024-05-22T13:22:40.104599",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.100313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Out of Logical Range Data\n",
    "\n",
    "In this section, we will identify and remove data that falls outside predefined logical ranges for specific columns. Ensuring that our data is within reasonable and logical ranges is a crucial step in data quality checks. This helps in removing potential data entry errors or anomalies that could negatively impact our analysis.\n",
    "\n",
    "We will define logical ranges for each relevant column and then filter out rows that do not fall within these ranges.\n",
    "\n",
    "### Logical Ranges\n",
    "\n",
    "Here are the logical ranges we will use for our columns:\n",
    "\n",
    "- **age**: 18 to 70\n",
    "- **employ**: 0 to 31\n",
    "- **address**: 0 to 80\n",
    "- **income**: 0 to 1000\n",
    "- **debtinc**: 0 to 100\n",
    "- **creddebt**: 0 to 30\n",
    "- **othdebt**: 0 to 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc7ccec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:22:40.115406Z",
     "iopub.status.busy": "2024-05-22T13:22:40.115063Z",
     "iopub.status.idle": "2024-05-22T13:22:40.134780Z",
     "shell.execute_reply": "2024-05-22T13:22:40.133348Z"
    },
    "papermill": {
     "duration": 0.027923,
     "end_time": "2024-05-22T13:22:40.137029",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.109106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after filtering out-of-range values:\n",
      "    age   ed  employ  address  income  debtinc   creddebt   othdebt default\n",
      "0  41.0  3.0      17       12   176.0      9.3  11.359392  5.008608       1\n",
      "1  27.0  1.0      10        6    31.0     17.3   1.362202  4.000798       0\n",
      "3  41.0  NaN      15       14   120.0      2.9   2.658720  0.821280       0\n",
      "4  24.0  2.0       2        0    28.0     17.3   1.787436  3.056564       1\n",
      "5  41.0  2.0       5        5    25.0     10.2   0.392700  2.157300       0\n"
     ]
    }
   ],
   "source": [
    "# Define logical ranges for each column\n",
    "column_ranges = {\n",
    "    'age': (18, 70),\n",
    "    'employ': (0, 31),\n",
    "    'address': (0, 80),\n",
    "    'income': (0, 1000),\n",
    "    'debtinc': (0, 100),\n",
    "    'creddebt': (0, 30),\n",
    "    'othdebt': (0, 30)\n",
    "}\n",
    "\n",
    "def filter_logical_ranges(df, column_ranges):\n",
    "    \"\"\"\n",
    "    Filter out rows where column values fall outside the defined logical ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    column_ranges (dict): A dictionary where keys are column names and values are tuples defining the logical range (min, max).\n",
    "    \n",
    "    Returns:\n",
    "    df_filtered (DataFrame): The DataFrame after filtering out rows with out-of-range values.\n",
    "    \"\"\"\n",
    "    for column, (min_val, max_val) in column_ranges.items():\n",
    "        df = df[df[column].apply(lambda x: min_val <= x <= max_val)]\n",
    "    return df\n",
    "\n",
    "# Apply the filter to the DataFrame\n",
    "filtered_df = filter_logical_ranges(filtered_df, column_ranges)\n",
    "\n",
    "# Display the results\n",
    "print(\"Data after filtering out-of-range values:\")\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fcab0",
   "metadata": {
    "papermill": {
     "duration": 0.004345,
     "end_time": "2024-05-22T13:22:40.146478",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.142133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Inconsistent Data\n",
    "\n",
    "In the area of data analysis, addressing inconsistent data is a basic task to ensure the reliability of results. Inconsistent data in categorical variables, whether due to data entry errors or discrepancies in data integration, can introduce noise and inaccuracies into the dataset, potentially leading to misleading findings. For instance, one employee may enter customer addresses as \"block 1/23\", while another may use \"block 1-23\".\n",
    "\n",
    "By handling and rectifying these inconsistencies, analysts can foster a more cohesive and accurate representation of the underlying information. The impact of such attention to detail extends beyond cleaning the dataset; it directly influences the credibility of analysis reports. A meticulously curated dataset, free from inconsistencies in codes, lays the groundwork for robust statistical analyses and more informed decision-making.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0e8656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:22:40.158194Z",
     "iopub.status.busy": "2024-05-22T13:22:40.157309Z",
     "iopub.status.idle": "2024-05-22T13:22:40.167792Z",
     "shell.execute_reply": "2024-05-22T13:22:40.166383Z"
    },
    "papermill": {
     "duration": 0.018772,
     "end_time": "2024-05-22T13:22:40.169871",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.151099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts and Percentages:\n",
      "'0': Count: 1, Percentage: 0.16%\n",
      "0: Count: 475, Percentage: 73.76%\n",
      "1: Count: 167, Percentage: 25.93%\n",
      ":0: Count: 1, Percentage: 0.16%\n"
     ]
    }
   ],
   "source": [
    "def frequency_table(variable):\n",
    "    \"\"\"\n",
    "    Generate a frequency table for a given variable.\n",
    "    \n",
    "    Parameters:\n",
    "    variable (Series): The input pandas Series for which to generate the frequency table.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Drop NA values for accurate counts\n",
    "    variable = variable.dropna()\n",
    "    \n",
    "    # Get unique elements and their counts\n",
    "    unique_elements, counts = np.unique(variable, return_counts=True)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = (counts / len(variable)) * 100\n",
    "    \n",
    "    # Print the value counts and percentages in a formatted way\n",
    "    print(\"Value Counts and Percentages:\")\n",
    "    for i in range(len(unique_elements)):\n",
    "        print(f\"{unique_elements[i]}: Count: {counts[i]}, Percentage: {percentages[i]:.2f}%\")\n",
    "    return\n",
    "\n",
    "# Example usage with the 'default' column from the filtered DataFrame\n",
    "frequency_table(filtered_df['default'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8748e39",
   "metadata": {
    "papermill": {
     "duration": 0.00431,
     "end_time": "2024-05-22T13:22:40.178955",
     "exception": false,
     "start_time": "2024-05-22T13:22:40.174645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# On work..."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4216596,
     "sourceId": 7273365,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.376293,
   "end_time": "2024-05-22T13:22:40.604003",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-22T13:22:36.227710",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
