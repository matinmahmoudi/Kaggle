{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd254f9f",
   "metadata": {
    "papermill": {
     "duration": 0.005225,
     "end_time": "2024-05-23T09:10:16.373071",
     "exception": false,
     "start_time": "2024-05-23T09:10:16.367846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üå≥ üìù Complete Guide to Data Quality from A to Z\n",
    "\n",
    "Welcome to this comprehensive guide on data quality, designed to equip you with the knowledge and skills to ensure the integrity and reliability of your datasets. Whether you're a budding data scientist or a seasoned professional looking to refine your data quality management skills, this notebook is tailored for you!\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "In this guide, we will explore various methods to assess, clean, and maintain data quality, ensuring you have the tools to confidently tackle any data-driven challenge. Here's what we'll cover:\n",
    "\n",
    "- **Feature Screening**: Learn how to identify and screen out features that do not contribute meaningful information to your analysis and modeling.\n",
    "  - **Features with a Coefficient of Variation Less than 0.1 for Continuous Variables**: Retain only those continuous features with significant variability.\n",
    "  \n",
    "  - **Features where the Mode Category Percentage is Greater than 95% for Categorical Variables**: Streamline your dataset by focusing on dominant categorical features.\n",
    "  \n",
    "  - **Features with a Percentage of Unique Categories Exceeding 90% for Categorical Variables**: Simplify your dataset by removing overly unique categorical features.\n",
    "  \n",
    "  \n",
    "\n",
    "- **Handling Out of Logical Range Data**: Address and correct values that fall outside logical ranges to maintain dataset integrity.\n",
    "\n",
    "- **Handling Inconsistent Data**: Resolve inconsistencies in categorical data to enhance the reliability of your analysis.\n",
    "\n",
    "- **Data Leakage**: Understand and prevent data leakage to ensure your machine learning models are robust and generalizable.\n",
    "\n",
    "- **Outlier Detection**: Employ one-dimensional and multidimensional methods to identify and manage outliers in your data.\n",
    "\n",
    "- **Handling Missing Data**: Learn various techniques for dealing with missing data, from simple imputation to advanced methods.\n",
    "\n",
    "## Why This Guide?\n",
    "\n",
    "- **Step-by-Step Tutorials**: Each section includes clear explanations followed by practical examples, ensuring you not only learn but also apply your knowledge.\n",
    "- **Interactive Learning**: Engage with interactive code cells that allow you to see the effects of data quality methods in real-time.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- **Run the Cells**: Follow along with the code examples by running the cells yourself. Modify the parameters to see how the results change.\n",
    "- **Explore Further**: After completing the guided sections, try applying the methods to your own datasets to reinforce your learning.\n",
    "\n",
    "Prepare to unlock the full potential of data quality management in data analysis. Let's dive in and transform data into reliable insights!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ca4ea",
   "metadata": {
    "papermill": {
     "duration": 0.004678,
     "end_time": "2024-05-23T09:10:16.383316",
     "exception": false,
     "start_time": "2024-05-23T09:10:16.378638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "To begin our analysis, we'll start by loading the dataset. This dataset contains information about bank loans, including various features such as age, education level, employment duration, address duration, income, debt-to-income ratio, credit card debt, other debt, and loan default status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a5abc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T09:10:16.395761Z",
     "iopub.status.busy": "2024-05-23T09:10:16.395094Z",
     "iopub.status.idle": "2024-05-23T09:10:17.632586Z",
     "shell.execute_reply": "2024-05-23T09:10:17.631373Z"
    },
    "papermill": {
     "duration": 1.247066,
     "end_time": "2024-05-23T09:10:17.635211",
     "exception": false,
     "start_time": "2024-05-23T09:10:16.388145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   ed  employ  address  income  debtinc   creddebt   othdebt default\n",
       "0  41.0  3.0      17       12   176.0      9.3  11.359392  5.008608       1\n",
       "1  27.0  1.0      10        6    31.0     17.3   1.362202  4.000798       0\n",
       "2  40.0  1.0      15        7     NaN      5.5   0.856075  2.168925       0\n",
       "3  41.0  NaN      15       14   120.0      2.9   2.658720  0.821280       0\n",
       "4  24.0  2.0       2        0    28.0     17.3   1.787436  3.056564       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "file_path = '/kaggle/input/bank-loan/Bankloan.txt' \n",
    "dataset = pd.read_csv(file_path, delimiter=\",\")\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8417209",
   "metadata": {
    "papermill": {
     "duration": 0.004872,
     "end_time": "2024-05-23T09:10:17.645171",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.640299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Explanation\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- **age**: The age of the applicant.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 20\n",
    "  - **Max**: 67\n",
    "  - **Mean**: 35.95\n",
    "  - **Median**: 34\n",
    "  - **Standard Deviation**: 11.36\n",
    "  - **Skewness**: 0.45 (slightly right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the age in years. This variable is important for understanding the demographic distribution of the applicants.\n",
    "\n",
    "\n",
    "- **ed**: The education level of the applicant, represented as an integer.\n",
    "  - **Type**: Categorical (Ordinal)\n",
    "  - **Min**: 1\n",
    "  - **Max**: 5\n",
    "  - **Mode**: 1\n",
    "  - **Missing Values**: 24 (4% of the dataset)\n",
    "  - **Details**: Represents the education level, where higher values indicate higher levels of education. This variable helps in assessing the education background of applicants.\n",
    "\n",
    "\n",
    "- **employ**: The number of years the applicant has been employed.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0\n",
    "  - **Max**: 35\n",
    "  - **Mean**: 9.8\n",
    "  - **Median**: 8\n",
    "  - **Standard Deviation**: 8.2\n",
    "  - **Skewness**: 0.65 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the number of years in employment. This variable is crucial for understanding the work experience of the applicants.\n",
    "\n",
    "\n",
    "- **address**: The number of years the applicant has lived at their current address.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0\n",
    "  - **Max**: 25\n",
    "  - **Mean**: 6.9\n",
    "  - **Median**: 4\n",
    "  - **Standard Deviation**: 7.2\n",
    "  - **Skewness**: 0.95 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the number of years at the current address. This variable helps in understanding the stability of the applicants' living situation.\n",
    "\n",
    "\n",
    "- **income**: The annual income of the applicant in thousands of dollars.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 8.0\n",
    "  - **Max**: 636.0\n",
    "  - **Mean**: 70.55\n",
    "  - **Median**: 40.0\n",
    "  - **Standard Deviation**: 66.4\n",
    "  - **Skewness**: 2.12 (highly right-skewed)\n",
    "  - **Missing Values**: 38 (6.3% of the dataset)\n",
    "  - **Details**: Represents the annual income in thousands. This variable is essential for assessing the financial status of the applicants.\n",
    "\n",
    "\n",
    "- **debtinc**: The debt-to-income ratio of the applicant, expressed as a percentage.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0.0\n",
    "  - **Max**: 69.9\n",
    "  - **Mean**: 10.1\n",
    "  - **Median**: 8.9\n",
    "  - **Standard Deviation**: 8.7\n",
    "  - **Skewness**: 1.4 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the debt-to-income ratio. This variable helps in understanding the financial burden on the applicants.\n",
    "\n",
    "\n",
    "- **creddebt**: The amount of credit card debt the applicant has, in thousands of dollars.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0.0\n",
    "  - **Max**: 11.36\n",
    "  - **Mean**: 3.55\n",
    "  - **Median**: 2.30\n",
    "  - **Standard Deviation**: 3.41\n",
    "  - **Skewness**: 0.75 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the credit card debt in thousands. This variable indicates the credit card liabilities of the applicants.\n",
    "\n",
    "\n",
    "- **othdebt**: The amount of other debt the applicant has, in thousands of dollars.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0.0\n",
    "  - **Max**: 11.0\n",
    "  - **Mean**: 3.02\n",
    "  - **Median**: 2.20\n",
    "  - **Standard Deviation**: 2.24\n",
    "  - **Skewness**: 0.95 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents other debts in thousands. This variable shows additional financial liabilities apart from credit card debt.\n",
    "  \n",
    "\n",
    "- **default**: The default status of the loan, where 1 indicates default and 0 indicates no default.\n",
    "  - **Type**: Categorical (Binary)\n",
    "  - **Unique Values**: [0, 1]\n",
    "  - **Mode**: 0\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Binary indicator of loan default status. This is the target variable for modeling and analysis.\n",
    "\n",
    "If you want to learn how to perform detailed data profiling and obtain these insights, visit the [Complete Guide to Data Profiling A to Z](https://www.kaggle.com/code/matinmahmoudi/complete-guide-to-data-profiling-a-to-z).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9d633",
   "metadata": {
    "papermill": {
     "duration": 0.00469,
     "end_time": "2024-05-23T09:10:17.654923",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.650233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Screening\n",
    "\n",
    "Feature screening is a crucial step in the data quality process that involves identifying and removing features (variables) that do not contribute meaningful information to the analysis or modeling. By screening out such features, we can streamline the dataset, improve model performance, and enhance interpretability. In this section, we will cover three specific criteria for feature screening:\n",
    "\n",
    "### Features with a Coefficient of Variation Less than 0.1 for Continuous Variables\n",
    "\n",
    "The coefficient of variation (CV) is a measure of relative variability. It is calculated as the ratio of the standard deviation to the mean. Features with a CV less than 0.1 are considered to have low variability and may not provide significant information for analysis. We will identify and remove such features.\n",
    "\n",
    "### Features where the Mode Category Percentage is Greater than 95% for Categorical Variables\n",
    "\n",
    "Categorical variables where a single category overwhelmingly dominates (mode category percentage > 95%) may not be useful for analysis as they do not provide much variation. We will identify and remove these categorical features to streamline the dataset.\n",
    "\n",
    "### Features with a Percentage of Unique Categories Exceeding 90% for Categorical Variables\n",
    "\n",
    "Categorical variables with a high percentage of unique categories ( > 90%) can complicate the analysis and lead to overfitting in models. We will identify and remove these features to ensure a more robust and generalizable model.\n",
    "\n",
    "By applying these screening criteria, we can ensure that the remaining features in the dataset provide meaningful and relevant information for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58248792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T09:10:17.668066Z",
     "iopub.status.busy": "2024-05-23T09:10:17.667171Z",
     "iopub.status.idle": "2024-05-23T09:10:17.733955Z",
     "shell.execute_reply": "2024-05-23T09:10:17.732273Z"
    },
    "papermill": {
     "duration": 0.077717,
     "end_time": "2024-05-23T09:10:17.737652",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.659935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with Coefficient of Variation less than 0.1: []\n",
      "Categorical features where mode category percentage is greater than 95%: []\n",
      "Categorical features with percentage of unique categories exceeding 90%: []\n",
      "Features to be removed: set()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   ed  employ  address  income  debtinc   creddebt   othdebt default\n",
       "0  41.0  3.0      17       12   176.0      9.3  11.359392  5.008608       1\n",
       "1  27.0  1.0      10        6    31.0     17.3   1.362202  4.000798       0\n",
       "2  40.0  1.0      15        7     NaN      5.5   0.856075  2.168925       0\n",
       "3  41.0  NaN      15       14   120.0      2.9   2.658720  0.821280       0\n",
       "4  24.0  2.0       2        0    28.0     17.3   1.787436  3.056564       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the dataset into input variables (predictors) and target variable (response)\n",
    "label = dataset['default']\n",
    "inputs = dataset.drop(columns=['default'])\n",
    "\n",
    "categorical_columns = ['ed']  \n",
    "numerical_columns = ['age', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']\n",
    "\n",
    "# Calculate Coefficient of Variation for continuous variables\n",
    "cv = inputs[numerical_columns].std() / inputs[numerical_columns].mean()\n",
    "\n",
    "# Identify features with CV less than 0.1\n",
    "low_cv_features = cv[cv < 0.1].index.tolist()\n",
    "print(\"Features with Coefficient of Variation less than 0.1:\", low_cv_features)\n",
    "\n",
    "# Calculate Mode Category Percentage for categorical variables\n",
    "mode_percentage = inputs[categorical_columns].apply(lambda x: x.value_counts(normalize=True).max() * 100)\n",
    "\n",
    "# Identify features where the mode category percentage is greater than 95%\n",
    "high_mode_features = mode_percentage[mode_percentage > 95].index.tolist()\n",
    "print(\"Categorical features where mode category percentage is greater than 95%:\", high_mode_features)\n",
    "\n",
    "# Calculate Percentage of Unique Categories for categorical variables\n",
    "unique_category_percentage = inputs[categorical_columns].nunique() / len(inputs) * 100\n",
    "\n",
    "# Identify features with a percentage of unique categories exceeding 90%\n",
    "high_unique_features = unique_category_percentage[unique_category_percentage > 90].index.tolist()\n",
    "print(\"Categorical features with percentage of unique categories exceeding 90%:\", high_unique_features)\n",
    "\n",
    "# Combine all features to be removed\n",
    "features_to_remove = set(low_cv_features + high_mode_features + high_unique_features)\n",
    "print(\"Features to be removed:\", features_to_remove)\n",
    "\n",
    "# Remove the identified features from the inputs dataframe\n",
    "cleaned_inputs = inputs.drop(columns=features_to_remove)\n",
    "\n",
    "# Combine the cleaned inputs with the label\n",
    "cleaned_dataset = pd.concat([cleaned_inputs, label], axis=1)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734ab2f",
   "metadata": {
    "papermill": {
     "duration": 0.006299,
     "end_time": "2024-05-23T09:10:17.750098",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.743799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Out of Logical Range Data\n",
    "\n",
    "In data analysis, handling values that fall outside the logical range of respective fields is a critical step to maintain the integrity and reliability of the dataset. Values significantly deviating from the expected range can distort analytical results and impact the overall quality of findings. It is essential to define these ranges based on domain knowledge, business rules, and the specific context of the data.\n",
    "\n",
    "### Defining Logical Ranges\n",
    "\n",
    "To ensure the data is within logical boundaries, we define acceptable ranges for each column based on reasonable assumptions and domain knowledge. Here are the defined ranges for each column in our dataset:\n",
    "\n",
    "- **age**: 18 to 70 years - This range assumes the typical age range of bank loan applicants.\n",
    "- **employ**: 0 to 31 years - This range covers the typical employment duration for most individuals.\n",
    "- **address**: 0 to 80 years - This range represents the duration someone might live at a given address.\n",
    "- **income**: 0 to 1000 thousand dollars - This upper limit is set to include high-income individuals while excluding outliers.\n",
    "- **debtinc**: 0 to 100 percent - This range covers the debt-to-income ratio, with 100% being the upper logical limit.\n",
    "- **creddebt**: 0 to 30 thousand dollars - This range is set to include typical credit card debt amounts.\n",
    "- **othdebt**: 0 to 30 thousand dollars - This range includes other types of debt and is set to exclude extreme outliers.\n",
    "\n",
    "By adhering to these logical ranges, we can filter out anomalous data points that may otherwise skew our analysis and ensure a more accurate and reliable dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64164d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T09:10:17.764263Z",
     "iopub.status.busy": "2024-05-23T09:10:17.763425Z",
     "iopub.status.idle": "2024-05-23T09:10:17.798886Z",
     "shell.execute_reply": "2024-05-23T09:10:17.797650Z"
    },
    "papermill": {
     "duration": 0.04579,
     "end_time": "2024-05-23T09:10:17.801511",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.755721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   ed  employ  address  income  debtinc   creddebt   othdebt default\n",
       "0  41.0  3.0    17.0     12.0   176.0      9.3  11.359392  5.008608       1\n",
       "1  27.0  1.0    10.0      6.0    31.0     17.3   1.362202  4.000798       0\n",
       "3  41.0  NaN    15.0     14.0   120.0      2.9   2.658720  0.821280       0\n",
       "4  24.0  2.0     2.0      0.0    28.0     17.3   1.787436  3.056564       1\n",
       "5  41.0  2.0     5.0      5.0    25.0     10.2   0.392700  2.157300       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define ranges for each column\n",
    "column_ranges = {\n",
    "    'age': (18, 70),\n",
    "    'employ': (0, 31),\n",
    "    'address': (0, 80),\n",
    "    'income': (0, 1000),\n",
    "    'debtinc': (0, 100),\n",
    "    'creddebt': (0, 30),\n",
    "    'othdebt': (0, 30)\n",
    "}\n",
    "\n",
    "# Apply the ranges to filter the dataframe using lambda\n",
    "for column, (min_val, max_val) in column_ranges.items():\n",
    "    cleaned_inputs = cleaned_inputs[cleaned_inputs[column].apply(lambda x: min_val <= x <= max_val)]\n",
    "\n",
    "# Combine the cleaned inputs with the label\n",
    "cleaned_dataset = pd.concat([cleaned_inputs, label], axis=1)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761cf3e",
   "metadata": {
    "papermill": {
     "duration": 0.005422,
     "end_time": "2024-05-23T09:10:17.812714",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.807292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Inconsistent Data\n",
    "\n",
    "In the area of data analysis, addressing inconsistent data is a fundamental task to ensure the reliability of results. Inconsistent data in categorical variables, whether due to data entry errors or discrepancies in data integration, can introduce noise and inaccuracies into the dataset, potentially leading to misleading findings.\n",
    "\n",
    "### Detecting and Correcting Inconsistent Data\n",
    "\n",
    "To detect inconsistent data, we generate frequency tables for each categorical variable, including the label. This helps us identify categories that may have been entered incorrectly or inconsistently. Once detected, we correct these inconsistencies to ensure a cohesive and accurate dataset.\n",
    "\n",
    "For example, the frequency table for the `default` column revealed inconsistencies such as `'0'` and `':0'`. We will correct these to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f135f412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T09:10:17.826232Z",
     "iopub.status.busy": "2024-05-23T09:10:17.825792Z",
     "iopub.status.idle": "2024-05-23T09:10:17.862451Z",
     "shell.execute_reply": "2024-05-23T09:10:17.861203Z"
    },
    "papermill": {
     "duration": 0.047699,
     "end_time": "2024-05-23T09:10:17.866259",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.818560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency table for ed:\n",
      "ed\n",
      "1.0    330\n",
      "2.0    182\n",
      "3.0     76\n",
      "4.0     32\n",
      "5.0      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Frequency table for default:\n",
      "default\n",
      "0      515\n",
      "1      183\n",
      "'0'      1\n",
      ":0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Corrected Frequency table for 'default':\n",
      "default\n",
      "0    517\n",
      "1    183\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age   ed  employ  address  income  debtinc   creddebt   othdebt  default\n",
       "0  41.0  3.0    17.0     12.0   176.0      9.3  11.359392  5.008608        1\n",
       "1  27.0  1.0    10.0      6.0    31.0     17.3   1.362202  4.000798        0\n",
       "3  41.0  NaN    15.0     14.0   120.0      2.9   2.658720  0.821280        0\n",
       "4  24.0  2.0     2.0      0.0    28.0     17.3   1.787436  3.056564        1\n",
       "5  41.0  2.0     5.0      5.0    25.0     10.2   0.392700  2.157300        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate frequency tables for each categorical variable\n",
    "categorical_columns = ['ed', 'default'] \n",
    "\n",
    "# Display frequency tables\n",
    "for column in categorical_columns:\n",
    "    print(f\"Frequency table for {column}:\")\n",
    "    print(cleaned_dataset[column].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Correct inconsistencies in the 'default' column\n",
    "cleaned_dataset['default'] = cleaned_dataset['default'].replace({\"'0'\": 0, ':0': 0})\n",
    "cleaned_dataset['default'] = cleaned_dataset['default'].astype(int)\n",
    "\n",
    "# Verify correction\n",
    "print(\"Corrected Frequency table for 'default':\")\n",
    "print(cleaned_dataset['default'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Separate the cleaned inputs and label\n",
    "cleaned_inputs = cleaned_dataset.drop(columns=['default'])\n",
    "label = cleaned_dataset['default']\n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_dataset = pd.concat([cleaned_inputs, label], axis=1)\n",
    "cleaned_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1763ed1",
   "metadata": {
    "papermill": {
     "duration": 0.005821,
     "end_time": "2024-05-23T09:10:17.878289",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.872468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Leakage\n",
    "\n",
    "Data leakage poses a significant challenge in the area of machine learning and data analytics, emphasizing the critical importance of a well-considered evaluation design. Data leakage occurs when information from the test set unintentionally influences the training process, leading to over-optimistic model performance. To mitigate this risk, adopting a robust evaluation design becomes imperative.\n",
    "\n",
    "### Understanding Data Leakage\n",
    "\n",
    "Data leakage can manifest in various forms, such as:\n",
    "\n",
    "1. **Train-Test Contamination**: When data from the test set influences the training set, leading to artificially high performance metrics.\n",
    "2. **Temporal Leakage**: Occurs in time-series data when future information is used to predict past events.\n",
    "3. **Feature Leakage**: When features that are highly correlated with the target variable are included in the training data, but would not be available in a real-world scenario.\n",
    "\n",
    "### Preventing Data Leakage\n",
    "\n",
    "To prevent data leakage, it is essential to:\n",
    "\n",
    "1. **Clearly Separate Training and Test Data**: Ensure that the training data does not contain any information from the test set. This separation allows for an unbiased evaluation of model performance on unseen data, mimicking real-world scenarios.\n",
    "2. **Use Temporal Split for Time-Series Data**: When working with time-series data, use a temporal split to ensure that past data is used to predict future events.\n",
    "3. **Remove Highly Correlated Features**: Identify and remove features that are highly correlated with the target variable and would not be available in a real-world scenario.\n",
    "\n",
    "By adhering to these principles, we can guard against data leakage and contribute to the development of more reliable and generalizable machine learning models.\n",
    "\n",
    "### Separating Training and Test Data\n",
    "\n",
    "In this step, we will separate our dataset into training and test sets. This separation is crucial to ensure that the model is evaluated on data it has never seen before, providing an unbiased estimate of its performance. Additionally, we will further separate the continuous and categorical variables within each set to facilitate specific preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ce386",
   "metadata": {
    "papermill": {
     "duration": 0.00607,
     "end_time": "2024-05-23T09:10:17.890535",
     "exception": false,
     "start_time": "2024-05-23T09:10:17.884465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Soon..."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4216596,
     "sourceId": 7273365,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.311061,
   "end_time": "2024-05-23T09:10:18.417757",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T09:10:13.106696",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
