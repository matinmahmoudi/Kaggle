{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc84c28",
   "metadata": {
    "papermill": {
     "duration": 0.005036,
     "end_time": "2024-11-20T12:54:46.314393",
     "exception": false,
     "start_time": "2024-11-20T12:54:46.309357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üå≥ üìù Complete Guide to Data Quality A to Z\n",
    "\n",
    "Welcome to this comprehensive guide on data quality, designed to equip you with the knowledge and skills to ensure the integrity and reliability of your datasets. Whether you're a budding data scientist or a seasoned professional looking to refine your data quality management skills, this notebook is tailored for you!\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "In this guide, we will explore various methods to assess, clean, and maintain data quality, ensuring you have the tools to confidently tackle any data-driven challenge. Here's what we'll cover:\n",
    "\n",
    "- **Feature Screening**: Learn how to identify and screen out features that do not contribute meaningful information to your analysis and modeling.\n",
    "  - **Features with a Coefficient of Variation Less than 0.1 for Continuous Variables**: Retain only those continuous features with significant variability.\n",
    "  \n",
    "  - **Features where the Mode Category Percentage is Greater than 95% for Categorical Variables**: Streamline your dataset by focusing on dominant categorical features.\n",
    "  \n",
    "  - **Features with a Percentage of Unique Categories Exceeding 90% for Categorical Variables**: Simplify your dataset by removing overly unique categorical features.\n",
    "  \n",
    "  \n",
    "\n",
    "- **Handling Out of Logical Range Data**: Address and correct values that fall outside logical ranges to maintain dataset integrity.\n",
    "\n",
    "- **Handling Inconsistent Data**: Resolve inconsistencies in categorical data to enhance the reliability of your analysis.\n",
    "\n",
    "- **Data Leakage**: Understand and prevent data leakage to ensure your machine learning models are robust and generalizable.\n",
    "\n",
    "- **Outlier Detection**: Employ one-dimensional and multidimensional methods to identify and manage outliers in your data.\n",
    "\n",
    "- **Handling Missing Data**: Learn various techniques for dealing with missing data, from simple imputation to advanced methods.\n",
    "\n",
    "## Why This Guide?\n",
    "\n",
    "- **Step-by-Step Tutorials**: Each section includes clear explanations followed by practical examples, ensuring you not only learn but also apply your knowledge.\n",
    "- **Interactive Learning**: Engage with interactive code cells that allow you to see the effects of data quality methods in real-time.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- **Run the Cells**: Follow along with the code examples by running the cells yourself. Modify the parameters to see how the results change.\n",
    "- **Explore Further**: After completing the guided sections, try applying the methods to your own datasets to reinforce your learning.\n",
    "\n",
    "Let's dive in and transform data into reliable insights!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83034cea",
   "metadata": {
    "papermill": {
     "duration": 0.004382,
     "end_time": "2024-11-20T12:54:46.323789",
     "exception": false,
     "start_time": "2024-11-20T12:54:46.319407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "To begin our analysis, we'll start by loading the dataset. This dataset contains information about bank loans, including various features such as age, education level, employment duration, address duration, income, debt-to-income ratio, credit card debt, other debt, and loan default status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6300a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:46.334453Z",
     "iopub.status.busy": "2024-11-20T12:54:46.334075Z",
     "iopub.status.idle": "2024-11-20T12:54:47.201545Z",
     "shell.execute_reply": "2024-11-20T12:54:47.200510Z"
    },
    "papermill": {
     "duration": 0.875326,
     "end_time": "2024-11-20T12:54:47.203586",
     "exception": false,
     "start_time": "2024-11-20T12:54:46.328260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.833874</td>\n",
       "      <td>16.668126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>1.239408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1.358348</td>\n",
       "      <td>3.277652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>2.777700</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.252356</td>\n",
       "      <td>0.943644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.929600</td>\n",
       "      <td>2.470400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.715901</td>\n",
       "      <td>3.011099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.703700</td>\n",
       "      <td>5.396300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.817516</td>\n",
       "      <td>3.396484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.918216</td>\n",
       "      <td>3.805784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>72.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.181952</td>\n",
       "      <td>4.290048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.563274</td>\n",
       "      <td>2.913726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.342992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   ed  employ  address  income  debtinc   creddebt    othdebt default\n",
       "0   41.0  3.0      17       12   176.0      9.3  11.359392   5.008608       1\n",
       "1   27.0  1.0      10        6    31.0     17.3   1.362202   4.000798       0\n",
       "2   40.0  1.0      15        7     NaN      5.5   0.856075   2.168925       0\n",
       "3   41.0  NaN      15       14   120.0      2.9   2.658720   0.821280       0\n",
       "4   24.0  2.0       2        0    28.0     17.3   1.787436   3.056564       1\n",
       "5   41.0  2.0       5        5    25.0     10.2   0.392700   2.157300       0\n",
       "6   39.0  1.0      20        9     NaN     30.6   3.833874  16.668126       0\n",
       "7    NaN  1.0      12       11    38.0      3.6   0.128592   1.239408       0\n",
       "8   24.0  1.0       3        4    19.0     24.4   1.358348   3.277652       1\n",
       "9   36.0  1.0       0       13    25.0     19.7   2.777700   2.147300       0\n",
       "10  27.0  1.0       0        1    16.0      1.7   0.182512   0.089488       0\n",
       "11  25.0  1.0       4        0    23.0      5.2   0.252356   0.943644       0\n",
       "12  52.0  1.0      24       14    64.0     10.0   3.929600   2.470400       0\n",
       "13  37.0  1.0       6        9    29.0     16.3   1.715901   3.011099       0\n",
       "14  48.0  1.0      22       15   100.0      9.1   3.703700   5.396300       0\n",
       "15  36.0  2.0       9        6    49.0      8.6   0.817516   3.396484       1\n",
       "16  36.0  2.0      13        6    41.0     16.4   2.918216   3.805784       1\n",
       "17  43.0  1.0      23       19    72.0      7.6   1.181952   4.290048       0\n",
       "18  39.0  1.0       6        9    61.0      5.7   0.563274   2.913726       0\n",
       "19   NaN  NaN       0       21    26.0      1.7   0.099008   0.342992       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "file_path = '/kaggle/input/loans-and-liability/LoanData_Raw_v1.0.csv' \n",
    "dataset = pd.read_csv(file_path, delimiter=\",\")\n",
    "\n",
    "dataset.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14352efb",
   "metadata": {
    "papermill": {
     "duration": 0.004522,
     "end_time": "2024-11-20T12:54:47.213173",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.208651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Explanation\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- **age**: The age of the applicant.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 20\n",
    "  - **Max**: 67\n",
    "  - **Mean**: 35.95\n",
    "  - **Median**: 34\n",
    "  - **Standard Deviation**: 11.36\n",
    "  - **Skewness**: 0.45 (slightly right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the age in years. This variable is important for understanding the demographic distribution of the applicants.\n",
    "\n",
    "\n",
    "- **ed**: The education level of the applicant, represented as an integer.\n",
    "  - **Type**: Categorical (Ordinal)\n",
    "  - **Min**: 1\n",
    "  - **Max**: 5\n",
    "  - **Mode**: 1\n",
    "  - **Missing Values**: 24 (4% of the dataset)\n",
    "  - **Details**: Represents the education level, where higher values indicate higher levels of education. This variable helps in assessing the education background of applicants.\n",
    "\n",
    "\n",
    "- **employ**: The number of years the applicant has been employed.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0\n",
    "  - **Max**: 35\n",
    "  - **Mean**: 9.8\n",
    "  - **Median**: 8\n",
    "  - **Standard Deviation**: 8.2\n",
    "  - **Skewness**: 0.65 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the number of years in employment. This variable is crucial for understanding the work experience of the applicants.\n",
    "\n",
    "\n",
    "- **address**: The number of years the applicant has lived at their current address.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0\n",
    "  - **Max**: 25\n",
    "  - **Mean**: 6.9\n",
    "  - **Median**: 4\n",
    "  - **Standard Deviation**: 7.2\n",
    "  - **Skewness**: 0.95 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the number of years at the current address. This variable helps in understanding the stability of the applicants' living situation.\n",
    "\n",
    "\n",
    "- **income**: The annual income of the applicant in thousands of dollars.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 8.0\n",
    "  - **Max**: 636.0\n",
    "  - **Mean**: 70.55\n",
    "  - **Median**: 40.0\n",
    "  - **Standard Deviation**: 66.4\n",
    "  - **Skewness**: 2.12 (highly right-skewed)\n",
    "  - **Missing Values**: 38 (6.3% of the dataset)\n",
    "  - **Details**: Represents the annual income in thousands. This variable is essential for assessing the financial status of the applicants.\n",
    "\n",
    "\n",
    "- **debtinc**: The debt-to-income ratio of the applicant, expressed as a percentage.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0.0\n",
    "  - **Max**: 69.9\n",
    "  - **Mean**: 10.1\n",
    "  - **Median**: 8.9\n",
    "  - **Standard Deviation**: 8.7\n",
    "  - **Skewness**: 1.4 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the debt-to-income ratio. This variable helps in understanding the financial burden on the applicants.\n",
    "\n",
    "\n",
    "- **creddebt**: The amount of credit card debt the applicant has, in thousands of dollars.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0.0\n",
    "  - **Max**: 11.36\n",
    "  - **Mean**: 3.55\n",
    "  - **Median**: 2.30\n",
    "  - **Standard Deviation**: 3.41\n",
    "  - **Skewness**: 0.75 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents the credit card debt in thousands. This variable indicates the credit card liabilities of the applicants.\n",
    "\n",
    "\n",
    "- **othdebt**: The amount of other debt the applicant has, in thousands of dollars.\n",
    "  - **Type**: Numerical\n",
    "  - **Min**: 0.0\n",
    "  - **Max**: 11.0\n",
    "  - **Mean**: 3.02\n",
    "  - **Median**: 2.20\n",
    "  - **Standard Deviation**: 2.24\n",
    "  - **Skewness**: 0.95 (moderately right-skewed)\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Represents other debts in thousands. This variable shows additional financial liabilities apart from credit card debt.\n",
    "  \n",
    "\n",
    "- **default**: The default status of the loan, where 1 indicates default and 0 indicates no default.\n",
    "  - **Type**: Categorical (Binary)\n",
    "  - **Unique Values**: [0, 1]\n",
    "  - **Mode**: 0\n",
    "  - **Missing Values**: 0\n",
    "  - **Details**: Binary indicator of loan default status. This is the target variable for modeling and analysis.\n",
    "\n",
    "If you want to learn how to perform detailed data profiling and obtain these insights, visit the [Complete Guide to Data Profiling A to Z](https://www.kaggle.com/code/matinmahmoudi/complete-guide-to-data-profiling-a-to-z).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000596a0",
   "metadata": {
    "papermill": {
     "duration": 0.004392,
     "end_time": "2024-11-20T12:54:47.222156",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.217764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Screening\n",
    "\n",
    "Feature screening is a crucial step in the data quality process that involves identifying and removing features (variables) that do not contribute meaningful information to the analysis or modeling. By screening out such features, we can streamline the dataset, improve model performance, and enhance interpretability. In this section, we will cover three specific criteria for feature screening:\n",
    "\n",
    "### Features with a Coefficient of Variation Less than 0.1 for Continuous Variables\n",
    "\n",
    "The coefficient of variation (CV) is a measure of relative variability. It is calculated as the ratio of the standard deviation to the mean. Features with a CV less than 0.1 are considered to have low variability and may not provide significant information for analysis. We will identify and remove such features.\n",
    "\n",
    "### Features where the Mode Category Percentage is Greater than 95% for Categorical Variables\n",
    "\n",
    "Categorical variables where a single category overwhelmingly dominates (mode category percentage > 95%) may not be useful for analysis as they do not provide much variation. We will identify and remove these categorical features to streamline the dataset.\n",
    "\n",
    "### Features with a Percentage of Unique Categories Exceeding 90% for Categorical Variables\n",
    "\n",
    "Categorical variables with a high percentage of unique categories ( > 90%) can complicate the analysis and lead to overfitting in models. We will identify and remove these features to ensure a more robust and generalizable model.\n",
    "\n",
    "By applying these screening criteria, we can ensure that the remaining features in the dataset provide meaningful and relevant information for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472acd30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:47.233292Z",
     "iopub.status.busy": "2024-11-20T12:54:47.232639Z",
     "iopub.status.idle": "2024-11-20T12:54:47.277940Z",
     "shell.execute_reply": "2024-11-20T12:54:47.276874Z"
    },
    "papermill": {
     "duration": 0.053108,
     "end_time": "2024-11-20T12:54:47.279874",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.226766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with Coefficient of Variation less than 0.1: []\n",
      "Categorical features where mode category percentage is greater than 95%: []\n",
      "Categorical features with percentage of unique categories exceeding 90%: []\n",
      "Features to be removed: set()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.833874</td>\n",
       "      <td>16.668126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>1.239408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1.358348</td>\n",
       "      <td>3.277652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>2.777700</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.252356</td>\n",
       "      <td>0.943644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.929600</td>\n",
       "      <td>2.470400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.715901</td>\n",
       "      <td>3.011099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.703700</td>\n",
       "      <td>5.396300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.817516</td>\n",
       "      <td>3.396484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.918216</td>\n",
       "      <td>3.805784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>72.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.181952</td>\n",
       "      <td>4.290048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.563274</td>\n",
       "      <td>2.913726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.342992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   ed  employ  address  income  debtinc   creddebt    othdebt default\n",
       "0   41.0  3.0      17       12   176.0      9.3  11.359392   5.008608       1\n",
       "1   27.0  1.0      10        6    31.0     17.3   1.362202   4.000798       0\n",
       "2   40.0  1.0      15        7     NaN      5.5   0.856075   2.168925       0\n",
       "3   41.0  NaN      15       14   120.0      2.9   2.658720   0.821280       0\n",
       "4   24.0  2.0       2        0    28.0     17.3   1.787436   3.056564       1\n",
       "5   41.0  2.0       5        5    25.0     10.2   0.392700   2.157300       0\n",
       "6   39.0  1.0      20        9     NaN     30.6   3.833874  16.668126       0\n",
       "7    NaN  1.0      12       11    38.0      3.6   0.128592   1.239408       0\n",
       "8   24.0  1.0       3        4    19.0     24.4   1.358348   3.277652       1\n",
       "9   36.0  1.0       0       13    25.0     19.7   2.777700   2.147300       0\n",
       "10  27.0  1.0       0        1    16.0      1.7   0.182512   0.089488       0\n",
       "11  25.0  1.0       4        0    23.0      5.2   0.252356   0.943644       0\n",
       "12  52.0  1.0      24       14    64.0     10.0   3.929600   2.470400       0\n",
       "13  37.0  1.0       6        9    29.0     16.3   1.715901   3.011099       0\n",
       "14  48.0  1.0      22       15   100.0      9.1   3.703700   5.396300       0\n",
       "15  36.0  2.0       9        6    49.0      8.6   0.817516   3.396484       1\n",
       "16  36.0  2.0      13        6    41.0     16.4   2.918216   3.805784       1\n",
       "17  43.0  1.0      23       19    72.0      7.6   1.181952   4.290048       0\n",
       "18  39.0  1.0       6        9    61.0      5.7   0.563274   2.913726       0\n",
       "19   NaN  NaN       0       21    26.0      1.7   0.099008   0.342992       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the dataset into input variables (predictors) and target variable (response)\n",
    "label = dataset['default']\n",
    "inputs = dataset.drop(columns=['default'])\n",
    "\n",
    "categorical_columns = ['ed']  \n",
    "numerical_columns = ['age', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']\n",
    "\n",
    "# Calculate Coefficient of Variation for continuous variables\n",
    "cv = inputs[numerical_columns].std() / inputs[numerical_columns].mean()\n",
    "\n",
    "# Identify features with CV less than 0.1\n",
    "low_cv_features = cv[cv < 0.1].index.tolist()\n",
    "print(\"Features with Coefficient of Variation less than 0.1:\", low_cv_features)\n",
    "\n",
    "# Calculate Mode Category Percentage for categorical variables\n",
    "mode_percentage = inputs[categorical_columns].apply(lambda x: x.value_counts(normalize=True).max() * 100)\n",
    "\n",
    "# Identify features where the mode category percentage is greater than 95%\n",
    "high_mode_features = mode_percentage[mode_percentage > 95].index.tolist()\n",
    "print(\"Categorical features where mode category percentage is greater than 95%:\", high_mode_features)\n",
    "\n",
    "# Calculate Percentage of Unique Categories for categorical variables\n",
    "unique_category_percentage = inputs[categorical_columns].nunique() / len(inputs) * 100\n",
    "\n",
    "# Identify features with a percentage of unique categories exceeding 90%\n",
    "high_unique_features = unique_category_percentage[unique_category_percentage > 90].index.tolist()\n",
    "print(\"Categorical features with percentage of unique categories exceeding 90%:\", high_unique_features)\n",
    "\n",
    "# Combine all features to be removed\n",
    "features_to_remove = set(low_cv_features + high_mode_features + high_unique_features)\n",
    "print(\"Features to be removed:\", features_to_remove)\n",
    "\n",
    "# Remove the identified features from the inputs dataframe\n",
    "cleaned_inputs = inputs.drop(columns=features_to_remove)\n",
    "\n",
    "# Combine the cleaned inputs with the label\n",
    "cleaned_dataset = pd.concat([cleaned_inputs, label], axis=1)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_dataset.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500ac63",
   "metadata": {
    "papermill": {
     "duration": 0.005066,
     "end_time": "2024-11-20T12:54:47.290270",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.285204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Out of Logical Range Data\n",
    "\n",
    "In data analysis, handling values that fall outside the logical range of respective fields is a critical step to maintain the integrity and reliability of the dataset. Values significantly deviating from the expected range can distort analytical results and impact the overall quality of findings. It is essential to define these ranges based on domain knowledge, business rules, and the specific context of the data.\n",
    "\n",
    "### Defining Logical Ranges\n",
    "\n",
    "To ensure the data is within logical boundaries, we define acceptable ranges for each column based on reasonable assumptions and domain knowledge. Here are the defined ranges for each column in our dataset:\n",
    "\n",
    "- **age**: 18 to 70 years - This range assumes the typical age range of bank loan applicants.\n",
    "- **employ**: 0 to 31 years - This range covers the typical employment duration for most individuals.\n",
    "- **address**: 0 to 80 years - This range represents the duration someone might live at a given address.\n",
    "- **income**: 0 to 1000 thousand dollars - This upper limit is set to include high-income individuals while excluding outliers.\n",
    "- **debtinc**: 0 to 100 percent - This range covers the debt-to-income ratio, with 100% being the upper logical limit.\n",
    "- **creddebt**: 0 to 30 thousand dollars - This range is set to include typical credit card debt amounts.\n",
    "- **othdebt**: 0 to 30 thousand dollars - This range includes other types of debt and is set to exclude extreme outliers.\n",
    "\n",
    "By adhering to these logical ranges, we can filter out anomalous data points that may otherwise skew our analysis and ensure a more accurate and reliable dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f9146e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:47.302502Z",
     "iopub.status.busy": "2024-11-20T12:54:47.301830Z",
     "iopub.status.idle": "2024-11-20T12:54:47.327089Z",
     "shell.execute_reply": "2024-11-20T12:54:47.326098Z"
    },
    "papermill": {
     "duration": 0.0336,
     "end_time": "2024-11-20T12:54:47.329004",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.295404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.833874</td>\n",
       "      <td>16.668126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>1.239408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1.358348</td>\n",
       "      <td>3.277652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>2.777700</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.252356</td>\n",
       "      <td>0.943644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.929600</td>\n",
       "      <td>2.470400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.715901</td>\n",
       "      <td>3.011099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.703700</td>\n",
       "      <td>5.396300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.817516</td>\n",
       "      <td>3.396484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.918216</td>\n",
       "      <td>3.805784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>72.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.181952</td>\n",
       "      <td>4.290048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.563274</td>\n",
       "      <td>2.913726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.342992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   ed  employ  address  income  debtinc   creddebt    othdebt default\n",
       "0   41.0  3.0      17       12   176.0      9.3  11.359392   5.008608       1\n",
       "1   27.0  1.0      10        6    31.0     17.3   1.362202   4.000798       0\n",
       "2   40.0  1.0      15        7     NaN      5.5   0.856075   2.168925       0\n",
       "3   41.0  NaN      15       14   120.0      2.9   2.658720   0.821280       0\n",
       "4   24.0  2.0       2        0    28.0     17.3   1.787436   3.056564       1\n",
       "5   41.0  2.0       5        5    25.0     10.2   0.392700   2.157300       0\n",
       "6   39.0  1.0      20        9     NaN     30.6   3.833874  16.668126       0\n",
       "7    NaN  1.0      12       11    38.0      3.6   0.128592   1.239408       0\n",
       "8   24.0  1.0       3        4    19.0     24.4   1.358348   3.277652       1\n",
       "9   36.0  1.0       0       13    25.0     19.7   2.777700   2.147300       0\n",
       "10  27.0  1.0       0        1    16.0      1.7   0.182512   0.089488       0\n",
       "11  25.0  1.0       4        0    23.0      5.2   0.252356   0.943644       0\n",
       "12  52.0  1.0      24       14    64.0     10.0   3.929600   2.470400       0\n",
       "13  37.0  1.0       6        9    29.0     16.3   1.715901   3.011099       0\n",
       "14  48.0  1.0      22       15   100.0      9.1   3.703700   5.396300       0\n",
       "15  36.0  2.0       9        6    49.0      8.6   0.817516   3.396484       1\n",
       "16  36.0  2.0      13        6    41.0     16.4   2.918216   3.805784       1\n",
       "17  43.0  1.0      23       19    72.0      7.6   1.181952   4.290048       0\n",
       "18  39.0  1.0       6        9    61.0      5.7   0.563274   2.913726       0\n",
       "19   NaN  NaN       0       21    26.0      1.7   0.099008   0.342992       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define ranges for each column\n",
    "column_ranges = {\n",
    "    'age': (18, 70),\n",
    "    'employ': (0, 31),\n",
    "    'address': (0, 80),\n",
    "    'income': (0, 1000),\n",
    "    'debtinc': (0, 100),\n",
    "    'creddebt': (0, 30),\n",
    "    'othdebt': (0, 30)\n",
    "}\n",
    "\n",
    "# Apply the ranges to filter the dataframe using lambda\n",
    "for column, (min_val, max_val) in column_ranges.items():\n",
    "    cleaned_inputs = cleaned_inputs[cleaned_inputs[column].apply(lambda x: min_val <= x <= max_val)]\n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_dataset.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87190b73",
   "metadata": {
    "papermill": {
     "duration": 0.005326,
     "end_time": "2024-11-20T12:54:47.339979",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.334653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Inconsistent Data\n",
    "\n",
    "In the area of data analysis, addressing inconsistent data is a fundamental task to ensure the reliability of results. Inconsistent data in categorical variables, whether due to data entry errors or discrepancies in data integration, can introduce noise and inaccuracies into the dataset, potentially leading to misleading findings.\n",
    "\n",
    "### Detecting and Correcting Inconsistent Data\n",
    "\n",
    "To detect inconsistent data, we generate frequency tables for each categorical variable, including the label. This helps us identify categories that may have been entered incorrectly or inconsistently. Once detected, we correct these inconsistencies to ensure a cohesive and accurate dataset.\n",
    "\n",
    "For example, the frequency table for the `default` column revealed inconsistencies such as `'0'` and `':0'`. We will correct these to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea67c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:47.352862Z",
     "iopub.status.busy": "2024-11-20T12:54:47.352016Z",
     "iopub.status.idle": "2024-11-20T12:54:47.377998Z",
     "shell.execute_reply": "2024-11-20T12:54:47.377015Z"
    },
    "papermill": {
     "duration": 0.034609,
     "end_time": "2024-11-20T12:54:47.380099",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.345490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency table for ed:\n",
      "ed\n",
      "1.0    363\n",
      "2.0    192\n",
      "3.0     84\n",
      "4.0     36\n",
      "5.0      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Frequency table for default:\n",
      "default\n",
      "0      515\n",
      "1      183\n",
      "'0'      1\n",
      ":0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Corrected Frequency table for 'default':\n",
      "default\n",
      "0    517\n",
      "1    183\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.833874</td>\n",
       "      <td>16.668126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>1.239408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1.358348</td>\n",
       "      <td>3.277652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>2.777700</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.252356</td>\n",
       "      <td>0.943644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.929600</td>\n",
       "      <td>2.470400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.715901</td>\n",
       "      <td>3.011099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.703700</td>\n",
       "      <td>5.396300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.817516</td>\n",
       "      <td>3.396484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.918216</td>\n",
       "      <td>3.805784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>72.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.181952</td>\n",
       "      <td>4.290048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.563274</td>\n",
       "      <td>2.913726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.342992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   ed  employ  address  income  debtinc   creddebt    othdebt  default\n",
       "0   41.0  3.0      17       12   176.0      9.3  11.359392   5.008608        1\n",
       "1   27.0  1.0      10        6    31.0     17.3   1.362202   4.000798        0\n",
       "2   40.0  1.0      15        7     NaN      5.5   0.856075   2.168925        0\n",
       "3   41.0  NaN      15       14   120.0      2.9   2.658720   0.821280        0\n",
       "4   24.0  2.0       2        0    28.0     17.3   1.787436   3.056564        1\n",
       "5   41.0  2.0       5        5    25.0     10.2   0.392700   2.157300        0\n",
       "6   39.0  1.0      20        9     NaN     30.6   3.833874  16.668126        0\n",
       "7    NaN  1.0      12       11    38.0      3.6   0.128592   1.239408        0\n",
       "8   24.0  1.0       3        4    19.0     24.4   1.358348   3.277652        1\n",
       "9   36.0  1.0       0       13    25.0     19.7   2.777700   2.147300        0\n",
       "10  27.0  1.0       0        1    16.0      1.7   0.182512   0.089488        0\n",
       "11  25.0  1.0       4        0    23.0      5.2   0.252356   0.943644        0\n",
       "12  52.0  1.0      24       14    64.0     10.0   3.929600   2.470400        0\n",
       "13  37.0  1.0       6        9    29.0     16.3   1.715901   3.011099        0\n",
       "14  48.0  1.0      22       15   100.0      9.1   3.703700   5.396300        0\n",
       "15  36.0  2.0       9        6    49.0      8.6   0.817516   3.396484        1\n",
       "16  36.0  2.0      13        6    41.0     16.4   2.918216   3.805784        1\n",
       "17  43.0  1.0      23       19    72.0      7.6   1.181952   4.290048        0\n",
       "18  39.0  1.0       6        9    61.0      5.7   0.563274   2.913726        0\n",
       "19   NaN  NaN       0       21    26.0      1.7   0.099008   0.342992        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate frequency tables for each categorical variable\n",
    "categorical_columns = ['ed', 'default']  # Include the label as well for this step\n",
    "\n",
    "# Display frequency tables\n",
    "for column in categorical_columns:\n",
    "    print(f\"Frequency table for {column}:\")\n",
    "    print(cleaned_dataset[column].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Correct inconsistencies in the 'default' column\n",
    "cleaned_dataset['default'] = cleaned_dataset['default'].replace({\"'0'\": 0, ':0': 0})\n",
    "cleaned_dataset['default'] = cleaned_dataset['default'].astype(int)\n",
    "\n",
    "# Verify correction\n",
    "print(\"Corrected Frequency table for 'default':\")\n",
    "print(cleaned_dataset['default'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Display the cleaned dataset\n",
    "cleaned_dataset.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d42b7",
   "metadata": {
    "papermill": {
     "duration": 0.005799,
     "end_time": "2024-11-20T12:54:47.392017",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.386218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Leakage\n",
    "\n",
    "Data leakage poses a significant challenge in the area of machine learning and data analytics, emphasizing the critical importance of a well-considered evaluation design. Data leakage occurs when information from the test set unintentionally influences the training process, leading to over-optimistic model performance. To mitigate this risk, adopting a robust evaluation design becomes imperative.\n",
    "\n",
    "### Understanding Data Leakage\n",
    "\n",
    "Data leakage can manifest in various forms, such as:\n",
    "\n",
    "1. **Train-Test Contamination**: When data from the test set influences the training set, leading to artificially high performance metrics.\n",
    "2. **Temporal Leakage**: Occurs in time-series data when future information is used to predict past events.\n",
    "3. **Feature Leakage**: When features that are highly correlated with the target variable are included in the training data, but would not be available in a real-world scenario.\n",
    "\n",
    "### Preventing Data Leakage\n",
    "\n",
    "To prevent data leakage, it is essential to:\n",
    "\n",
    "1. **Clearly Separate Training and Test Data**: Ensure that the training data does not contain any information from the test set. This separation allows for an unbiased evaluation of model performance on unseen data, mimicking real-world scenarios.\n",
    "2. **Use Temporal Split for Time-Series Data**: When working with time-series data, use a temporal split to ensure that past data is used to predict future events.\n",
    "3. **Remove Highly Correlated Features**: Identify and remove features that are highly correlated with the target variable and would not be available in a real-world scenario.\n",
    "\n",
    "By adhering to these principles, we can guard against data leakage and contribute to the development of more reliable and generalizable machine learning models.\n",
    "\n",
    "### Separating Training and Test Data\n",
    "\n",
    "In this step, we will separate our dataset into training and test sets. This separation is crucial to ensure that the model is evaluated on data it has never seen before, providing an unbiased estimate of its performance. Additionally, we will further separate the continuous and categorical variables within each set to facilitate specific preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4c3295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:47.405768Z",
     "iopub.status.busy": "2024-11-20T12:54:47.405082Z",
     "iopub.status.idle": "2024-11-20T12:54:48.596525Z",
     "shell.execute_reply": "2024-11-20T12:54:48.595426Z"
    },
    "papermill": {
     "duration": 1.200815,
     "end_time": "2024-11-20T12:54:48.598800",
     "exception": false,
     "start_time": "2024-11-20T12:54:47.397985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (490, 8)\n",
      "Test data shape: (210, 8)\n",
      "Training labels shape: (490,)\n",
      "Test labels shape: (210,)\n",
      "Training Continuous Data:\n",
      "      age  employ  address  income  debtinc  creddebt   othdebt\n",
      "357  24.0       0        2    15.0     16.1  0.321195  2.093805\n",
      "649  29.0       5        7    28.0     18.7  2.125816  3.110184\n",
      "291  33.0       9        8    32.0      5.5  0.496320  1.263680\n",
      "420  46.0       1       15    21.0     14.8  1.376844  1.731156\n",
      "177  28.0       7        2    34.0      7.0  0.359380  2.020620\n",
      "\n",
      "Training Categorical Data:\n",
      "      ed\n",
      "357  2.0\n",
      "649  2.0\n",
      "291  NaN\n",
      "420  1.0\n",
      "177  2.0\n",
      "Test Continuous Data:\n",
      "      age  employ  address  income  debtinc  creddebt   othdebt\n",
      "158  33.0       9        4     NaN     13.8  1.348674  2.653326\n",
      "500  22.0       0        1    18.0      7.7  0.478170  0.907830\n",
      "396  34.0      14        8    28.0     17.0  1.137640  3.622360\n",
      "155  36.0      13        3    39.0     19.2  2.800512  4.687488\n",
      "321  27.0       3        4    45.0     11.0  0.292050  4.657950\n",
      "\n",
      "Test Categorical Data:\n",
      "      ed\n",
      "158  1.0\n",
      "500  3.0\n",
      "396  1.0\n",
      "155  3.0\n",
      "321  3.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "categorical_columns = ['ed']  \n",
    "numerical_columns = ['age', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']\n",
    "\n",
    "# Separate the cleaned inputs and label\n",
    "label = cleaned_dataset['default']\n",
    "cleaned_inputs = cleaned_dataset.drop(columns=['default'])\n",
    "\n",
    "# Separate the cleaned inputs and label into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_inputs, label, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and test datasets to ensure correctness\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Separate continuous and categorical data in the training set\n",
    "X_train_continuous = X_train[numerical_columns]\n",
    "X_train_categorical = X_train[categorical_columns]\n",
    "\n",
    "# Separate continuous and categorical data in the test set\n",
    "X_test_continuous = X_test[numerical_columns]\n",
    "X_test_categorical = X_test[categorical_columns]\n",
    "\n",
    "# Display the separated dataframes to ensure correctness\n",
    "print(\"Training Continuous Data:\")\n",
    "print(X_train_continuous.head())\n",
    "print(\"\\nTraining Categorical Data:\")\n",
    "print(X_train_categorical.head())\n",
    "\n",
    "print(\"Test Continuous Data:\")\n",
    "print(X_test_continuous.head())\n",
    "print(\"\\nTest Categorical Data:\")\n",
    "print(X_test_categorical.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fc2ab",
   "metadata": {
    "papermill": {
     "duration": 0.006082,
     "end_time": "2024-11-20T12:54:48.611285",
     "exception": false,
     "start_time": "2024-11-20T12:54:48.605203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Outlier Detection and Handling\n",
    "\n",
    "### What are Outliers?\n",
    "\n",
    "An outlier is an observation that is unlike the other observations. They are rare, distinct, or do not fit in some way. We generally define outliers as samples that are exceptionally far from the mainstream of the data.\n",
    "\n",
    "**Caution**: In data mining issues involving deviation detection tasks, the identification and management of outliers should be disregarded as part of the data cleaning process.\n",
    "\n",
    "### Outlier Detection\n",
    "\n",
    "Outlier detection is a crucial step in data analysis, employing various methods to identify and manage data points that significantly deviate from expected patterns. There are two main approaches to outlier detection:\n",
    "\n",
    "## One-Dimensional Methods\n",
    "\n",
    "One-dimensional methods can be applied only to continuous variables. We will explore two common methods:\n",
    "\n",
    "1. **Standard Deviation Method**: This method uses the standard deviation from the mean to identify outliers. Values that fall outside of 3 standard deviations are typically considered outliers.\n",
    "2. **Interquartile Range (IQR) Method**: This method uses the IQR to define limits on the sample values. Values below the 25th percentile minus 1.5 times the IQR or above the 75th percentile plus 1.5 times the IQR are considered outliers.\n",
    "\n",
    "#### Why IQR Method is Better\n",
    "\n",
    "The IQR method is preferred in many cases because it does not assume normality of the data distribution. The standard deviation method assumes the data follows a normal distribution, which may not always be the case. The IQR method is more robust as it uses percentiles and can be applied to non-Gaussian distributions.\n",
    "\n",
    "### Choosing Thresholds and k Values\n",
    "\n",
    "When detecting outliers, the choice of threshold for the standard deviation method and the value of k for the IQR method can vary based on the dataset and business requirements. For instance:\n",
    "\n",
    "- **Standard Deviation Method**: While 3 standard deviations is a common threshold, smaller datasets might use 2 standard deviations to identify outliers, and larger datasets might use 4 standard deviations to account for greater variability.\n",
    "- **IQR Method**: The common value for k is 1.5, but this can be adjusted to 2 for a stricter outlier detection or 3 to identify extreme outliers.\n",
    "\n",
    "These adjustments depend on the specific context and goals of the analysis. It's essential to consider the characteristics of the data and the impact of outliers on the analysis and business outcomes.\n",
    "\n",
    "### Outlier Handling\n",
    "\n",
    "When dealing with detected outliers, two common approaches are employed:\n",
    "\n",
    "1. **Remove Rows**: Exclude entire rows containing outliers from the dataset.\n",
    "2. **Coerce to Bounds**: Modify outlier values to fall within an acceptable range, either by setting them to the lower or upper bound.\n",
    "\n",
    "These methods offer flexibility in managing the impact of outliers on data analysis, allowing analysts to choose the most suitable strategy based on the specific requirements of their analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89c20a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:48.626658Z",
     "iopub.status.busy": "2024-11-20T12:54:48.625927Z",
     "iopub.status.idle": "2024-11-20T12:54:48.668006Z",
     "shell.execute_reply": "2024-11-20T12:54:48.667132Z"
    },
    "papermill": {
     "duration": 0.05171,
     "end_time": "2024-11-20T12:54:48.670210",
     "exception": false,
     "start_time": "2024-11-20T12:54:48.618500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation Method Outliers in Training (age): 1 outliers\n",
      "IQR Method Outliers in Training (age): 1 outliers\n",
      "\n",
      "\n",
      "Standard Deviation Method Outliers in Training (employ): 4 outliers\n",
      "IQR Method Outliers in Training (employ): 2 outliers\n",
      "\n",
      "\n",
      "Standard Deviation Method Outliers in Training (address): 2 outliers\n",
      "IQR Method Outliers in Training (address): 2 outliers\n",
      "\n",
      "\n",
      "Standard Deviation Method Outliers in Training (income): 9 outliers\n",
      "IQR Method Outliers in Training (income): 18 outliers\n",
      "\n",
      "\n",
      "Standard Deviation Method Outliers in Training (debtinc): 6 outliers\n",
      "IQR Method Outliers in Training (debtinc): 6 outliers\n",
      "\n",
      "\n",
      "Standard Deviation Method Outliers in Training (creddebt): 8 outliers\n",
      "IQR Method Outliers in Training (creddebt): 22 outliers\n",
      "\n",
      "\n",
      "Standard Deviation Method Outliers in Training (othdebt): 10 outliers\n",
      "IQR Method Outliers in Training (othdebt): 19 outliers\n",
      "\n",
      "\n",
      "Training data shape after removing outliers using IQR method:\n",
      "(440, 7)\n",
      "Training data shape after coercing outliers using IQR method:\n",
      "(490, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to detect outliers using the Standard Deviation Method\n",
    "def detect_outliers_std(df, columns, threshold=3):\n",
    "    outliers_dict = {}\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        std_dev = df[col].std()\n",
    "        outliers = (df[col] > mean + threshold * std_dev) | (df[col] < mean - threshold * std_dev)\n",
    "        outliers_dict[col] = df[outliers]\n",
    "    return outliers_dict\n",
    "\n",
    "# Define a function to detect outliers using the IQR Method\n",
    "def detect_outliers_iqr(df, columns, k=2):\n",
    "    outliers_dict = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = (df[col] < (Q1 - k * IQR)) | (df[col] > (Q3 + k * IQR))\n",
    "        outliers_dict[col] = df[outliers]\n",
    "    return outliers_dict\n",
    "\n",
    "# Detect outliers in the training set using the Standard Deviation Method\n",
    "std_outliers_train = detect_outliers_std(X_train_continuous, numerical_columns, threshold=3)\n",
    "\n",
    "# Detect outliers in the training set using the IQR Method\n",
    "iqr_outliers_train = detect_outliers_iqr(X_train_continuous, numerical_columns, k=2)\n",
    "\n",
    "# Display detected outliers from both methods\n",
    "for col in numerical_columns:\n",
    "    print(f\"Standard Deviation Method Outliers in Training ({col}): {std_outliers_train[col].shape[0]} outliers\")\n",
    "    print(f\"IQR Method Outliers in Training ({col}): {iqr_outliers_train[col].shape[0]} outliers\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Handling Outliers\n",
    "\n",
    "# Remove Rows Example \n",
    "X_train_continuous_removed = X_train_continuous.copy()\n",
    "for col in numerical_columns:\n",
    "    X_train_continuous_removed = X_train_continuous_removed[~X_train_continuous_removed.index.isin(iqr_outliers_train[col].index)]\n",
    "\n",
    "print(\"Training data shape after removing outliers using IQR method:\")\n",
    "print(X_train_continuous_removed.shape)\n",
    "\n",
    "# Coerce to Bounds Example \n",
    "X_train_continuous_coerced = X_train_continuous.copy()\n",
    "for col in numerical_columns:\n",
    "    Q1 = X_train_continuous_coerced[col].quantile(0.25)\n",
    "    Q3 = X_train_continuous_coerced[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 2 * IQR\n",
    "    upper_bound = Q3 + 2 * IQR\n",
    "    X_train_continuous_coerced[col] = np.where(X_train_continuous_coerced[col] < lower_bound, lower_bound, X_train_continuous_coerced[col])\n",
    "    X_train_continuous_coerced[col] = np.where(X_train_continuous_coerced[col] > upper_bound, upper_bound, X_train_continuous_coerced[col])\n",
    "\n",
    "print(\"Training data shape after coercing outliers using IQR method:\")\n",
    "print(X_train_continuous_coerced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f92b4",
   "metadata": {
    "papermill": {
     "duration": 0.006015,
     "end_time": "2024-11-20T12:54:48.682712",
     "exception": false,
     "start_time": "2024-11-20T12:54:48.676697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Multidimensional Method\n",
    "\n",
    "Outlier detection can also be extended to multidimensional datasets where interactions between multiple variables can reveal anomalies that one-dimensional methods might miss. One effective multidimensional method is the Isolation Forest algorithm.\n",
    "\n",
    "#### Isolation Forest Method\n",
    "\n",
    "The Isolation Forest method is a powerful approach to outlier detection that leverages decision trees to isolate outliers. This method isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The fewer the splits required to isolate an observation, the more likely it is an outlier.\n",
    "\n",
    "**Advantages of Isolation Forest**:\n",
    "1. It is particularly effective in handling high-dimensional data.\n",
    "2. It does not rely on assumptions about the data distribution.\n",
    "3. It is efficient and scales well to large datasets.\n",
    "\n",
    "### Applying Isolation Forest for Outlier Detection\n",
    "\n",
    "We will use the Isolation Forest algorithm to detect outliers in both the training and test sets. This method will help us identify anomalies based on the relationships between multiple variables.\n",
    "\n",
    "### Outlier Handling\n",
    "\n",
    "After detecting outliers using the Isolation Forest method, we can choose to either remove the corresponding rows from the dataset or mark them for further analysis. For this notebook, we will demonstrate how to remove the rows containing outliers from both the training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6011577c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:48.697364Z",
     "iopub.status.busy": "2024-11-20T12:54:48.696997Z",
     "iopub.status.idle": "2024-11-20T12:54:49.336075Z",
     "shell.execute_reply": "2024-11-20T12:54:49.335084Z"
    },
    "papermill": {
     "duration": 0.648891,
     "end_time": "2024-11-20T12:54:49.338285",
     "exception": false,
     "start_time": "2024-11-20T12:54:48.689394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest detected 25 outliers in the training set.\n",
      "Isolation Forest detected 8 outliers in the test set.\n",
      "Training data shape after removing outliers using Isolation Forest:\n",
      "(465, 8)\n",
      "(465,)\n",
      "Test data shape after removing outliers using Isolation Forest:\n",
      "(202, 8)\n",
      "(202,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#  Data Preprocessing\n",
    "# Before applying the Isolation Forest for outlier detection, we need to preprocess the data. This includes handling missing values, encoding categorical variables, and scaling numerical features. These steps are crucial because:\n",
    "# 1. **Imputation**: Missing values can lead to errors in model training and predictions. Imputing missing values ensures that our dataset is complete.\n",
    "# 2. **Encoding**: Machine learning algorithms require numerical inputs. Encoding categorical variables allows us to convert them into a numerical format that can be processed by the model.\n",
    "# 3. **Scaling**: Standardizing numerical features by removing the mean and scaling to unit variance is important for algorithms like Isolation Forest, which are sensitive to the scale of input data.\n",
    "\n",
    "# Define the numerical and categorical columns - to show here\n",
    "numerical_columns = ['age', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']\n",
    "categorical_columns = ['ed']\n",
    "\n",
    "# Define the preprocessing pipeline for numerical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())  # Standardize numerical features\n",
    "])\n",
    "\n",
    "# Define the preprocessing pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical features\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines into a single preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipeline, numerical_columns),\n",
    "    ('cat', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "\n",
    "# Apply the preprocessing pipeline to the combined data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Isolation Forest for Outlier Detection\n",
    "# After preprocessing, we apply the Isolation Forest algorithm to detect and remove outliers from the dataset. Isolation Forest is an unsupervised learning algorithm that identifies anomalies in the data by isolating observations. Anomalies are more susceptible to isolation, thus making them easier to identify. We set the contamination parameter to 0.05, which means we expect 5% of the data to be outliers.\n",
    "\n",
    "# Initialize the Isolation Forest for outlier detection\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the Isolation Forest model on the preprocessed training data\n",
    "iso_forest.fit(X_train_preprocessed)\n",
    "\n",
    "# Predict outliers in the training set (output is -1 for outliers and 1 for inliers)\n",
    "outliers_train = iso_forest.predict(X_train_preprocessed) == -1\n",
    "\n",
    "# Predict outliers in the test set\n",
    "outliers_test = iso_forest.predict(X_test_preprocessed) == -1\n",
    "\n",
    "# Print the number of detected outliers in both sets\n",
    "print(f\"Isolation Forest detected {np.sum(outliers_train)} outliers in the training set.\")\n",
    "print(f\"Isolation Forest detected {np.sum(outliers_test)} outliers in the test set.\")\n",
    "\n",
    "# Handling Outliers\n",
    "\n",
    "# Remove outliers from the training set\n",
    "X_train_cleaned = X_train[~outliers_train]\n",
    "y_train_cleaned = y_train[~outliers_train]\n",
    "\n",
    "# Remove outliers from the test \n",
    "X_test_cleaned = X_test[~outliers_test]\n",
    "y_test_cleaned = y_test[~outliers_test]\n",
    "\n",
    "# Print the shape of the cleaned training and test sets\n",
    "print(\"Training data shape after removing outliers using Isolation Forest:\")\n",
    "print(X_train_cleaned.shape)\n",
    "print(y_train_cleaned.shape)\n",
    "\n",
    "print(\"Test data shape after removing outliers using Isolation Forest:\")\n",
    "print(X_test_cleaned.shape)\n",
    "print(y_test_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adca259",
   "metadata": {
    "papermill": {
     "duration": 0.006075,
     "end_time": "2024-11-20T12:54:49.351023",
     "exception": false,
     "start_time": "2024-11-20T12:54:49.344948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Missing Data\n",
    "\n",
    "Handling missing data is crucial for maintaining the integrity and quality of your dataset. Various methods can be applied depending on the extent and nature of the missing data.\n",
    "\n",
    "### Missing Values: \n",
    "\n",
    "#### Initial Check\n",
    "\n",
    "1. **Assess Missing Values**:\n",
    "   - Generate a report on the percentage of records with missing data.\n",
    "   - If the percentage of missing data is low, consider discarding those records.\n",
    "\n",
    "#### Detailed Check\n",
    "\n",
    "2. **Create a Detailed Report**:\n",
    "   - For higher percentages of missing data, generate a detailed report based on the number of missing cells in each record.\n",
    "   - Sort this report to identify records with a significant number of missing cells.\n",
    "   - Discard records with a significant number of missing cells to maintain data quality and prevent misleading analyses.\n",
    "\n",
    "#### Feature-Wise Check\n",
    "\n",
    "3. **Analyze Missing Values for Each Feature**:\n",
    "   - Generate a report on missing values for each feature.\n",
    "   - Use simple imputation methods (mean, median, mode) for features with low missing data.\n",
    "   - Consider advanced imputation methods (iterative, k-NN) for features with a higher percentage of missing data.\n",
    "\n",
    "### Imputation Methods for Handling Missing Values\n",
    "\n",
    "## One-Dimensional Methods\n",
    "\n",
    "1. **Fixed Values Imputation**:\n",
    "   - **Mean Imputation**: Replace missing values with the mean of the observed values. Suitable for continuous variables without extreme outliers.\n",
    "   - **Median Imputation**: Replace missing values with the median of the observed values. Effective for skewed distributions and data with outliers.\n",
    "   - **Mode Imputation**: Replace missing values with the most frequent value. Useful for categorical variables.\n",
    "   - **Constant Value Imputation**: Replace missing values with a predefined constant value.\n",
    "\n",
    "\n",
    "2. **Random Values Following Statistical Distribution**:\n",
    "   - Replace missing values with random numbers drawn from a distribution (e.g., normal distribution based on mean and standard deviation of observed values). This preserves the variability of features.\n",
    "\n",
    "## Multi-Dimensional Methods\n",
    "\n",
    "1. **Iterative Imputation Methods**:\n",
    "   - Impute missing values by iteratively updating estimates based on observed values and relationships with other variables. Suitable for datasets with complex dependencies between variables.\n",
    "\n",
    "\n",
    "2. **k-NN (k-Nearest Neighbors) Model**:\n",
    "   - Impute missing values based on the values of their k-nearest neighbors in the feature space. Useful for datasets where local patterns or clusters exist.\n",
    "\n",
    "### Applying Imputation Methods\n",
    "\n",
    "We will apply these methods to both the training and test sets to ensure consistency and avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef51140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T12:54:49.365730Z",
     "iopub.status.busy": "2024-11-20T12:54:49.365375Z",
     "iopub.status.idle": "2024-11-20T12:54:49.423400Z",
     "shell.execute_reply": "2024-11-20T12:54:49.422112Z"
    },
    "papermill": {
     "duration": 0.067876,
     "end_time": "2024-11-20T12:54:49.425519",
     "exception": false,
     "start_time": "2024-11-20T12:54:49.357643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Report (Training Set):\n",
      "          Missing Values  Percentage\n",
      "age                   14    3.010753\n",
      "ed                    16    3.440860\n",
      "employ                 0    0.000000\n",
      "address                0    0.000000\n",
      "income                24    5.161290\n",
      "debtinc                0    0.000000\n",
      "creddebt               0    0.000000\n",
      "othdebt                0    0.000000\n",
      "\n",
      "Missing Values Report (Test Set):\n",
      "          Missing Values  Percentage\n",
      "age                    4    1.980198\n",
      "ed                     4    1.980198\n",
      "employ                 0    0.000000\n",
      "address                0    0.000000\n",
      "income                11    5.445545\n",
      "debtinc                0    0.000000\n",
      "creddebt               0    0.000000\n",
      "othdebt                0    0.000000\n",
      "Training data shape after imputation:\n",
      "(413, 8)\n",
      "(413,)\n",
      "Test data shape after imputation:\n",
      "(183, 8)\n",
      "(183,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Initial Check: Assess missing values and create a report\n",
    "def missing_values_report(df):\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percentage = (missing_data / len(df)) * 100\n",
    "    missing_report = pd.DataFrame({'Missing Values': missing_data, 'Percentage': missing_percentage})\n",
    "    return missing_report\n",
    "\n",
    "# Detailed Check: Generate detailed report based on the number of missing cells in each record\n",
    "def detailed_missing_report(df):\n",
    "    missing_data = df.isnull().sum(axis=1)\n",
    "    missing_report = pd.DataFrame({'Missing Cells': missing_data, 'Percentage': (missing_data / df.shape[1]) * 100})\n",
    "    return missing_report.sort_values(by='Missing Cells', ascending=False)\n",
    "\n",
    "# Generate missing values report for the training and test sets\n",
    "train_missing_report = missing_values_report(X_train_cleaned)\n",
    "test_missing_report = missing_values_report(X_test_cleaned)\n",
    "\n",
    "print(\"Missing Values Report (Training Set):\")\n",
    "print(train_missing_report)\n",
    "print(\"\\nMissing Values Report (Test Set):\")\n",
    "print(test_missing_report)\n",
    "\n",
    "# Initial Check: Discard records with missing data if the percentage is low\n",
    "low_missing_threshold = 5  # 5% threshold for missing values\n",
    "X_train_initial_check = X_train_cleaned.dropna(thresh=X_train_cleaned.shape[1] * (1 - low_missing_threshold / 100), axis=0)\n",
    "X_test_initial_check = X_test_cleaned.dropna(thresh=X_test_cleaned.shape[1] * (1 - low_missing_threshold / 100), axis=0)\n",
    "\n",
    "# Detailed Check: Create a detailed report and discard low quality records (100% missing)\n",
    "train_detailed_report = detailed_missing_report(X_train_initial_check)\n",
    "test_detailed_report = detailed_missing_report(X_test_initial_check)\n",
    "\n",
    "# Remove records with 100% missing values\n",
    "X_train_detailed_check = X_train_initial_check.loc[train_detailed_report[train_detailed_report['Percentage'] < 100].index]\n",
    "X_test_detailed_check = X_test_initial_check.loc[test_detailed_report[test_detailed_report['Percentage'] < 100].index]\n",
    "\n",
    "# Update labels to match the filtered data\n",
    "y_train_filtered = y_train_cleaned.loc[X_train_detailed_check.index]\n",
    "y_test_filtered = y_test_cleaned.loc[X_test_detailed_check.index]\n",
    "\n",
    "# Feature-Wise Check: Apply simple imputation for low missing data features and advanced methods for higher missing data features\n",
    "# Define iterative imputer\n",
    "iterative_imputer = IterativeImputer(random_state=42)\n",
    "\n",
    "# Apply iterative imputation to training and test sets (main datasets)\n",
    "X_train_final = pd.DataFrame(iterative_imputer.fit_transform(X_train_detailed_check), columns=X_train.columns)\n",
    "X_test_final = pd.DataFrame(iterative_imputer.transform(X_test_detailed_check), columns=X_test.columns)\n",
    "\n",
    "# Display the shapes of the final datasets\n",
    "print(\"Training data shape after imputation:\")\n",
    "print(X_train_final.shape)\n",
    "print(y_train_filtered.shape)\n",
    "\n",
    "print(\"Test data shape after imputation:\")\n",
    "print(X_test_final.shape)\n",
    "print(y_test_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ddbe9",
   "metadata": {
    "papermill": {
     "duration": 0.006308,
     "end_time": "2024-11-20T12:54:49.438437",
     "exception": false,
     "start_time": "2024-11-20T12:54:49.432129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Thank You for Exploring This Notebook!\n",
    "\n",
    "If you have any questions, suggestions, or just want to discuss any of the topics further, please don't hesitate to reach out or leave a comment. Your feedback is not only welcome but also invaluable! If you have any additional insights or methods that were not covered in this notebook, please suggest them in the comments. This notebook will be updated regularly to include more helpful tips and techniques!\n",
    "\n",
    "Happy analyzing, and stay curious!\n",
    "\n",
    "Best regards,\n",
    "\n",
    "[Matin Mahmoudi](https://www.kaggle.com/matinmahmoudi)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5115791,
     "sourceId": 8559240,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.650887,
   "end_time": "2024-11-20T12:54:49.963682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-20T12:54:43.312795",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
