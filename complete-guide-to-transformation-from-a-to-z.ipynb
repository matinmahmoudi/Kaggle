{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ed59b8",
   "metadata": {
    "papermill": {
     "duration": 0.003894,
     "end_time": "2024-05-30T14:40:55.056564",
     "exception": false,
     "start_time": "2024-05-30T14:40:55.052670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸŒ³ ðŸ”„ Complete Guide to Transformation from A to Z\n",
    "\n",
    "Welcome to this comprehensive guide on data transformation, designed to equip you with the knowledge and skills to effectively preprocess and transform your datasets. Whether you're a budding data scientist or a seasoned professional looking to refine your data transformation techniques, this notebook is tailored for you!\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "In this guide, we will explore various methods to normalize, construct, discretize, and aggregate features, ensuring you have the tools to confidently prepare your data for any analysis or modeling task. Here's what we'll cover:\n",
    "\n",
    "### 1. Feature Normalization\n",
    "\n",
    "Learn how to scale and adjust the statistical distribution of feature values to improve the performance and accuracy of your models.\n",
    "\n",
    "- **Min-Max Scaling**: Scale features to a fixed range, typically 0 to 1.\n",
    "- **Z-Score Standardization**: Transform features to have a mean of 0 and a standard deviation of 1.\n",
    "- **Robust Scaling**: Scale features using statistics that are robust to outliers, such as the median and interquartile range.\n",
    "- **Yeo-Johnson Transformation**: Apply a transformation that can handle both positive and negative values to achieve normality.\n",
    "- **Box-Cox Transformation**: Apply a transformation that works with positive values to achieve normality and reduce skewness.\n",
    "\n",
    "### 2. Feature Construction\n",
    "\n",
    "Learn techniques to create new features from existing ones, enhancing the predictive power of your models.\n",
    "\n",
    "- **Use of Domain Knowledge**: Incorporate insights from the specific field or industry to construct meaningful features.\n",
    "- **Using Statistical Relationships Between Features**: Identify and utilize correlations and interactions between features.\n",
    "- **Numerical Coding of Nominal Values**:\n",
    "  - **One-Hot Encoding**: Convert categorical variables into a series of binary variables.\n",
    "  - **Ordinal or Label Encoding**: Assign integer values to categories based on their order or labels.\n",
    "  - **Probability Ratio Encoding**: Encode categorical features based on the probability ratio of the target variable.\n",
    "\n",
    "### 3. Feature Discretization\n",
    "\n",
    "Learn how to transform continuous features into discrete ones to simplify models and capture nonlinear relationships.\n",
    "\n",
    "- **Domain Knowledge**: Use expert knowledge to define meaningful bins.\n",
    "- **Unsupervised Methods**:\n",
    "  - **Equal-Width Binning**: Divide the range of values into equal-width bins.\n",
    "  - **Equal-Frequency Binning**: Divide the range of values so that each bin has approximately the same number of observations.\n",
    "  - **K-Means Binning**: Use k-means clustering to create bins based on feature similarity.\n",
    "- **Supervised Methods**:\n",
    "  - **ChiMerge**: Merge bins based on the chi-squared statistic to ensure similarity with respect to the target variable.\n",
    "  - **Decision Tree Binning**: Use decision trees to create bins based on target variable splits.\n",
    "\n",
    "### 4. Feature Aggregation\n",
    "\n",
    "Learn how to combine multiple features into single aggregated features to reduce dimensionality and capture higher-level information.\n",
    "\n",
    "- **Summarizing Features**: Calculate summary statistics (e.g., mean, median, sum) for groups of features.\n",
    "- **Hierarchical Aggregation**: Aggregate features based on hierarchical or nested groupings.\n",
    "- **Temporal Aggregation**: Aggregate features based on time intervals (e.g., daily, monthly averages).\n",
    "\n",
    "## Why This Guide?\n",
    "\n",
    "- **Step-by-Step Tutorials**: Each section includes clear explanations followed by practical examples, ensuring you not only learn but also apply your knowledge.\n",
    "- **Interactive Learning**: Engage with interactive code cells that allow you to see the effects of data transformation methods in real-time.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "- **Run the Cells**: Follow along with the code examples by running the cells yourself. Modify the parameters to see how the results change.\n",
    "- **Explore Further**: After completing the guided sections, try applying the methods to your own datasets to reinforce your learning.\n",
    "\n",
    "Prepare to unlock the full potential of data transformation in data analysis. Let's dive in and transform data into valuable insights!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea06fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T14:40:55.066582Z",
     "iopub.status.busy": "2024-05-30T14:40:55.066050Z",
     "iopub.status.idle": "2024-05-30T14:40:56.089696Z",
     "shell.execute_reply": "2024-05-30T14:40:56.088469Z"
    },
    "papermill": {
     "duration": 1.031969,
     "end_time": "2024-05-30T14:40:56.092403",
     "exception": false,
     "start_time": "2024-05-30T14:40:55.060434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700 entries, 0 to 699\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       680 non-null    float64\n",
      " 1   employ    700 non-null    int64  \n",
      " 2   address   700 non-null    int64  \n",
      " 3   income    663 non-null    float64\n",
      " 4   debtinc   700 non-null    float64\n",
      " 5   creddebt  700 non-null    float64\n",
      " 6   othdebt   700 non-null    float64\n",
      " 7   ed        680 non-null    object \n",
      " 8   default   700 non-null    object \n",
      "dtypes: float64(5), int64(2), object(2)\n",
      "memory usage: 49.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/kaggle/input/loans-and-liability/LoanData_Preprocessed_v1.1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'ed' and 'default' columns to object type\n",
    "data['ed'] = data['ed'].astype('object')\n",
    "data['default'] = data['default'].astype('object')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca5b574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T14:40:56.100898Z",
     "iopub.status.busy": "2024-05-30T14:40:56.100511Z",
     "iopub.status.idle": "2024-05-30T14:40:56.144433Z",
     "shell.execute_reply": "2024-05-30T14:40:56.142919Z"
    },
    "papermill": {
     "duration": 0.05126,
     "end_time": "2024-05-30T14:40:56.147111",
     "exception": false,
     "start_time": "2024-05-30T14:40:56.095851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>680.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>663.00000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.750000</td>\n",
       "      <td>8.388571</td>\n",
       "      <td>8.268571</td>\n",
       "      <td>45.74359</td>\n",
       "      <td>10.260571</td>\n",
       "      <td>1.553553</td>\n",
       "      <td>3.058209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.973215</td>\n",
       "      <td>6.658039</td>\n",
       "      <td>6.821609</td>\n",
       "      <td>37.44108</td>\n",
       "      <td>6.827234</td>\n",
       "      <td>2.117197</td>\n",
       "      <td>3.287555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.045584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.369059</td>\n",
       "      <td>1.044178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.854869</td>\n",
       "      <td>1.987567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>54.50000</td>\n",
       "      <td>14.125000</td>\n",
       "      <td>1.901955</td>\n",
       "      <td>3.923065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>446.00000</td>\n",
       "      <td>41.300000</td>\n",
       "      <td>20.561310</td>\n",
       "      <td>27.033600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age      employ     address     income     debtinc    creddebt  \\\n",
       "count  680.000000  700.000000  700.000000  663.00000  700.000000  700.000000   \n",
       "mean    34.750000    8.388571    8.268571   45.74359   10.260571    1.553553   \n",
       "std      7.973215    6.658039    6.821609   37.44108    6.827234    2.117197   \n",
       "min     20.000000    0.000000    0.000000   14.00000    0.400000    0.011696   \n",
       "25%     28.000000    3.000000    3.000000   24.00000    5.000000    0.369059   \n",
       "50%     34.000000    7.000000    7.000000   34.00000    8.600000    0.854869   \n",
       "75%     40.000000   12.000000   12.000000   54.50000   14.125000    1.901955   \n",
       "max     56.000000   31.000000   34.000000  446.00000   41.300000   20.561310   \n",
       "\n",
       "          othdebt  \n",
       "count  700.000000  \n",
       "mean     3.058209  \n",
       "std      3.287555  \n",
       "min      0.045584  \n",
       "25%      1.044178  \n",
       "50%      1.987567  \n",
       "75%      3.923065  \n",
       "max     27.033600  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2aca3",
   "metadata": {
    "papermill": {
     "duration": 0.003695,
     "end_time": "2024-05-30T14:40:56.154659",
     "exception": false,
     "start_time": "2024-05-30T14:40:56.150964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset contains information about loan applicants and includes the following columns:\n",
    "\n",
    "- **age**: The age of the applicant, indicating how many years they have lived.\n",
    "  - **Range**: 18 - 66\n",
    "  - **Mean**: 34.40\n",
    "  - **Skewness**: Slightly skewed to the right (positive skew).\n",
    "\n",
    "\n",
    "- **employ**: The number of years the applicant has been employed, which can indicate their job stability and experience.\n",
    "  - **Range**: 0 - 31\n",
    "  - **Mean**: 8.21\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **address**: The number of years the applicant has lived at their current address, providing insights into their residential stability.\n",
    "  - **Range**: 0 - 28\n",
    "  - **Mean**: 5.58\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **income**: The annual income of the applicant (in thousands), representing their earning capacity.\n",
    "  - **Range**: 10 - 330\n",
    "  - **Mean**: 55.50\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **debtinc**: The debt-to-income ratio of the applicant, calculated as the percentage of their income that goes towards paying debts. This ratio helps assess their financial burden.\n",
    "  - **Range**: 0.00 - 37.30\n",
    "  - **Mean**: 10.27\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **creddebt**: The amount of credit card debt the applicant has (in thousands), showing their reliance on credit and their debt levels.\n",
    "  - **Range**: 0.00 - 22.12\n",
    "  - **Mean**: 3.51\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **othdebt**: The amount of other debt the applicant has (in thousands), which includes all other forms of debt apart from credit card debt.\n",
    "  - **Range**: 0.00 - 57.03\n",
    "  - **Mean**: 5.05\n",
    "  - **Skewness**: Right-skewed (positive skew).\n",
    "\n",
    "\n",
    "- **ed**: The education level of the applicant (encoded numerically), where higher numbers may represent higher levels of education.\n",
    "  - **Unique Values**: 1.0, 2.0, 3.0, 4.0, 5.0\n",
    "  - **Most Frequent Value (Mode)**: 1.0\n",
    "\n",
    "\n",
    "- **default**: A binary indicator of whether the applicant defaulted on the loan (1 for default, 0 for no default), indicating their credit risk.\n",
    "  - **Unique Values**: 0, 1\n",
    "  - **Most Frequent Value (Mode)**: 0 (majority did not default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4325c",
   "metadata": {
    "papermill": {
     "duration": 0.003184,
     "end_time": "2024-05-30T14:40:56.161442",
     "exception": false,
     "start_time": "2024-05-30T14:40:56.158258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Feature Normalization\n",
    "\n",
    "Feature normalization is a crucial step in data preprocessing, especially for machine learning algorithms that are sensitive to the scale of the data. Algorithms such as gradient descent-based methods (e.g., linear regression, logistic regression) and distance-based methods (e.g., k-nearest neighbors, K-means clustering) can perform poorly or converge slowly if the features have vastly different scales. Normalizing features ensures that all features contribute equally to the model, improving its performance and convergence speed.\n",
    "\n",
    "Different normalization techniques can be applied depending on the nature of the data and the specific requirements of the model. The primary goal of normalization is to transform the features so that they fall within a similar range or distribution, which helps the model learn more effectively.\n",
    "\n",
    "### Types of Feature Normalization\n",
    "\n",
    "- **Min-Max Scaling**: This technique scales features to a fixed range, typically 0 to 1. Min-Max Scaling preserves the relationships between the original data values while transforming them to a new scale. This method is useful when the data needs to be bounded within a specific range.\n",
    "  \n",
    "- **Z-Score Standardization**: Also known as standardization, this technique transforms features to have a mean of 0 and a standard deviation of 1. Z-Score Standardization is particularly useful when the features have different units or scales, as it ensures that each feature contributes equally to the model.\n",
    "\n",
    "- **Robust Scaling**: This technique scales features using statistics that are robust to outliers, such as the median and the interquartile range (IQR). Robust Scaling is less sensitive to outliers than Min-Max Scaling and Z-Score Standardization, making it a good choice for datasets with significant outliers.\n",
    "\n",
    "- **Yeo-Johnson Transformation**: This transformation is used to achieve normality in the data. It can handle both positive and negative values, making it versatile for different types of data. The Yeo-Johnson transformation is particularly useful when the data does not follow a normal distribution.\n",
    "\n",
    "- **Box-Cox Transformation**: This transformation stabilizes variance and reduces skewness in the data to make it more closely meet the assumptions of a linear model. The Box-Cox transformation requires the data to be positive and is effective in transforming skewed data into a more normal distribution.\n",
    "\n",
    "We will cover each of these normalization techniques separately, providing detailed explanations and code examples to illustrate their application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8810a7",
   "metadata": {
    "papermill": {
     "duration": 0.003452,
     "end_time": "2024-05-30T14:40:56.168286",
     "exception": false,
     "start_time": "2024-05-30T14:40:56.164834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Min-Max Scaling\n",
    "\n",
    "Min-Max Scaling, also known as normalization, transforms the features by scaling each feature to a given range, typically between 0 and 1. This technique is particularly useful when you need the data to be bounded within a specific range. Min-Max Scaling preserves the relationships between the original data values while transforming them to a new scale.\n",
    "\n",
    "**Formula**:\n",
    "$$ X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}} $$\n",
    "\n",
    "Let's apply Min-Max Scaling to the `age`, `pathsize`, `lnpos`, and `time` columns.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5115791,
     "sourceId": 8559240,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.659154,
   "end_time": "2024-05-30T14:40:56.693370",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-30T14:40:52.034216",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
