{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf19240",
   "metadata": {
    "papermill": {
     "duration": 0.00484,
     "end_time": "2024-08-15T15:39:00.175070",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.170230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸŒ¿ ðŸ§± In Depth: Matrix Decomposition Techniques\n",
    "\n",
    "Matrix decomposition is a foundational concept in linear algebra and is extensively used in fields such as machine learning, data science, and numerical analysis. It allows us to break down a complex matrix into simpler, more manageable components. This notebook will cover two primary matrix decomposition techniques:\n",
    "\n",
    "1. **Eigen Decomposition**: Used for square matrices.\n",
    "2. **Singular Value Decomposition (SVD)**: Used for any matrix, square or non-square.\n",
    "\n",
    "These techniques help us understand the structure of matrices and have numerous practical applications, such as Principal Component Analysis (PCA), image compression, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b17b7e",
   "metadata": {
    "papermill": {
     "duration": 0.003598,
     "end_time": "2024-08-15T15:39:00.182850",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.179252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Eigen Decomposition for Square Matrices\n",
    "\n",
    "**Eigen Decomposition** is a process of factorizing a square matrix into its eigenvalues and eigenvectors. Eigenvalues and eigenvectors are fundamental concepts in linear algebra and provide significant insights into the properties of a matrix.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "For a given square matrix $A$, if there exists a non-zero vector $v$ and a scalar $\\lambda$ such that:\n",
    "\n",
    "$$ A \\times v = \\lambda \\times v $$\n",
    "\n",
    "Then:\n",
    "- $v$ is called an **eigenvector** of the matrix $A$.\n",
    "- $\\lambda$ is called the corresponding **eigenvalue**.\n",
    "\n",
    "The matrix $A$ can be decomposed into the following form:\n",
    "\n",
    "$$ A = V \\times \\Lambda \\times V^{-1} $$\n",
    "\n",
    "Where:\n",
    "- $V$ is the matrix of eigenvectors.\n",
    "- $\\Lambda$ is a diagonal matrix containing the eigenvalues.\n",
    "- $V^{-1}$ is the inverse of the matrix of eigenvectors.\n",
    "\n",
    "This decomposition shows that the matrix $A$ can be represented as a product of its eigenvectors and eigenvalues.\n",
    "\n",
    "### Proof (Sketch):\n",
    "\n",
    "The eigenvalue equation can be rewritten as:\n",
    "\n",
    "$$ (A - \\lambda I) v = 0 $$\n",
    "\n",
    "Where $I$ is the identity matrix of the same dimension as $A$. For a non-trivial solution to exist, the determinant of $(A - \\lambda I)$ must be zero:\n",
    "\n",
    "$$ \\text{det}(A - \\lambda I) = 0 $$\n",
    "\n",
    "This is known as the **characteristic equation**, and solving it yields the eigenvalues $\\lambda$. Once the eigenvalues are found, the corresponding eigenvectors can be determined by solving the system of linear equations $(A - \\lambda I) v = 0$ for each eigenvalue $\\lambda$.\n",
    "\n",
    "Eigen decomposition is only possible for square matrices, and the matrix must have enough linearly independent eigenvectors to form the matrix $V$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfb523",
   "metadata": {
    "papermill": {
     "duration": 0.00342,
     "end_time": "2024-08-15T15:39:00.190120",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.186700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## From scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc2f67b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T15:39:00.199621Z",
     "iopub.status.busy": "2024-08-15T15:39:00.199172Z",
     "iopub.status.idle": "2024-08-15T15:39:00.238666Z",
     "shell.execute_reply": "2024-08-15T15:39:00.237373Z"
    },
    "papermill": {
     "duration": 0.047629,
     "end_time": "2024-08-15T15:39:00.241419",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.193790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 8.94427191e-01  4.47213596e-01]\n",
      " [-3.56788796e-10  1.00000000e+00]]\n",
      "\n",
      "Numpy Eigenvalues: [5. 2.]\n",
      "Numpy Eigenvectors:\n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def power_iteration(A, num_iterations=1000, tolerance=1e-9):\n",
    "    \"\"\"\n",
    "    Perform power iteration to find the dominant eigenvalue and eigenvector of matrix A.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: The input square matrix.\n",
    "    - num_iterations: Number of iterations to run the power iteration algorithm.\n",
    "    - tolerance: The convergence tolerance for stopping the iteration.\n",
    "    \n",
    "    Returns:\n",
    "    - eigenvalue: The dominant eigenvalue of matrix A.\n",
    "    - eigenvector: The corresponding eigenvector of matrix A.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize a random vector\n",
    "    n = A.shape[1]\n",
    "    b_k = np.random.rand(n)\n",
    "    \n",
    "    # Step 2: Perform the iteration\n",
    "    for _ in range(num_iterations):\n",
    "        # Matrix-vector multiplication\n",
    "        b_k1 = np.dot(A, b_k)\n",
    "        \n",
    "        # Normalize the vector\n",
    "        b_k1_norm = np.linalg.norm(b_k1)\n",
    "        b_k = b_k1 / b_k1_norm\n",
    "        \n",
    "        # Calculate the Rayleigh quotient for the dominant eigenvalue\n",
    "        eigenvalue = np.dot(b_k.T, np.dot(A, b_k)) / np.dot(b_k.T, b_k)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(np.dot(A, b_k) - eigenvalue * b_k) < tolerance:\n",
    "            break\n",
    "\n",
    "    return eigenvalue, b_k\n",
    "\n",
    "def deflate_matrix(A, eigenvalue, eigenvector):\n",
    "    \"\"\"\n",
    "    Deflate the matrix by subtracting the outer product of the found eigenvector.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: The original matrix.\n",
    "    - eigenvalue: The found eigenvalue.\n",
    "    - eigenvector: The corresponding eigenvector.\n",
    "    \n",
    "    Returns:\n",
    "    - A_deflated: The deflated matrix.\n",
    "    \"\"\"\n",
    "    return A - eigenvalue * np.outer(eigenvector, eigenvector)\n",
    "\n",
    "def find_all_eigenvalues_eigenvectors(A, num_eigenvalues=None):\n",
    "    \"\"\"\n",
    "    Find all eigenvalues and eigenvectors of a square matrix A using power iteration and deflation.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: The input square matrix.\n",
    "    - num_eigenvalues: Number of eigenvalues to find (default is the size of the matrix).\n",
    "    \n",
    "    Returns:\n",
    "    - eigenvalues: List of all eigenvalues.\n",
    "    - eigenvectors: List of all eigenvectors.\n",
    "    \"\"\"\n",
    "    n = A.shape[0] if num_eigenvalues is None else num_eigenvalues\n",
    "    eigenvalues = []\n",
    "    eigenvectors = []\n",
    "    \n",
    "    A_copy = A.copy()  # Copy of the matrix to apply deflation\n",
    "    \n",
    "    for _ in range(n):\n",
    "        # Find the dominant eigenvalue and eigenvector using power iteration\n",
    "        eigenvalue, eigenvector = power_iteration(A_copy)\n",
    "        eigenvalues.append(eigenvalue)\n",
    "        eigenvectors.append(eigenvector)\n",
    "        \n",
    "        # Deflate the matrix to remove the found eigenpair's influence\n",
    "        A_copy = deflate_matrix(A_copy, eigenvalue, eigenvector)\n",
    "    \n",
    "    return np.array(eigenvalues), np.array(eigenvectors)\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "\n",
    "# Find all eigenvalues and eigenvectors using power iteration and deflation\n",
    "eigenvalues, eigenvectors = find_all_eigenvalues_eigenvectors(A)\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "# Compare with numpy's built-in function for reference\n",
    "eigenvalues_np, eigenvectors_np = np.linalg.eig(A)\n",
    "print(\"\\nNumpy Eigenvalues:\", eigenvalues_np)\n",
    "print(\"Numpy Eigenvectors:\\n\", eigenvectors_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3bdac7",
   "metadata": {
    "papermill": {
     "duration": 0.004083,
     "end_time": "2024-08-15T15:39:00.250185",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.246102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using np.linalg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b1e695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T15:39:00.260176Z",
     "iopub.status.busy": "2024-08-15T15:39:00.259755Z",
     "iopub.status.idle": "2024-08-15T15:39:00.270090Z",
     "shell.execute_reply": "2024-08-15T15:39:00.268588Z"
    },
    "papermill": {
     "duration": 0.018275,
     "end_time": "2024-08-15T15:39:00.272601",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.254326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n",
      "\n",
      "Reconstructed Matrix:\n",
      " [[4. 2.]\n",
      " [1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Define a square matrix\n",
    "matrix = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Perform Eigen decomposition\n",
    "eigen_values, eigen_vectors = np.linalg.eig(matrix)\n",
    "\n",
    "# Display the eigenvalues and eigenvectors\n",
    "print(\"Eigenvalues:\", eigen_values)\n",
    "print(\"Eigenvectors:\\n\", eigen_vectors)\n",
    "\n",
    "# Reconstruct the matrix from eigen decomposition\n",
    "V = eigen_vectors\n",
    "V_inverse = np.linalg.inv(V)\n",
    "diag = np.diag(eigen_values)\n",
    "reconstructed_matrix = V @ diag @ V_inverse\n",
    "\n",
    "# Display the reconstructed matrix\n",
    "print(\"\\nReconstructed Matrix:\\n\", reconstructed_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143e6f3",
   "metadata": {
    "papermill": {
     "duration": 0.003876,
     "end_time": "2024-08-15T15:39:00.280717",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.276841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 2: Singular Value Decomposition (SVD) for Non-Square Matrices\n",
    "\n",
    "**Singular Value Decomposition (SVD)** is a more general decomposition method that can be applied to any matrix, regardless of whether it is square or non-square. SVD breaks down a matrix into three other matrices, providing a deeper understanding of its structure.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "For any matrix $A$ of dimensions $m \\times n$, SVD represents $A$ as the product of three matrices:\n",
    "\n",
    "$$ A = U \\times \\Sigma \\times V^T $$\n",
    "\n",
    "Where:\n",
    "- $U$ is an $m \\times m$ matrix whose columns are the **left singular vectors** of $A$.\n",
    "- $\\Sigma$ is an $m \\times n$ diagonal matrix containing the **singular values** of $A$.\n",
    "- $V^T$ is the transpose of an $n \\times n$ matrix whose columns are the **right singular vectors** of $A$.\n",
    "\n",
    "The singular values in $\\Sigma$ are non-negative and sorted in decreasing order. They represent the \"strength\" or \"importance\" of the corresponding singular vectors.\n",
    "\n",
    "### Proof (Sketch):\n",
    "\n",
    "SVD can be derived from the **Eigen Decomposition** of $A^T A$. For a matrix $A$, the matrix $A^T A$ is square and symmetric, meaning it has an Eigen Decomposition:\n",
    "\n",
    "$$ A^T A = V \\times \\Lambda \\times V^T $$\n",
    "\n",
    "The eigenvalues of $A^T A$ are the squares of the singular values of $A$, and the eigenvectors of $A^T A$ are the right singular vectors of $A$.\n",
    "\n",
    "Similarly, the matrix $A A^T$ can be decomposed to obtain the left singular vectors of $A$.\n",
    "\n",
    "This approach allows us to decompose any matrix, regardless of its shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d85fe",
   "metadata": {
    "papermill": {
     "duration": 0.003794,
     "end_time": "2024-08-15T15:39:00.288765",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.284971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## From scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09062ec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T15:39:00.299437Z",
     "iopub.status.busy": "2024-08-15T15:39:00.298318Z",
     "iopub.status.idle": "2024-08-15T15:39:00.315559Z",
     "shell.execute_reply": "2024-08-15T15:39:00.314175Z"
    },
    "papermill": {
     "duration": 0.025616,
     "end_time": "2024-08-15T15:39:00.318318",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.292702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U matrix:\n",
      " [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "Sigma matrix:\n",
      " [[3.46410162 0.         0.        ]\n",
      " [0.         3.16227766 0.        ]]\n",
      "V transpose matrix:\n",
      " [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]\n",
      " [-8.94427191e-01  4.47213595e-01  5.07704275e-16]\n",
      " [ 1.82574186e-01  3.65148372e-01 -9.12870929e-01]]\n",
      "\n",
      "Reconstructed Matrix:\n",
      " [[ 3.  1.  1.]\n",
      " [-1.  3.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svd_from_scratch(A, tol=1e-9):\n",
    "    \"\"\"\n",
    "    Perform Singular Value Decomposition (SVD) from scratch.\n",
    "\n",
    "    Parameters:\n",
    "    - A: The input matrix.\n",
    "    - tol: Tolerance for handling small values that should be treated as zero.\n",
    "\n",
    "    Returns:\n",
    "    - U: Matrix of left singular vectors.\n",
    "    - Sigma: Diagonal matrix of singular values.\n",
    "    - Vt: Transpose of the matrix of right singular vectors.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute A^T * A and A * A^T\n",
    "    AtA = np.dot(A.T, A)\n",
    "    AAt = np.dot(A, A.T)\n",
    "\n",
    "    # Step 2: Compute eigenvalues and eigenvectors of A^T * A (V and Sigma)\n",
    "    eigenvalues_AtA, V = np.linalg.eig(AtA)\n",
    "    singular_values = np.sqrt(np.abs(eigenvalues_AtA))\n",
    "    \n",
    "    # Sort singular values and corresponding right singular vectors (V)\n",
    "    sorted_indices = np.argsort(-singular_values)\n",
    "    singular_values = singular_values[sorted_indices]\n",
    "    V = V[:, sorted_indices]\n",
    "\n",
    "    # Step 3: Compute left singular vectors U from A * A^T using singular values\n",
    "    U = np.zeros(AAt.shape)\n",
    "    for i in range(len(singular_values)):\n",
    "        if singular_values[i] > tol:\n",
    "            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n",
    "    \n",
    "    # Step 4: Construct the Sigma matrix with singular values on the diagonal\n",
    "    Sigma = np.zeros(A.shape)\n",
    "    np.fill_diagonal(Sigma, singular_values)\n",
    "    \n",
    "    return U, Sigma, V.T\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[3, 1, 1],\n",
    "              [-1, 3, 1]])\n",
    "\n",
    "# Perform SVD from scratch\n",
    "U, Sigma, Vt = svd_from_scratch(A)\n",
    "\n",
    "print(\"U matrix:\\n\", U)\n",
    "print(\"Sigma matrix:\\n\", Sigma)\n",
    "print(\"V transpose matrix:\\n\", Vt)\n",
    "\n",
    "# Reconstruct the matrix from SVD\n",
    "reconstructed_matrix = np.dot(U, np.dot(Sigma, Vt))\n",
    "print(\"\\nReconstructed Matrix:\\n\", reconstructed_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad3f26",
   "metadata": {
    "papermill": {
     "duration": 0.003934,
     "end_time": "2024-08-15T15:39:00.326413",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.322479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using np.linalg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e6b104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T15:39:00.337538Z",
     "iopub.status.busy": "2024-08-15T15:39:00.336345Z",
     "iopub.status.idle": "2024-08-15T15:39:00.349571Z",
     "shell.execute_reply": "2024-08-15T15:39:00.348227Z"
    },
    "papermill": {
     "duration": 0.021532,
     "end_time": "2024-08-15T15:39:00.352111",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.330579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U matrix:\n",
      " [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "Sigma values: [3.46410162 3.16227766]\n",
      "V transpose matrix:\n",
      " [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]\n",
      " [-8.94427191e-01  4.47213595e-01  5.26260748e-16]\n",
      " [-1.82574186e-01 -3.65148372e-01  9.12870929e-01]]\n",
      "\n",
      "Reconstructed Matrix:\n",
      " [[ 3.  1.  1.]\n",
      " [-1.  3.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Define a non-square matrix\n",
    "matrix = np.array([[3, 1, 1], [-1, 3, 1]])\n",
    "\n",
    "# Perform SVD\n",
    "U, sigma, Vt = np.linalg.svd(matrix)\n",
    "\n",
    "# Display the results\n",
    "print(\"U matrix:\\n\", U)\n",
    "print(\"Sigma values:\", sigma)\n",
    "print(\"V transpose matrix:\\n\", Vt)\n",
    "\n",
    "# Reconstruct the matrix from SVD\n",
    "Sigma = np.zeros(matrix.shape)\n",
    "Sigma[:len(sigma), :len(sigma)] = np.diag(sigma)\n",
    "reconstructed_matrix = U @ Sigma @ Vt\n",
    "\n",
    "# Display the reconstructed matrix\n",
    "print(\"\\nReconstructed Matrix:\\n\", reconstructed_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080a11a",
   "metadata": {
    "papermill": {
     "duration": 0.003909,
     "end_time": "2024-08-15T15:39:00.360378",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.356469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reconstructing Matrices from Decompositions\n",
    "\n",
    "Matrix decomposition allows us to break down matrices into simpler components, but it's essential to verify that we can reconstruct the original matrices from these components. \n",
    "\n",
    "For both Eigen Decomposition and SVD, we can reconstruct the original matrix by multiplying the decomposed matrices. This step is crucial to ensure the correctness of the decomposition process. The reconstructed matrices should closely match the original matrices, validating that the decompositions were performed accurately.\n",
    "\n",
    "In the examples provided above, we demonstrated how to reconstruct the matrices from both Eigen Decomposition and SVD. The results show that the reconstructed matrices are nearly identical to the original ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0bd1c",
   "metadata": {
    "papermill": {
     "duration": 0.00382,
     "end_time": "2024-08-15T15:39:00.368355",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.364535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Practical Applications of Matrix Decomposition\n",
    "\n",
    "Matrix decomposition techniques are not just theoretical; they have numerous real-world applications in various fields, including:\n",
    "\n",
    "- **Principal Component Analysis (PCA)**: Eigen decomposition is a key component of PCA, which is widely used for dimensionality reduction in machine learning and data analysis. By decomposing a covariance matrix into its eigenvectors, PCA identifies the directions (principal components) along which the data varies the most.\n",
    "  \n",
    "- **Latent Semantic Analysis (LSA)**: SVD is used in natural language processing to discover latent relationships between words and documents. By decomposing a term-document matrix using SVD, LSA can identify hidden topics in a corpus of text.\n",
    "\n",
    "- **Image Compression**: SVD is employed in image processing to compress images by retaining only the most significant singular values. By discarding the less important singular values, we can reduce the size of the image data while preserving essential features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb08474",
   "metadata": {
    "papermill": {
     "duration": 0.004077,
     "end_time": "2024-08-15T15:39:00.377088",
     "exception": false,
     "start_time": "2024-08-15T15:39:00.373011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Thank You for Exploring This Notebook!\n",
    "\n",
    "\n",
    "If you have any questions, suggestions, or just want to discuss any of the topics further, please don't hesitate to reach out or leave a comment. Your feedback is not only welcome but also invaluable!\n",
    "\n",
    "Happy analyzing, and stay curious!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.524493,
   "end_time": "2024-08-15T15:39:00.802872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-15T15:38:57.278379",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
